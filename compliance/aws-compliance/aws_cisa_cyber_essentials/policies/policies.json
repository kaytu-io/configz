[
  {
    "ID": "aws_es_domain_in_vpc",
    "Title": "ES domains should be in a VPC",
    "Description": "Manage access to the AWS Cloud by ensuring Amazon Elasticsearch Service (Amazon ES) Domains are within an Amazon Virtual Private Cloud (Amazon VPC).",
    "QueryID": "aws_es_domain_in_vpc",
    "DocumentURI": "policies/aws_es_domain_in_vpc.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ES"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "This AWS control refers to the practice of managing access control to the AWS Cloud. One recommended control measure is guaranteeing that Amazon Elasticsearch Service (Amazon ES) Domains are located within an Amazon Virtual Private Cloud (Amazon VPC).\n\nThis is strongly recommended because an Amazon VPC provides a more secure network environment over the public cloud. By default, resources inside the VPC, such as Amazon ES domains, are not accessible from the public internet; they're only reachable from within the VPC. This effectively isolates them from possible external threats.\n\nIn the context of AWS Control, Amazon ES would be the data and applications that are organized for analysis and visualization. Amazon VPC would be the private network section of the AWS cloud dedicated to your AWS account.\n\nHere is the control in Markup:\n```markdown\n# Manage Access to AWS Cloud\n\nIt is essential to **ensure that Amazon Elasticsearch Service (Amazon ES) Domains are within an Amazon Virtual Private Cloud (Amazon VPC)**. Amazon ES domains inside an Amazon VPC are not directly approachable from the internet and can only be accessed from within the VPC. This implementation thereby provides a more secure and controlled network environment. \n```\nThe above control in markup format helps to present the control more visibly with essential aspects highlighted."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the control of managing access to the AWS Cloud can lead to the following costs:\n\n1. **Security Risks:**\n\n   Not keeping Amazon Elasticsearch Service (Amazon ES) domains within an Amazon Virtual Private Cloud (Amazon VPC) could expose your resources to security vulnerabilities. This could potentially lead to unauthorized access and data breaches, compromising your valuable data.\n\n2. **Increased Operational Costs:**\n\n   Responding to security incidents caused by improper access management can be costly. Activities such as forensic analysis, incident response, system recovery, and repairing reputational damage might add significant unplanned expenses. \n\n3. **Non-compliance Penalties:**\n\n   Depending on the regulatory environment of your business, non-compliance with certain standard security controls may result in hefty fines and penalties due to regulatory violations or failure to comply with industry standards.\n\n4. **Loss of Business or Customers:**\n\n   If your business security is compromised, whether it’s customer data or proprietary information, this could result in loss of customer trust and ultimately, loss of business. It may also prohibit obtaining future business, affecting the long-term financial health of your organization.\n\n5. **Legal Consequences:**\n\n   Apart from regulatory fines and penalties, there could be other legal consequences associated with data breaches, such as lawsuits and settlements."
      ],
      "x-kaytu-usefulness-example": [
        "For example, a company hosts an e-commerce application on AWS. They use Amazon Elasticsearch Service for application search capabilities, and the related data is sensitive that requires to be protected. In this case, placing Amazon ES domains within an Amazon VPC would be beneficial. The company can ensure secure access to its Elasticsearch cluster and restrict public access to it. Moreover, by using the VPC service, network traffic between the company's Amazon ES and other services within the VPC won't leave the Amazon network, which highly secures their data.\n\nThis control can be further strengthened with VPC Security Groups and Network Access Control Lists (ACLs) to allow only trusted traffic, adding an additional layer of security to the company's environment."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_guardduty_enabled",
    "Title": "GuardDuty should be enabled",
    "Description": "Amazon GuardDuty can help to monitor and detect potential cybersecurity events by using threat intelligence feeds.",
    "QueryID": "aws_guardduty_enabled",
    "DocumentURI": "policies/aws_guardduty_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/GuardDuty"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# Amazon GuardDuty\n\nAmazon GuardDuty is a threat detection service that can monitor and detect potential cybersecurity events within your AWS environment. It uses machine learning, anomaly detection, and integrated threat intelligence to classify and prioritize potential threats - facilitating immediate action with minimal false positives.\n\n## How It Works\n\nGuardDuty continuously monitors for malicious or unauthorized behaviour that can indicate a possible cyber threat to your environment. This service analyzes and processes various data streams such as AWS CloudTrail Event Logs, Amazon VPC Flow Logs, and DNS logs. It detects activities like unusual API calls or potentially unauthorized deployments that can indicate a threat.\n\n## Key Features\n\n1. **Continuous Monitoring** - GuardDuty operates 24/7, needing no additional software or hardware.\n\n2. **Threat Intelligence Feeds** - It uses numerous threat intelligence feeds to detect malicious IP addresses, domains and URLs.\n\n3. **Machine Learning** - It applies machine learning to detect anomaly patterns in your network traffic and AWS usage to proactively identify potential threats.\n\n4. **Integration with AWS services** - GuardDuty findings can be exported to other AWS services such as CloudWatch Events and AWS Lambda for further inspection and remediation.\n\n5. **Scalability** - It scales with your AWS usage with no impact on performance.\n\nBy enabling GuardDuty, you can automate continuous security monitoring and threat detection in your AWS environment, thereby enhancing your organization's ability to meet regulatory compliance mandates and reduce overall risk."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to controls such as Amazon GuardDuty can result in several direct and indirect costs to the organization:\n\n1. **Security breaches:** Without Amazon GuardDuty, the organization is more vulnerable to potential cybersecurity events as there would be no continuous scanning for unusual or unauthorized behavior. This could result in breaches that could potentially cost millions of dollars in damages.\n\n2. **Downtime**: Security breaches could result in network downtime while the issue is being resolved. This can lead to loss of revenue and customers especially if the organization operates e-commerce websites and apps.\n\n3. **Costly incident response**: After a security breach, the organization would need to investigate the incident, fix the vulnerability and to recover any losses. These incident response operations can be cost-intensive.\n\n4. **Damage to reputation**: Security breaches can have a significant impact on an organization's reputation. This can lead to loss of trust from customers and partners, and can be extremely costly to repair.\n\n5. **Fines and legal charges**: If the organization is found to be negligent in implementing appropriate security controls, it could face litigation from customers and regulatory fines.\n\n6. **Loss of competitive advantage**: If an organization's proprietary information is compromised in a breach, it could lose its competitive advantage.\n\n7. **Cost of Remediation**: If an organization is not using a service like GuardDuty, it may have to invest in alternate solutions or manage a complex, manual process to achieve the same level of protection, potentially at a higher cost.\n\n8. **Regulatory Compliance Costs**: Many industries are regulated and need to conform to standards that require certain levels of security. Non-compliance with AWS control like Amazon GuardDuty might draw heavy fines and other regulatory penalties. \n\nBy investing in Amazon GuardDuty, organizations can avoid these costs and better protect their networks and data."
      ],
      "x-kaytu-usefulness-example": [
        "Example:\n\n```\nConsider a scenario where a company, XYZ Tech, operates its entire Information Technology infrastructure on AWS. They store sensitive client and company data across several databases and constantly run multiple applications and services. Without a proper security mechanism, XYZ Tech is vulnerable to data breaches, cyber attacks, or unauthorized access.\n\nBy enabling Amazon GuardDuty, this company can provide a more secure environment for its operations. GuardDuty can continually monitor the company's AWS environment for malicious or unauthorized behavior. It uses threat intelligence feeds and machine learning based anomaly detection to identify potentially unauthorized and unintended access to their AWS resources.\n\nFor instance, GuardDuty can detect potentially compromised instances such as instances in the company's environment that might be mining bitcoin, or it can identify instances that are communicating with known malicious IP addresses. \n\nWith Amazon GuardDuty, XYZ Tech can deal with potential threats before they escalate and cause damage, thus adding an additional, robust protective layer to its AWS environment.\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ebs_volume_unused",
    "Title": "EBS volumes should be attached to EC2 instances",
    "Description": "Checks if EBS volumes are attached to EC2 instances.",
    "QueryID": "aws_ebs_volume_unused",
    "DocumentURI": "policies/aws_ebs_volume_unused.md",
    "ManualVerification": false,
    "Severity": "low",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/EBS"
      ],
      "x-kaytu-explanation": [
        "This AWS control is designed to verify if Elastic Block Store (EBS) volumes are attached to Elastic Compute Cloud (EC2) instances. The purpose of this control is to ensure that no EBS volumes are left unattached, which could lead to a wastage of resources and unnecessary costs.\n\nHere's a brief explanation of the components:\n\n- **EBS Volumes**: These are a type of storage block that attach to EC2 instances for the storage of persistent data. They act like hard drives attached to a server. EBS volumes are designed to be highly available and reliable.\n  \n- **EC2 Instances**: These are the virtual servers within AWS's cloud environment on which you run your applications.\n\nThe markup of this AWS control might look like this:\n\n```markdown\n# AWS Control: EBS Volumes Attachment\n\nThis control is responsible for checking if Elastic Block Store (EBS) volumes are attached to Amazon Elastic Compute Cloud (EC2) instances.\n\n## Purpose:\n\nThe primary purpose of this control is to avoid wastage of resources and prevent unnecessary costs by ensuring no EBS volumes are left unattached.\n\n## How it works:\n\n- Checks all EBS volumes within the AWS account's region.\n- Verifies that each EBS volume is attached to an EC2 instance.\n- If EBS volumes are found to be unattached, the control raises an exception for further action to be taken.\n\n## Components:\n\n1. **EBS Volumes:** A type of storage block that's attached to EC2 instances for the storage of persistent data.\n2. **EC2 Instances:** Virtual servers within AWS's cloud on which applications run.\n```\nRemember that the information you are able to retrieve and the actions you can take will depend on the permissions set up on your AWS account."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS Control that checks if EBS volumes are attached to EC2 instances can result in various cost implications:\n\n1. **Unused EBS Volumes**: If EBS volumes are not attached to an EC2 instance, they could be lying idle but still accruing cost as AWS charges for data stored in these provisioned EBS volumes.\n\n2. **Data Loss**: In the case of root volumes, if they're not attached to an EC2 instance, then it's essentially serving no purpose. Any data present in it is not being used and could potentially be lost if for example, the volume gets deleted accidentally or intentionally.\n\n3. **Operational Efficiency**: Not attaching EBS volumes can also affect the overall operational efficiency of your AWS infrastructure. If application data is stored in these volumes and these aren't attached to the necessary EC2 instances, it could potentially impact the performance of these applications, leading to business loss.\n\n4. **Performance Cost**: There is a performance cost related to backup and restore operations. If your EBS volumes are not attached but are being backed up, it's an unnecessary cost.\n\n5. **Security Risks**: Unattached EBS volumes can be attached to any instance by any user with the necessary permissions. This poses a potential security risk as any sensitive data in the volume could be compromised.\n\nBy regularly monitoring and ensuring that all provisioned EBS volumes are attached to an EC2 instance, you can reduce unnecessary costs and enhance the overall efficiency and security of your AWS environment."
      ],
      "x-kaytu-usefulness-example": [
        "AWS Config provides a rule, \"ebs-volume-in-use-check\", which identifies EBS volumes that are not attached to an Amazon EC2 instance. This rule is useful in a several scenarios:\n\n1. **Cost Optimization**: Unattached EBS volumes continue to incur storage costs and can result in unnecessary expenditure. Using this rule, organizations can ensure that all their EBS volumes are in use, helping to optimize costs. \n\n```\n\u003cexample\u003e\n  EBS volumes detected as not in use will generate AWS Config rule compliance results. The system administrators can then review these results and delete unneeded EBS volumes to reduce costs.\n\u003c/example\u003e\n```\n\n2. **Resource Management**: Keeping track of resources is essential for efficient cloud management. Organizations can use the rule to monitor EBS volumes, improving visibility and control over their AWS resources.\n\n```\n\u003cexample\u003e\n  If the system reports an EBS volume as not attached to an EC2 instance, administrators can verify whether it should be attached to a particular instance or deleted to improve resource management.\n\u003c/example\u003e\n```\n\n3. **Security**: Unattached EBS volumes can present a security risk if they contain sensitive data. By ensuring all volumes are attached and in use, organizations can reduce this risk.\n\n```\n\u003cexample\u003e\n  If a sensitive data-containing EBS volume is detected as unattached, security admins can take immediate actions to attach or securely delete the volume. It prevents potential exposure of sensitive data.\n\u003c/example\u003e\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_vpc_eip_associated",
    "Title": "VPC EIPs should be associated with an EC2 instance or ENI",
    "Description": "This rule ensures Elastic IPs allocated to a Amazon Virtual Private Cloud (Amazon VPC) are attached to Amazon Elastic Compute Cloud (Amazon EC2) instances or in-use Elastic Network Interfaces.",
    "QueryID": "aws_vpc_eip_associated",
    "DocumentURI": "policies/aws_vpc_eip_associated.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/VPC"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Enforce Elastic IP Attachment for VPC\n\n## Overview\nThis AWS control is designed to ensure that Elastic IPs (EIPs) allocated to an Amazon Virtual Private Cloud (VPC) are  efficiently used by enforcing their attachment to Amazon Elastic Compute Cloud (EC2) instances or in-use Elastic Network Interfaces (ENIs).\n\n## Rationale\nAmazon charges for EIPs that are allocated to a VPC but aren't associated with an instance or a Network Interface. To prevent wastage of resources and reduce costs, it is important to ensure that all allocated EIPs are attached to active usage points such as an EC2 instance or a Network Interface.\n\n## Description\nIf an Elastic IP address is allocated to a VPC and not associated with an instance or a Network Interface, it incurs charges. This rule checks whether or not all EIPs that are allocated are also attached. The rule returns FAILED if any EIP in a VPC is allocated but not attached.  \n\n## Remediation\nIf the result of the above rule is that the Elastic IP is allocated but not attached, it is recommended to either associate the EIP to an EC2 instance or Network Interface or release the EIP to stop incurring charges. \n\n## Compliance\nMaking sure that all allocated EIPs are attached to a usage point ensures efficient use of resources and can help with cost-optimized architecture. \n\n## Related Links\n\n1. [Elastic IP Address - Amazon EC2](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html)\n2. [What is Amazon VPC? - Amazon Virtual Private Cloud](https://docs.aws.amazon.com/vpc/latest/userguide/what-is-amazon-vpc.html)\n3. [What is Amazon EC2? - Amazon Elastic Compute Cloud](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html)"
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the AWS rule that ensures Elastic IPs allocated to a Amazon Virtual Private Cloud (Amazon VPC) are attached to Amazon Elastic Compute Cloud (Amazon EC2) instances or in-use Elastic Network Interfaces could be summarized into two main elements:\n\n**1. Unnecessary Costs:** \n\nAWS charges a small hourly fee for every Elastic IP that is allocated but not associated with an Amazon EC2 instance or Elastic Network Interfaces. This might seem inconsequential, but these costs can add up quickly if you have multiple unattached Elastic IPs over a long period of time. This is especially true in higher volumes or over a long period of time.\n\n**2. Resource Wastage:**\n\nElastic IPs are a limited resource. Wasting them by not attaching to an EC2 instance or Elastic Network Interfaces could lead to scarcity, and you might end up hitting a limit when you require it in necessity. In worst case scenarios, if other users adequately use their resources, you could cause an unnecessary shortage.\n\n**3. Security Risks:**\n\nUnassociated Elastic IPs can also pose a security risk as they could be potentially used by unauthorized individuals if they acquired control over them somehow. This can lead to exposure of your network to unwanted security threats.\n\n**4. Poor Infrastructure Management:**\n\nKeeping track of the utilization of cloud resources is an important aspect in managing infrastructure effectively. Therefore, having unattached Elastic IPs can indicate poor infrastructure and resource management. This could potentially lead to bigger management issues if not addressed.\n\nIn summary, not complying with this rule could result in unnecessary financial costs, inefficient utilization of resources, potential security risks, and indicators of poor infrastructure management."
      ],
      "x-kaytu-usefulness-example": [
        "Here's an example instance of how this AWS Control can be useful:\n\nConsider a scenario where a business organization uses AWS as part of their IT infrastructure. They use both Amazon Virtual Private Cloud (VPC) for their networking infrastructure and Amazon Elastic Compute Cloud (EC2) instances for flexible and scalable computing capacity.\n\nThey allocate Elastic IPs for their Amazon VPC as they offer reliable static IPv4 addresses, which helps prevent downtime by allowing them to remap their public IP addresses to any instance in their VPC. \n\nSometimes, due to misconfigurations or human errors, Elastic IPs could remain unattached from their EC2 instances. This leads to potential waste of resources because AWS charges for unattached Elastic IPs. It can also be a security risk because unattached Elastic IPs can become points of vulnerability.\n\nWith AWS Control in place, the organization can ensure that every allocated Elastic IP in their Amazon VPC is properly attached to their EC2 instances or to in-use Elastic Network Interfaces. This allows them to eliminate unnecessary costs and maintain a secure, efficient, and robust cloud environment. \n\nThus, through AWS Control functionalities, the organization can both optimize their costs and enhance their security in AWS cloud."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ec2_instance_in_vpc",
    "Title": "EC2 instances should be in a VPC",
    "Description": "Deploy Amazon Elastic Compute Cloud (Amazon EC2) instances within an Amazon Virtual Private Cloud (Amazon VPC) to enable secure communication between an instance and other services within the amazon VPC, without requiring an internet gateway, NAT device, or VPN connection.",
    "QueryID": "aws_ec2_instance_in_vpc",
    "DocumentURI": "policies/aws_ec2_instance_in_vpc.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/EC2"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# Secure Communication on AWS with EC2 Instances using VPC \n\nAWS recommends deploying Amazon Elastic Compute Cloud (Amazon EC2) instances within an Amazon Virtual Private Cloud (Amazon VPC) to establish secure communication channels. This not only improves the security of your AWS resources but also provides you more control over your resource's networking configuration.\n\n## What is Amazon EC2?\n\nAmazon EC2 provides scalable computing capacity in the Amazon Web Services (AWS) cloud. You can use Amazon EC2 to launch as many or as few virtual servers as you need and configure security and networking settings, and manage storage.\n\n## What is Amazon VPC?\n\nAmazon Virtual Private Cloud (VPC) is a service that lets you launch AWS resources in a logically isolated virtual network that you define. You're given control over your virtual networking environment, including selection of your IP address range, creation of subnets, and configuration of route tables and network gateways.\n\n## Secure Communication between Services\n\nWhen you deploy EC2 instances in VPC, they can securely communicate with other services within the same VPC. This secure communication is established without the need of an internet gateway, a Network Address Translation (NAT) device, or a Virtual Private Network (VPN) connection. This configuration enhances security as the communication between services does not need to go through the internet but stays within the secure environment of the Amazon VPC.\n\nThis security control is particularly useful in cases where you have sensitive data that needs to be securely transferred between AWS services without exposure to the public internet."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to this Amazon Web Services (AWS) control primarily relates to security, reliability, and financial consequences such as:\n\n1. **Security Risk**: Failure to confine Amazon EC2 instances within an Amazon VPC may expose data and application to outside threats. Instances not within a VPC are potentially accessible via the internet, which increases the risk of malicious activities such as hacking or data breaches.\n\n2. **Network Communication**: In the absence of a VPC, secure communication between instances and other services within the Amazon environment becomes difficult. Therefore, ignoring this control means jeopardizing the network integrity and the smooth flow of data and information within your environment.\n\n3. **Reliability**: Using Amazon VPC ensures that your resources are residing in a stable and secure environment. So, a lack of compliance might affect the reliability of the applications and services running on EC2 instances.\n\n4. **Additional Costs**: Non-compliance could mean relying on an internet gateway, NAT device, or a VPN connection for allowing communication between services and instances. This adds financial burden as using these services incurs costs, instead of utilising the inherent benefits of VPCs which come at no extra charge.\n\nIn summary, it's vital to adhere to the given AWS control to maintain robust security, ensure smooth network communication, uphold service reliability, and avoid unnecessary costs."
      ],
      "x-kaytu-usefulness-example": [
        "An example use case could be:\n\n```markdown\nA company, XYZ Inc., is using AWS services to host and process sensitive data related to their business. They use a mixture of Amazon EC2 instances for compute, Amazon RDS for managed database services, and Amazon Elastic Map Reduce (EMR) for big data processing. To enhance the security of their environment, they have set up an Amazon VPC.\n\nTraditionally, they might need to use an internet gateway, NAT device, or VPN connection to access these services from their EC2 instance in a secure manner. But doing so introduces additional complexity, potential security risks, and potentially increased latency due to additional networking hops.\n\nBy deploying their EC2 instances within the same VPC as their other services, XYZ Inc. can enable secure, direct communication between these services without the need for these additional networking components. This not only simplifies their architecture, but it also makes it easier to manage and secure, while potentially improving performance by reducing networking latency.\n```\nThis is particularly useful in a scenario where XYZ Inc. operates in heavily regulated industries such as finance or healthcare, where security and compliance are critical. By keeping all communication within the Amazon VPC, they can more confidently meet their compliance requirements."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_securityhub_enabled",
    "Title": "AWS Security Hub should be enabled for an AWS Account",
    "Description": "AWS Security Hub helps to monitor unauthorized personnel, connections, devices, and software. AWS Security Hub aggregates, organizes, and prioritizes the security alerts, or findings, from multiple AWS services.",
    "QueryID": "aws_securityhub_enabled",
    "DocumentURI": "policies/aws_securityhub_enabled.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/SecurityHub"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Security Hub\n\nAWS Security Hub is a primary service provided by Amazon Web Services (AWS) that provides a comprehensive view of your high-priority security alerts and security status across your AWS accounts. It is a centralized hub that helps you manage and check the security and compliance of your accounts and workloads. \n\n## Features\n\n- **Unified security and compliance center:** AWS Security Hub acts as a single place that collects and aggregates findings from multiple AWS services and from the AWS partner security solutions.\n- **Security alerts management:** It prioritizes your security alerts, or findings, across AWS accounts so that you can spot trends, identify potential issues, and take remedial action if necessary.\n- **Monitoring:** This control helps to monitor potentially unauthorized personnel, connections, devices, and software, thereby enhancing your security posture.\n- **Built-in compliance standards checks:** AWS Security Hub provides automated compliance checks based on industry standards and best practices, helping you to understand your overall compliance status.\n\n## Benefits\n\n- **Improved visibility:** It provides you with a comprehensive view into your security and compliance status across multiple AWS accounts.\n- **Increased efficiency:** By integrating and consolidating security findings from various sources, it reduces the effort of continuous security checks and managing compliance data.\n- **Faster identification and action:** By organizing and prioritizing security alerts, it enables you to identify and react to critical threats or issues more quickly.\n\nAWS Security Hub is a crucial tool for businesses using AWS services that seeks to enhance their security management through efficient monitoring and managing of security alerts and compliance data."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the AWS Security Hub control could result in several potential consequences:\n\n1. **Monetary Losses:** If an unauthorized access leads to breaches, it could result in financial losses either directly (through theft of financial information) or indirectly (through the theft of customer data that leads to lawsuits or regulatory fines).\n\n2. **Data Breaches:** Non-compliance could lead to unauthorized access of sensitive company data, customer information, intellectual property, which can in turn lead to serious legal consequences and damage of the company's reputation.\n\n3. **Damage to Brand Reputation:** A security breach can cause serious reputation damage which can lead to loss of customer trust and business. \n\n4. **Business Continuity Risk:** Severe security breaches can lead to significant system downtimes, impacting the business operations. \n\n5. **Non-Compliance Legal Consequences:** Depending on the sector in which the company operates, non-compliance might result in significant fines and legal ramifications, this is particularly true for industries like healthcare, finance etc which are subject to strict regulations on data security and privacy. \n\nThus, adhering to AWS Security Hub control not only helps in ensuring proper security practices but also safeguards the organization against potential security breaches, ensuring business continuity, maintaining legal compliance, preserving reputation and avoiding financial losses."
      ],
      "x-kaytu-usefulness-example": [
        "Example Instance:\n\nThis is particularly useful for a large enterprise with numerous AWS accounts. Suppose this large company is using various AWS services like IAM, AWS Firewall Manager, Amazon GuardDuty, and AWS Macie all serving different roles and generating multiple security alerts.\n\nWithout AWS Security Hub, the company would have to monitor and manage each alert individually in each corresponding service which can be quite difficult and time-consuming. Some serious threats might even go unnoticed due to the lack of centralized reporting.\n\nHowever, with AWS Security Hub, all these alerts from different services are aggregated, organized, and prioritized in one place. The enterprise can rapidly and systematically identify the highest priority security issues.\n\nFor instance, if someone attempts to access their AWS data unauthorized, both Amazon GuardDuty (for threat detection) and IAM (to check if they have appropriate permissions) would generate alerts. AWS Security Hub consolidates these alerts into a single finding, making it easier for the security team to follow up the issue.\n\nThus, AWS Security Hub proves its usefulness by simplifying security tasks and enhancing the overall security posture of their AWS environment."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_user_mfa_enabled",
    "Title": "IAM user MFA should be enabled",
    "Description": "Enable this rule to restrict access to resources in the AWS Cloud.",
    "QueryID": "aws_iam_user_mfa_enabled",
    "DocumentURI": "policies/aws_iam_user_mfa_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/IAM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "This rule is a part of AWS (Amazon Web Services) Identity and Access Management (IAM) policy that helps you manage access to your AWS services and resources securely. Once the rule is enabled, it restricts unauthorised access to the resources on AWS Cloud. \n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"*\",\n            \"Resource\": \"*\"\n        }\n    ]\n}\n```\n\nThe above example is a basic IAM policy in JSON format where the `Effect` is `Allow`, `Action` is `*` (which means all actions are allowed), and `Resource` is `*` (which means all resources can be accessed). You can change these details per your requirements, for example, if you want to restrict access to a specific resource or want to deny an action."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with AWS control rules, such as the restriction of access to resources in the AWS Cloud, can lead to several potential costs:\n\n1. **Financial Cost**\n\n   A data breach can lead to financial penalties, depending on the regulations in place like GDPR. Non-compliance may lead to unauthorized access, with potential financial repercussions associated with theft or loss of sensitive data. \n\n2. **Reputation Cost**\n\n   A data breach can severely damage your reputation, making it difficult to establish trust with existing and potential clients. If unauthorized individuals gain access to sensitive data, your customers might lose faith in your company's ability to protect their personal information. This could lead to loss of customers and revenue.\n\n3. **Operational Cost**\n\n   The cost of mitigating a data breach can be significant. This includes the cost associated with identifying the breach, containing it, investigating it, and implementing enhanced security measures to prevent the re-occurrence of such breaches. This also includes the cost of potential downtime, if services have to be temporarily suspended due to the breach.\n\n4. **Regulatory Costs**\n\n   Many industries are regulated by laws that require companies to safeguard their customer data. Failure to comply with these laws can result in heavy fines and penalties.\n\n5. **Legal Cost**\n\n   If confidential or sensitive information is breached, companies could potentially face legal actions from customers or business partners. These lawsuits can result in legal fees and payments of damages, further increasing the cost of non-compliance."
      ],
      "x-kaytu-usefulness-example": [
        "- **Example 1** \n\nScenario: Jane works for a Tech Firm named \"XYZ Co.\" They use AWS Cloud for storage and operation. However, there are different teams in the company, and not everyone should have access to all resources on the Cloud. \n\nHere, enabling the rule to restrict access to resources in the AWS Cloud is crucial to protect sensitive data from unauthorized access.\n\n```\naws iam create-policy \\\n    --policy-name XYZCo.RestrictedPolicy \\\n    --policy-document file://XYZCo.RestrictedPolicy.json\n```\n\nIn this json policy example, defined in the `XYZCo.RestrictedPolicy.json`, only those resources can be accessed where `\"aws:RequestedRegion\": \"us-west-2\"` and `\"aws:SourceIp\": \"203.0.113.0/24\"`. \n\n- **Example 2** \n\nScenario: John is the IT manager of a retail company called \"ABC Retail\". They have a custom application hosted in the AWS Cloud. The application performs all sorts of background operations, including customer data manipulations. \n\nTo ensure the security and privacy of customers, John enables this rule to restrict access to resources on the cloud.\n\n```\naws ec2 authorize-security-group-ingress \\\n    --group-id sg-903004f8 \\\n    --protocol tcp \\\n    --port 80 \\\n    --cidr 203.0.113.0/24\n```\nBy running this command, John has implemented an access control rule in the security group that only allows traffic on TCP port 80 (typically a web server running HTTP) from the IP address range `203.0.113.0/24`. This way, John ensures that only the authorized people have access to customer data."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_vpc_flow_logs_enabled",
    "Title": "VPC flow logs should be enabled",
    "Description": "The VPC flow logs provide detailed records for information about the IP traffic going to and from network interfaces in your Amazon Virtual Private Cloud (Amazon VPC.",
    "QueryID": "aws_vpc_flow_logs_enabled",
    "DocumentURI": "policies/aws_vpc_flow_logs_enabled.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/VPC"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "VPC Flow Logs is a feature provided by Amazon Web Services (AWS), allowing you to effectively and securely monitor your networking activity in your Amazon Virtual Private Cloud (VPC). It records and captures information about the IP traffic going to and from network interfaces in your Amazon VPC.\n\n```markdown\n# AWS VPC Flow Logs\n\nAmazon Virtual Private Cloud (VPC) provides a feature called VPC Flow Logs that allows you to:\n\n- **Capture Information**: VPC Flow Logs captures detailed records about the IP traffic going to and from network interfaces in your VPC.\n\n- **Monitor and Troubleshoot**: By capturing the traffic data, you can monitor network traffic that reaches your instances and troubleshoot why specific traffic is not reaching an instance.\n\n- **Track and Store Data**: You can store these records or logs in Amazon CloudWatch Logs and Amazon S3. It also helps in tracking the bytes of data transferred to and from your VPC.\n\n**Note**:\n- The use of VPC Flow Logs may incur additional charges.\nThis feature does not capture real-time log streams, but instead captures and publishes flow logs to your chosen destination (Amazon S3 or Amazon CloudWatch Logs) after a short time delay.\n```\nBefore you can create VPC Flow Logs, you need to set up the necessary IAM roles and policies to allow the service to publish logs to your chosen destination. The actual setup process may involve several steps including setting up IAM roles, creating the VPC Flow Log, and then setting up the delivery of the logs to your chosen destination."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS control of enabling and monitoring VPC flow logs can lead to significant risks and potential costs, which include:\n\n1. **Security Risks:** Without VPC flow logs, malicious activities cannot be traced such as DDoS attack, brute force attack or any other intrusion attempts. This can lead to unauthorized access to sensitive data which can severely damage business reputation.\n\n2. **Diagnostics and Troubleshooting:** Flow logs enable prompt diagnosis and resolution of network issues. Without them, troubleshooting becomes difficult, causing longer downtimes. These downtimes can cost the business in the form of lost productivity, decreased user satisfaction, and lost revenue.\n\n3. **Compliance Violations:** Many industries are regulated by laws that demand certain levels of data security. Non-compliance to these regulations due to lack of network traffic logs can result in heavy fines. \n\n4. **Uncontrolled Costs:** Without visibility and control over the network traffic, there could be wastage of resources, resulting in higher costs. \n\n5. **Forensic Analysis:** In case of any security incident, the flow logs are essential for deep dive investigation. Non-availability of logs will not only hamper incident analysis but can also lead to inability to prove due diligence in court if required.\n\nIn conclusion, the cost of non-compliance with implementing VPC flow logs can be considerable in terms of financial impact, business reputation damage, and potential legal liabilities."
      ],
      "x-kaytu-usefulness-example": [
        "For example, consider a company that uses Amazon VPC for hosting their various applications. There was a sudden spike in traffic which led to slower performance of some applications. The network admins were not sure if it was due to some sort of DDoS attack or due to legitimate traffic.\n\nTo diagnose the issue, the admins activated the VPC flow logs. The logs helped them to analyse the IP traffic going in and out of their network interfaces. They could see detailed information such as source and destination IP addresses, packet and byte counts, and start and end times. \n\nThrough these logs, they identified a set of unfamiliar IP addresses with unusually high amount of traffic which suggested that it could be a DDoS attack. They quickly managed to block these suspicious IP addresses, and the traffic went back to normal.\n\nHence, the VPC flow logs proved instrumental in identifying and resolving the network traffic issue."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_s3_bucket_enforces_ssl",
    "Title": "S3 buckets should enforce SSL",
    "Description": "To help protect data in transit, ensure that your Amazon Simple Storage Service (Amazon S3) buckets require requests to use Secure Socket Layer (SSL).",
    "QueryID": "aws_s3_bucket_enforces_ssl",
    "DocumentURI": "policies/aws_s3_bucket_enforces_ssl.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/S3"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "This AWS control refers to the practice of enforcing the Secure Socket Layer (SSL) encryption for accessing data from your Amazon S3 buckets. An Amazon S3 bucket is a public cloud storage resource on Amazon Web Services (AWS).\n\nWhen you require requests to your S3 buckets to use SSL, all traffic between the client and the server is encrypted, effectively protecting your data in transit from eavesdropping, tampering, or message forgery.\n\nHere is how you can describe this control in the markup format:\n\n```\n### Secure Transmissions to Amazon S3 Buckets\n\nTo enhance the security of your data while it's being transmitted, make sure to put in place necessary requirements for requests to your Amazon S3 buckets to use Secure Socket Layer (SSL). \n\nSSL is a security protocol that establishes encrypted links between a web server and a browser, ensuring that all data passed between them remain private. When SSL is used, all data from your S3 buckets that are sent to a client are encrypted, providing an effective protection against eavesdropping, tampering, and message forgery. \n\nTherefore, enforcing SSL on your Amazon S3 buckets helps to safeguard your data in transit and uphold the principle of confidentiality in your data transfer processes.\n\nRemember to regularly review AWS's recommended security best practices and apply them as necessary to maintain the security and reliability of your services.\n```"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS control that requires the use of Secure Socket Layer (SSL) for Amazon Simple Storage Service (Amazon S3) bucket requests can lead to several costs and risks:\n\n1. **Data Breach**: Without SSL, data in transit can be snatched by hackers. This kind of attack, known as Man-in-the-Middle (MitM), allows attackers to steal sensitive data and can lead to legal and financial consequences, including penalties for non-compliance with data protection regulations.\n\n2. **Loss of Reputation**: A data breach can seriously damage your organization's reputation. This could cost you customers, partnerships, or potential business opportunities.\n\n3. **Regulatory Fines**: Non-compliance with industry data protection standards such as GDPR, HIPAA, or PCI DSS can result in heavy fines. These regulations require businesses to take steps to protect sensitive data in transit, such as using SSL.\n\n4. **Legal and Lawsuit Costs**: If a data breach leads to significant damage or loss for customers or partners, they could sue your organization, leading to legal fees and settlement or judgment costs.\n\n5. **Operational Disruption**: Depending on the severity of a data breach, you might need to halt operations to investigate and contain the breach, resulting in lost productivity and revenue.\n\nIn summary, ensuring that Amazon S3 buckets require requests to use SSL is an essential precaution to prevent costly data breaches and maintain compliance with data protection regulations."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, an organization is developing a cloud-based application on AWS that will deal with sensitive data related to their clients. This data is planned to be stored in an Amazon S3 bucket. To ensure that all incoming and outgoing data transfers are secure and encrypted, it is a priority for the organization to enable SSL for their S3 bucket. This AWS control of using SSL for ensuring encrypted connection helps to prevent unauthorized individuals from tampering with the data while it is in transit. As a result, it significantly enhances the application's security posture by protecting sensitive client data from potential breaches."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_lambda_function_in_vpc",
    "Title": "Lambda functions should be in a VPC",
    "Description": "Deploy AWS Lambda functions within an Amazon Virtual Private Cloud (Amazon VPC) for a secure communication between a function and other services within the Amazon VPC.",
    "QueryID": "aws_lambda_function_in_vpc",
    "DocumentURI": "policies/aws_lambda_function_in_vpc.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/Lambda"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS Lambda is a compute service that lets you run code without provisioning or managing servers. This control relates to deploying these AWS Lambda functions within an Amazon Virtual Public Cloud (Amazon VPC). \n\nVPC is an on-demand configurable pool of shared computing resources allocated within a public cloud environment. Provisioning a private section of the AWS Cloud where you can launch AWS resources within a virtual network.\n\nBy deploying AWS Lambda functions within an VPC, it provides a secure way for these functions to communicate with other services within the same VPC. This is particularly useful in cases where sensitive information needs to be exchanged between services, the communication is kept within the private network and away from the public internet.\n\nHere is some sample markup that can provide a syntax reference for how to accomplish this:\n\n```markdown\nTo deploy AWS Lambda functions within a VPC:\n\n1. Open the AWS Management Console, navigate to the AWS Lambda console.\n2. Choose 'Create Function' \n3. Under 'Basic information', configure your function settings as needed.\n4. In the 'VPC' section, select a VPC from the list.\n5. For 'Subnets', choose 'Add subnet' and select a subnet.\n6. For 'Security groups', choose 'Add security group' and select a security group.\n7. Choose 'Create function' to finalize.\n\nNote that the VPC, subnets, and security group should already be set up and properly configured for your services.\n```\n\nRemember to ensure that you have correct permissions and that resources within your VPC (like databases) are reachable from the Lambda function and the Egress (outbound) and Ingress (inbound) rules for your VPC are set up correctly.\n"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with the AWS control to deploy Amazon Lambda functions within an Amazon Virtual Private Cloud (VPC) can lead to various potential costs such as:\n\n1. **Security risks**: Without the protective layer of a VPC, your AWS Lambda functions are more exposed, and thus vulnerable to security attacks. This can lead to unauthorized access or data breaches, potentially causing financial and reputational harm.\n\n2. **Network performance**: Communication between your Lambda functions and other services might not be optimal if not in a VPC. This can lead to impaired performance or increased latency, and thus decreased efficiency and productivity.\n\n3. **Regulatory penalties**: Depending on your industry, you may be under a regulatory obligation to ensure that your data and services are secured in specific ways. Failing to use a VPC could be a violation of such obligations and could result in heavy financial penalties.\n\n4. **Increased maintenance cost**: Without a VPC, you might find yourself spending more time and resources to manage and secure communication between services, leading to higher operational costs.\n\n5. **Data transfer costs**: AWS does charge for data transfer between services if not done within a VPC. This can lead to increased costs particularly if large amounts of data are being transferred on a regular basis.\n\nAdhering to this control is therefore beneficial in ensuring the secure, efficient, compliant, and cost-effective functioning of your AWS services."
      ],
      "x-kaytu-usefulness-example": [
        "For example, a medium-sized corporation within the financial sector requires processing substantial datasets from their PostgreSQL database located in Amazon RDS. The data processing must take place within secure and private network for compliance and security reasons. \n\nTo meet these requirements, they can use AWS Lambda within Amazon VPC, which allows them to securely access their RDS infrastructure, process data, and store results in an encrypted AWS S3 bucket. This strategy ensures that sensitive data only traverses the secure, private network and is never exposed to the public internet. By using this AWS control, the continuity of secure and private processing is maintained, thus reducing the chance of data leakage or unauthorized access."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_vpc_network_acl_unused",
    "Title": "VPC network access control lists (network ACLs) should be associated with a subnet.",
    "Description": "Ensure there are no unused network access control lists (network ACLs). The rule is compliant if each network ACL is associated with a subnet. The rule is non-compliant if a network ACL is not associated with a subnet.",
    "QueryID": "aws_vpc_network_acl_unused",
    "DocumentURI": "policies/aws_vpc_network_acl_unused.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/VPC"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: No Unused Network Access Control Lists (Network ACLs)\n\nThis control monitors your AWS infrastructure to ensure that there are no unused network access control lists (network ACLs) within your setup. Network ACLs act as a layer of security for your AWS subnets, controlling inbound and outbound traffic at the subnet level.\n\n## Compliance\nThis rule is considered **compliant** if each network ACL within your AWS environment is associated with at least one subnet. \n\n## Non-Compliance\nThis rule is **non-compliant** if there is a network ACL that is not associated with any subnet. Such network ACLs can be considered as unused or redundant.\n\nThe purpose of this control is to maintain a clean, efficient, and secure AWS environment by avoiding unused and potentially unnecessary resources. Unused network ACLs increase management overhead and complicate your security model without providing any benefits. They can also increase the risk of misconfiguration and potential exposure if not managed properly.\n\n## How to Fix Non-Compliance\nIn the event of a non-compliant network ACL (i.e., an network ACL not associated with any subnet), you should review and determine if the ACL can be safely deleted. If it's required for future use, it should be documented and justified accordingly. If not, it should be removed to ensure a clean and efficient AWS setup. \n\nRemember to always follow your organization's change management procedures when making changes to your infrastructure."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS control could pose several costs, both potential and tangible:\n\n1. **Security Risk:** Unused network access control lists (ACLs) pose a security risk. This is because they may hold outdated rules which exposed services that ought to be hidden, or they could be manipulated by a malicious actor to gain access to your AWS resources.\n\n2. **Operational Efficiency:** Unused network ACLs might cause confusion and operational inefficiency when managing network access. This is especially true for large and complex networks spread across multiple VPCs and subnets.\n\n3. **Increased Costs:** While the direct cost of unused ACLs may be negligible, there could be indirect costs incurred from time spent on managing and reviewing these unnecessary resources. This would distract from valuable time which could be better spent on maintaining and improving the system.\n\n4. **Non-Compliance Costs:** Depending on the regulatory environment of your industry, non-compliance with certain security controls might lead to fines, sanctions, or loss of particular certifications or accreditations. Especially, if data breaches could be traced back to poor management of network access controls.\n\n5. **Reputational Damage:** From a business perspective, potential data breaches that result from inefficient access control could lead to serious reputational damage which could lead to loss of customers in the long run.\n\nIn summary, while unused network ACLs might seem harmless, they pose several risks that can lead to serious consequences and should be appropriately managed to maintain a healthy and secure AWS environment."
      ],
      "x-kaytu-usefulness-example": [
        "In an AWS infrastructure, it's a best practice to minimize unnecessary components to avoid potential security risks and to optimize the usage of resources. Each unused network access control list (ACL) can be a potential target for attackers. By ensuring the use of all network ACLs, you help to tighten security, minimize misconfigurations, and offer efficient resource use.\n\nFor instance, consider a scenario where you have several network ACLs set up in your AWS environment for multiple subnets associated with different environments, such as Development, Staging, and Production. Over time, you remove some subnets as part of a clean-up or resource optimization effort. However, you forget to remove the associated network ACLs. \n\nThese orphaned network ACLs may still have ingress and egress rules that expose your infrastructure to unnecessary risk. Additionally, these unused network ACLs misuse your team's cognitive resources as they are part of your ACL inventory and show up in routine audits and reports. \n\nRunning a periodic automated compliance check that validates and ensures that each ACL is associated with a subnet can help minimize these risks. Automated AWS services such as AWS Config can be instrumental in such tasks, providing you with a report on the compliance status of your infrastructure against AWS best practices."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ec2_instance_ssm_managed",
    "Title": "EC2 instances should be managed by AWS Systems Manager",
    "Description": "An inventory of the software platforms and applications within the organization is possible by managing Amazon Elastic Compute Cloud (Amazon EC2) instances with AWS Systems Manager.",
    "QueryID": "aws_ec2_instance_ssm_managed",
    "DocumentURI": "policies/aws_ec2_instance_ssm_managed.md",
    "ManualVerification": false,
    "Severity": "low",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/SSM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Systems Manager with Amazon EC2\n\nThe **Amazon Elastic Compute Cloud (Amazon EC2)** provides scalable computing capacity in the Amazon Web Services (AWS) cloud. It is designed to make web-scale computing easier for developers, as it allows users to obtain and configure capacity with minimal effort.\n\nThe **AWS Systems Manager** aids in managing your EC2 instances, making it possible for you to visualize and control your infrastructure on AWS. One of the key functions it provides is the maintaining and generation of an inventory of the software platforms and applications within your organization.\n\n## How it Works\n\nThe AWS Systems Manager Inventory gathers metadata from your managed instances which includes the instance's system components such as:\n\n- Installed applications\n- Installed updates\n- Network configurations\n- Environment variables\n\nThis captured metadata is then stored and organized for future reference. You can use this inventory to:\n\n- Identify applications currently installed in your instances\n- Track software platforms and their configurations\n- Gather OS details of the running instances \n\nThus, providing a consistent overview and helping in managing, tracking and troubleshooting any issues within your environment. \n\nThis is particularly beneficial in organizations dealing with a large number of instances, as AWS Systems Manager Inventory helps you to avoid manual record-keeping and enhances the visibility over your operations.\n\nTo start using AWS Systems Manager to manage your EC2 instances, you can use the AWS Management Console, AWS CLI, or SDKs. AWS provides extensive documentation to guide you through the process. \n\nIn conclusion, using AWS Systems Manager with Amazon EC2 allows you to easily manage and take inventory of your software platforms and applications, simplifying the task of infrastructure management in the cloud."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS control which requires inventory of software platforms and applications managed with AWS Systems Manager would have several costs:\n\n1. **Security Risks:** Without a complete software inventory, it would be challenging to ensure each software component is updated and secured against known vulnerabilities. This might expose the system to potential cyber-attacks which could lead to data breaches, resulting in financial loss and damage to the company's reputation.\n\n2. **Operational Inefficiency:** An outdated inventory may result in overutilization or underutilization of resources, leading to inefficiencies. It may also lead to unexpected operational issues due to potential conflicts between different software components that haven't been accounted for.\n\n3. **Audit Failure:** In some sectors such as financial services or health care, regulatory bodies often mandate maintaining an up-to-date software inventory as part of their compliance requirements. Non-compliance to this might result in audit failure, substantial fines, and penalties.\n\n4. **Redundancy:** Without a current inventory, there might be a risk of using and paying for redundant software applications. This lack of control and visibility can add unnecessary software costs.\n\nIn summary, non-compliance to this AWS control could result in significant financial and operational risks. Therefore, it's essential to regularly update and maintain a software inventory within AWS System Manager while managing Amazon EC2 instances."
      ],
      "x-kaytu-usefulness-example": [
        "Amazon EC2, in combination with AWS Systems Manager, allows organizations to inventory and manage their software applications effectively. For example, a large SaaS (Software as a service) company has its applications distributed across various EC2 instances. By leveraging AWS Systems Manager, the company can create an inventory of these applications, view detailed system configurations, and track their software configurations across their EC2 instances. This helps in maintaining a single source of truth for software applications, improving troubleshooting, maintaining compliance, and managing vulnerabilities. Inventories can include file information, network configuration, installed applications, OS details, or custom configurations according to the organization's needs. This centralized view reduces the complexity and aids in planning software and security patch updates."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_policy_no_star_star",
    "Title": "IAM policy should not have statements with admin access",
    "Description": "AWS Identity and Access Management (IAM) can help you incorporate the principles of least privilege and separation of duties with access permissions and authorizations, restricting policies from containing 'Effect': 'Allow' with 'Action': '*' over 'Resource': '*'.",
    "QueryID": "aws_iam_policy_custom_no_star_star",
    "DocumentURI": "policies/aws_iam_policy_no_star_star.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/IAM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. It helps you control who is authenticated (signed in) and authorized (has permissions) to use resources. It employs the principles of least privilege and separation of duties.\n\nPrinciple of Least Privilege refers to giving a user only those privileges which are essential to perform his/her duties. In AWS IAM, you can make sure that AWS users have minimal access to resources by appropriately assigning permissions.\n\nSimilarly, the principle of separation of duties ensures that key tasks are not left to a single person thereby reducing the risk of accident or fraud.\n\nAWS offers a variety of policy types that can be used to manage permissions, which include identity-based policies, resource-based policies, permissions boundaries, and AWS managed policies. \n\nOne of the ways to ensure both these principles is to avoid granting 'Effect': 'Allow' with 'Action': '*' over 'Resource': '*'. This effectively gives the policy holder the power to perform any action over any of the AWS resources, overriding both principles of least privilege and separation of duties. \n\nTo avoid this, it's recommended to design IAM policies stating specific actions that can be performed on specific resources, therefore implementing a much secure and granular control of your AWS environment.\n\n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"s3:PutObject\",\n                \"s3:GetObject\",\n                \"s3:ListBucket\"\n            ],\n            \"Resource\": [\n                \"arn:aws:s3:::example_bucket/*\"\n            ]\n        }\n    ]\n}\n```\n\nIn the above example the IAM policy limits the actions to only put, get, and list on `example_bucket` in S3 service."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance with this AWS IAM control can be significant. This specific control typically seeks to prevent the misuse or abuse of access rights within AWS, ensuring that users, groups, and roles are given only the minimal necessary permissions to perform their duties, and the potential impact of non-compliance may include:\n\n1. **Security Breaches**: Without the necessary restrictions, malicious individuals might gain access to critical systems and sensitive data, leading to unauthorized data access or alteration, data breaches, and system disruptions.\n\n2. **Unintended Actions**: Unrestricted permissions mean that even well-intentioned users could inadvertently modify or delete critical resources, causing operational issues.\n\n3. **Regulatory Fines and Costs**: Depending on the applicable regulations (e.g., GDPR, HIPAA, etc.), non-compliance could lead to heavy financial penalties. You may also face the cost of audits and potential legal fees.\n\n4. **Reputation Damage**: If your organization suffers a security breach due to poor access controls, it could face significant reputational damage, leading to loss of customers and revenue.\n\n5. **Increased Operational Costs**: Without the constraints enforced by this control, managing and tracking who has access to what can become very complex and cost-ineffective.\n\nThe cost of non-compliance, therefore, can be thought of as the potential financial, operational, and reputational damage your organization could suffer due to poor IAM practices. It further underlines the importance of adhering to AWS IAM controls and general best practices for IAM."
      ],
      "x-kaytu-usefulness-example": [
        "AWS Identity and Access Management (IAM) is essential for ensuring secure access control to AWS services. As an example, consider the context where your organization has a large number of employees, each requiring different levels of access to AWS resources for various projects.\n\nYou have project managers who only need read access to the AWS Management Console to supervise project progress. You have developers who need access to EC2 instances, S3 buckets, and DynamoDB databases to build and deploy applications. You also have database administrators who should have full access to RDS instances but limited or no access to EC2 instances or S3 buckets. \n\nUsing IAM, you can set-up users and groups, each with properly scoped access policies that follow the principle of least privilege. This means a user can only access what they need for their job, nothing more, nothing less. This helps to notably reduce the risk of accidental or malicious actions affecting sensitive resources or data.\n\nThe existing policy of 'Effect':'Allow' with 'Action':'*' over 'Resource':'*' implies complete access to all resources. This is regarded as an unsafe practice because it does not follow the principle of least privilege, increasing the risk of unwanted actions, and IAM helps you prevent this.\n\nImplementing IAM also supports the concept of separation of duties. This means that the duties can be divided among different users or teams, to prevent any potential fraudulent activities or mistakes, thereby enhancing the security of your AWS environment.\n\n```\nIAM Example policy:\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"dynamodb:*\"\n            ],\n            \"Resource\": \"arn:aws:dynamodb:us-east-1:123456789012:table/MyTable\"\n        },\n        {\n            \"Effect\": \"Deny\",\n            \"Action\": [\n                \"s3:*\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n```\n\nIn the above example, the user is allowed to perform any DynamoDB operations they need on the 'MyTable' in the us-east-1 region, but they are denied all actions on S3, providing effective utilization of IAM."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_cloudtrail_trail_enabled",
    "Title": "At least one enabled trail should be present in a region",
    "Description": "AWS CloudTrail can help in non-repudiation by recording AWS Management Console actions and API calls. You can identify the users and AWS accounts that called an AWS service, the source IP address where the calls generated, and the timings of the calls. Details of captured data are seen within AWS CloudTrail Record Contents.",
    "QueryID": "aws_cloudtrail_trail_enabled",
    "DocumentURI": "policies/aws_cloudtrail_trail_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/CloudTrail"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS CloudTrail for Non-Repudiation\n\n[AWS CloudTrail](https://aws.amazon.com/cloudtrail/) provides event history for your AWS account to help simplify security analysis, resource change tracking, and troubleshooting. In the context of security and compliance, CloudTrail can also assist with the non-repudiation process.\n\n## What is Non-Repudiation?\n\nNon-repudiation is a critical aspect in digital security that ensures a party involved in a communication cannot deny the authenticity of their engagement or the integrity of the data communicated.\n\n## How CloudTrail Assists in Non-Repudiation\n\nOne of the ways CloudTrail helps in non-repudiation is by recording all actions executed in the AWS Management Console and all API calls made in your AWS ecosystem. With this capability, it is possible to identify:\n\n- The specific user and AWS account that made a call to an AWS service\n- The source IP address from where the call was initiated\n- The exact timing of the call\n\nThis level of granular detail contributes to the effective tracking and auditing of all activities within the AWS environment, thereby supporting non-repudiation efforts.\n\n## AWS CloudTrail Record Contents\n\nThe captured data is made available in the AWS CloudTrail Record Contents. This record includes essential details such as:\n\n- Event timestamp\n- AWS region\n- Source IP address\n- Event name (e.g., 'DeleteKeyPair')\n- User identity (e.g., type, principal ID, ARN, account ID, access key ID, etc.)\n\nThese CloudTrail Record Contents provide comprehensive insight into all console actions and API calls, reinforcing transparency, accountability, and non-repudiation in the AWS ecosystem."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS control can lead to a number of costly consequences that can harm a business:\n\n1. **Security Breaches**: Without the use of AWS CloudTrail, you would not have a proper record of all activities made on AWS Management Console actions and API calls. This can lead to unidentified security leaks and breaches, which can cost businesses in data compromises, financial losses, and reputational damage.\n   \n2. **Regulatory fines and penalties**: Many industries are regulated by laws and industry standards that require audit trails as a means of ensuring integrity and confidentiality of data. Non-compliance of this control expose your organization to fines and penalties. \n\n3. **Investigative and recovery costs**: In the event of a security incident, without CloudTrail you lose the ability to investigate it adequately which could mean longer recovery times, more manpower and other resources needed.\n\n4. **Loss of customer trust**: If data breaches and security leaks come to light, it can result in loss of customer and stakeholder trust, which can further lead to lost revenue and a tarnished reputation.\n\n5. **Inability to identify fraudulent actions**: AWS CloudTrail makes it easier to spot any unauthorized activity or fraudulent actions made by users. Without the use of this control, these activities may go unnoticed.\n\nIn conclusion, it's important for AWS users to follow this control to prevent any potential loss and ensure security."
      ],
      "x-kaytu-usefulness-example": [
        "For example, suppose you're running a large scale business that heavily relies on AWS services - compute, storage, databases, and more. There are multiple teams with various levels of access to your AWS accounts. Unfortunately, some unintended changes were made to your databases that disrupted your business services.\n\nBy leveraging AWS CloudTrail, you can easily track down the cause of changes. You will be able to identify who made the changes, from which IP address the calls generated and when the changes were actually made. This could help you rectify the issue and also implement better policies for preventing similar occurrences in the future.\n\nHere is an instance of markup usage:\n\n```\n{\n\"eventVersion\": \"1.05\",\n\"userIdentity\": {\n    \"type\": \"IAMUser\",\n    \"principalId\": \"AIDAIO23MP6XMS7GDTWV4\",\n    \"arn\": \"arn:aws:iam::123456789012:user/Alice\",\n    \"accountId\": \"123456789012\",\n    \"accessKeyId\": \"ASIAIOSFODNN7EXAMPLE\",\n    \"userName\": \"Alice\"\n},\n\"eventTime\": \"2016-07-09T23:44:16Z\",\n\"eventSource\": \"dynamodb.amazonaws.com\",\n\"eventName\": \"DeleteItem\",\n\"awsRegion\": \"us-west-2\",\n\"sourceIPAddress\": \"205.251.233.182\",\n\"userAgent\": \"aws-sdk-java/1.11.48 Mac_OS_X/10.11.4 Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60\",\n\"requestParameters\": {\n    \"tableName\": \"ExampleTable\"\n},\n\"responseElements\": null,\n}\n```\nThis AWS CloudTrail log file excerpt shows that user `Alice` performed a `DeleteItem` action on the `ExampleTable` on DynamoDB service from the IP address `205.251.233.182` at `2016-07-09T23:44:16Z`. This recorded information can serve as an important piece of evidence in case of any investigations in the future."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_kms_cmk_rotation_enabled",
    "Title": "KMS CMK rotation should be enabled",
    "Description": "Enable key rotation to ensure that keys are rotated once they have reached the end of their crypto period.",
    "QueryID": "aws_kms_cmk_rotation_enabled",
    "DocumentURI": "policies/aws_kms_cmk_rotation_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/KMS"
      ],
      "x-kaytu-explanation": [
        "AWS Key Management Service (KMS) allows you to rotate the cryptographic material for your customer-managed keys in AWS KMS. The KMS Key rotation feature is designed to automatically rotate your key every year without requiring any action on your part.\n\nRotating cryptographic keys helps reduce the potential impact of a key compromise because data encrypted with a new key cannot be accessed with a previous key.\n\n# AWS KMS Key Rotation \n\n## Enable key rotation\n\nTo enable key rotation for a customer managed key, use the EnableKeyRotation operation.\n\n```markdown\naws kms enable-key-rotation --key-id 1234abcd-12ab-34cd-56ef-1234567890ab\n```\n## Verify key rotation status\n\nTo verify that key rotation is enabled for a particular key, use the GetKeyRotationStatus operation.\n```markdown\naws kms get-key-rotation-status --key-id 1234abcd-12ab-34cd-56ef-1234567890ab\n```\n\nThe output will provide the status of key rotation - whether it's enabled or not.\n\n## Disable Key Rotation \n\nIf needed, to disable the key rotation use the DisableKeyRotation operation.\n```markdown\naws kms disable-key-rotation --key-id 1234abcd-12ab-34cd-56ef-1234567890ab\n```\nRemember that when you disable rotation, AWS KMS stops rotating the key on its yearly schedule, but previous versions of the key are still available for decryption.\n\nThis is a simple technique that allows you to strengthen the security of your encrypted data by automatically rotating the KMS Key used. \n\nFor manual rotating keys or other advanced key rotation scenarios, AWS allows you to create new customer-managed keys and change the key specified in your applications, storage, and compute services.\n\nKey rotation is a fundamental part of cryptographic best practices. It is highly recommended to have it enabled for sensitive information. By doing this, you're maintaining a high security standard and protecting your data from potential key compromise."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance can be significant and varied depending on the specific circumstances. \n\n1. **Security Risks**: Not enabling key rotation could potentially expose your organization to security risks. If a key is compromised and it is not rotated, unauthorized individuals could potentially access sensitive data. This could result in a data breach, which leads to loss of confidential data and trust.\n\n2. **Financial Costs**: A data breach can also lead to severe financial repercussions. This can come in the form of fines from regulatory authorities - especially if you are operating in industries that handle sensitive user data like healthcare or finance. Plus, the cost of remediation and damage control following a data breach can also be substantial.\n\n3. **Regulatory Non-compliance**: Many industry guidelines and regulations such as GDPR, CCPA, and HIPAA mandate organizations to follow certain security practices which include key rotation. Disregarding these may lead to punitive actions, including hefty fines and sanctions.\n\n4. **Loss of Trust**: Failure to protect the data due to the absence of key rotation can result in loss of trust from customers and partners, affecting the reputation and business of an organization. \n\n5. **Business Continuity Threat**: If the keys are not rotated, there might be a chance of loss of access to critical business data and resources which might disrupt business operations.\n\nTo avoid these costs and to enhance security, it is strongly recommended to enable and adhere to the practice of key rotation in AWS."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, consider a large financial company which handles a lot of sensitive data related to their customers. The company is using AWS KMS (Key Management Service) to manage and store encryption keys meant for encrypting the customer data. Now, with time and extensive usage, these keys reach their end of crypto period which makes them vulnerable to brute force or other cyber attacks.\n\nTo enhance the security, the company enables key rotation feature of AWS. With this feature enabled, a new cryptographic material is created for the customer master keys (CMKs) every year, and all the new data is encrypted using the new cryptographic material. Old keys are still used to decrypt the older data, providing seamless access to historical data.\n\nThis key rotation doesn't require re-encrypting the old data and hence, doesn't disrupt the company processes. Also, every year, the risk window for compromising the data is reduced due to the rotation of keys. Further, AWS takes care of the secure and compliant deletion of the old keys, making it a hassle-free process for the company. Thus, key rotation proves useful in maintaining an up-to-date and secure encryption standard for the company."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_s3_bucket_logging_enabled",
    "Title": "S3 bucket logging should be enabled",
    "Description": "Amazon Simple Storage Service (Amazon S3) server access logging provides a method to monitor the network for potential cybersecurity events.",
    "QueryID": "aws_s3_bucket_logging_enabled",
    "DocumentURI": "policies/aws_s3_bucket_logging_enabled.md",
    "ManualVerification": false,
    "Severity": "low",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/S3"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "```markdown\n[Amazon Simple Storage Service (Amazon S3)](https://aws.amazon.com/s3/) server access logging is a feature that records all requests made to a specified S3 bucket. This data includes the requester's identity, the time of the request, the request type, the HTTP status, and error codes (if any). This feature can be extremely useful for security and access audits, as well as for understanding customer behaviors and troubleshooting issues.\n\nHere's how it works:\n\n1. **Enable Server Access Logging**: You must first enable server access logging for each S3 bucket for which you want to capture access logs. You can do this by modifying the properties of your S3 bucket via the AWS Management Console, AWS CLI, or SDKs.\n\n2. **Specify an S3 Bucket for Log Delivery**: After enabling, you need to specify another S3 bucket where AWS can deposit the log files. This bucket can be in the same AWS account as the source bucket or another account.\n\n3. **Monitor and Analyze Logs**: AWS will deliver access log records as log files to the target bucket. You can then use various AWS services like [Amazon Athena](https://aws.amazon.com/athena/), [Amazon QuickSight](https://aws.amazon.com/quicksight/), or [Amazon CloudWatch](https://aws.amazon.com/cloudwatch/) to analyze and visualize this data.\n\nEnabling Amazon S3 server access logging can be an essential part of your cybersecurity strategy as it provides insights into who is accessing what data and when. This can help you detect potential security incidents, validate access policies, and ensure compliance with regulatory standards.\n```\n"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the aforementioned AWS Control - Amazon Simple Storage Service (S3) server access logging - could result in the organization incurring various types of costs. Below are the potential costs, explained in markup language:\n\n1. **Financial Penalty**: \n   Non-compliance may lead to financial penalties if data is compromised due to inadequate security measures, especially when it goes against regulatory laws and standards like GDPR, HIPAA, or PCI DSS.\n\n2. **Data Breach**: \n    If server access logging is not enabled, cyber threats may go undetected and lead to data breaches. Remedial measures after the data breach can incur significant costs.\n\n3. **Reputation Damage**:\n    Companies can suffer severe reputational damage as non-compliance can result in data breaches that can be made public, damaging the trust of customers and potentially harming the company's sales and market value.\n\n4. **Loss of Business Opportunities**:\n    In severe cases, non-compliance can lead to the loss of business opportunities. Many businesses will avoid special relationships or deals with non-compliant organizations to prevent potential harm to their own reputation or legal penalization.\n\n5. **Investigation and Audit Costs**:\n    If a cybersecurity event does occur and goes undetected due to non-compliance with this control, the cost of investigation and the subsequent audit could be significant.\n\n6. **Legal Costs**:\n    If the non-compliance incurs a breach of any regulations or laws (such as GDPR, HIPAA, etc), the firm may be subject to legal actions. This may lead to costly lawsuits, which is not limited to financial damage but also includes loss in time and other resources.\n   \nIn conclusion, Amazon S3 server access logging is an important control for any organization to maintain in order to prevent any severe financial and reputational losses. It provides a method to monitor the network for potential cybersecurity events and act proactively in case of any threats detected. Non-compliance to this control will not only increase the risk profile of the organization but can also lead to significant tangible and intangible costs."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, an IT company hosting its applications on AWS might need to periodically audit its data access for security compliance. By enabling Amazon S3 server access logging, they can keep track of all requests made to their S3 bucket. This includes details like the requester, bucket name, request time, action taken, response status, and error codes if any.\n\nIf any suspicious activity occurs, such as an unusually high number of requests from a certain IP or requests attempting to access sensitive information, these can be easily flagged for review. This essential for identifying potential security risks such as data breaches, and hence, implementing the necessary security measures."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_user_access_key_age_90",
    "Title": "IAM user access keys should be rotated at least every 90 days",
    "Description": "The credentials are audited for authorized devices, users, and processes by ensuring IAM access keys are rotated as per organizational policy.",
    "QueryID": "aws_iam_user_access_key_age_90",
    "DocumentURI": "policies/aws_iam_user_access_key_age_90.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/IAM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Auditing Credentials\n\nThe primary purpose of this control is to ensure that the security credentials in Amazon Web Services (AWS) are audited regularly for authorized devices, users, and processes. This practice aims to maintain a high level of security within an AWS environment.\n\n## Key terms:\n\n- **Credentials**: These refer to the security access keys (Access Key ID and Secret Access Key) associated with an AWS account.\n- **IAM access keys**: IAM stands for Identity and Access Management. IAM access keys are one type of AWS credentials that are used to authenticate when interacting with AWS services programmatically.\n\n## Practice Explanation\n\nRegular audits on the credentials used within AWS is a best practice control process that minimizes security risks. The audit involves evaluating if the IAM access keys are rotated according to the organizational policy.\n\nRotation of IAM access keys means generating a new set of access keys (Access Key ID and Secret Access) and removing the old set. The scheduling and frequency of this rotation depend on the organization's policy but it's a good practice to rotate keys every 45 to 90 days.\n\nFurthermore, only authorized devices, users, and processes should have access to the IAM keys. This further reduces the risk of unauthorized activities.\n\nRegular auditing of credentials can help in:\n\n- Identifying unused AWS access keys that are best to be removed.\n- Spotting non-rotated access keys which pose a potential security risk.\n- Enforcing credential management best practices, such as regular key rotation.\n\nAll of these practices enhance the security of your AWS operations and data.\n\nIn summary, auditing AWS credentials and IAM access key rotation practices according to your organization's declared policy can help to ensure a secure AWS environment.\n\n## Code\n\nNo code is associated with this concept.\n\n## Related AWS Documentation\n\nFor further reading, refer to the following AWS documentation:\n\n- [Managing Access Keys for IAM Users](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html)\n- [Rotating Access Keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_RotateAccessKey)"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with AWS control for auditing credentials and enforcing the rotation of IAM access keys as per organizational policies can have serious cost and security implications. Here are some potential consequences:\n\n1. **Security Risks**: Non-compliance can lead to unauthorized access to sensitive applications and data stored in AWS environments. Hackers could exploit unchanged keys to infiltrate systems and access confidential information. This can result in significant data breaches or cyber-attacks.\n\n1. **Financial Loss**: A data breach could lead to substantial financial losses. In addition to the loss of sensitive proprietary or customer data, businesses might also have to bear the costs of incident response, PR crises management, liability claims, and any penalties imposed by regulatory authorities.\n\n1. **Non-Compliance Penalties**: The organization may be subject to fines, sanctions, or legal actions for failing to comply with regulations that require strict access control and accountability, such as GDPR, HIPAA, or SOX.\n\n1. **Loss of Trust**: If unauthorized access to systems leads to a data breach, the trust between the organization and its customers, vendors, or partners could be severely damaged. This could affect business relations and lead to a loss of business opportunities.\n\n1. **Audit Failure**: If non-compliance is discovered during an audit, it can result in failure to pass the audit that could lead to penalties, loss of certifications, and subsequently, potential loss of business.\n\nThe failure to rotate IAM access keys and audit credential use introduces vulnerabilities that can be avoided. To minimize risks and maintain the integrity of your AWS environments, it is crucial to comply with this control."
      ],
      "x-kaytu-usefulness-example": [
        "Instance of Usefulness:\n\nAt an e-commerce organization, there are quite a number of cross-functional teams that need access to various AWS resources to perform their duties. These teams access resources using the AWS IAM access keys. So, proper management of these keys is crucial.\n\nOne of the team members, John, had recently left the organization. He had access to critical resources of the organization as he was handling those as part of his duties. Even after he left, his IAM access keys were not disabled or deleted. \n\nLater there has been a data breach and it was discovered that the breach had happened using John's old IAM access keys. \n\nIf the organization had a policy to rotate IAM access keys and had enforced them strictly, such a data breach could have been avoided. \n\nAWS's control of auditing credentials for authorized devices, users, and processes by ensuring IAM access keys are rotated as per organizational policy can prove really useful in such situations. This control ensures that old, unused, and potentially vulnerable access keys are regularly replaced by fresh ones.\n\nThis could not only prevent unauthorized access but also make sure only the authorized and current users have the access to the organizational resources. Regular rotation of access keys will limit the damage if the access keys are compromised. \n\nThe organization can customize AWS IAM's access key rotation policy as per its specific needs and enforce it across the organization effectively."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_root_user_mfa_enabled",
    "Title": "IAM root user MFA should be enabled",
    "Description": "Manage access to resources in the AWS Cloud by ensuring MFA is enabled for the root user.",
    "QueryID": "aws_iam_root_user_mfa_enabled",
    "DocumentURI": "policies/aws_iam_root_user_mfa_enabled.md",
    "ManualVerification": false,
    "Severity": "critical",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/IAM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# Manage Access to Resources in AWS Cloud\n\nTo ensure the highest level of security for your AWS (Amazon Web Services) resources, it is crucial to enable MFA (multi-factor authentication) for the root user account.\n\nThis can be achieved by following these steps:\n\n## Enable MFA for Root User\n\n1. **Sign in to the AWS Management Console:** This will be done as the root user. \n\n2. **Navigate to your account:** You can do this by clicking on the avatar in the top right corner.\n\n3. **Click on 'Security Credentials':** This will present you with an overview of your security settings.\n\n4. **Navigate to Multi-Factor Authentication (MFA):** Click on the 'Edit' button.\n\n5. **Click on ‘Activate MFA’:** This will allow you to move to the next step, which is setting up the MFA device.\n\n6. **Choose either a virtual or hardware MFA device:** Follow the on-screen prompts to complete the MFA setup.\n\nBy enabling MFA for your root user account, AWS requires two or more separate forms of identification (password and a dynamically-generated code) to grant access, offering an additional layer of security. If an unauthorised individual gains your password, they will still be invalid without the current MFA code. \n\nRemember not to share your MFA device and to keep it secure at all times. \n\nThrough these actions, you will be better able to manage and control who has access to different resources in your AWS Cloud, drastically reducing the risk associated with unauthorized access."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the AWS control of managing access to resources in the cloud by ensuring Multi-Factor Authentication (MFA) is enabled for the root user can be quite significant. \n\n1. **Security Breaches**: If MFA is not enabled for the root user, there's an increased risk of unauthorized access to your AWS resources. This could lead to data breaches, with sensitive customer data potentially being exposed. Such a breach could have serious financial, legal, and reputational consequences for a company.\n\n2. **Financial Loss**: The unauthorized user could potentially start, stop, or terminate resources that could lead to financial loss. They could increase compute or storage resources unnecessarily, leading to higher costs. In worst-case scenarios, they could intentionally sabotage operations by deleting critical data or shutting down important processes.\n\n3. **Non-compliance penalties**: Depending on the industry and the jurisdiction your business operates in, non-compliance with certain security measures can result in hefty fines or penalties from regulatory bodies.\n\n4. **Loss of trust**: Sometimes, the damage done to a company's reputation following a data breach can be far more costly than any immediate financial loss. Customers trust companies with their data, and a breach of that trust could lead to loss of customers and business.\n\nIn conclusion, ensuring MFA for the root user is not just a best practice, it's critical for protecting your AWS cloud resources from unauthorized access. Non-compliance could result in massive costs both financially and operationally."
      ],
      "x-kaytu-usefulness-example": [
        "AWS multi-factor authentication (MFA) is a security system that requires more than one method of authentication from independent categories of credentials. It helps to prevent unauthorized access.\n\nExample Instance:\n\nImagine, if you are running a company that hosts a high-traffic, transaction intensive e-commerce application on the AWS cloud. The security of your application and the customer data it holds is paramount. You have various engineers and developers who need varying levels of access to the many AWS services and resources your application uses.\n\nBy enabling MFA for the root user, you add an additional layer of protection to your AWS account and resources. If a bad actor somehow acquires your root user credentials, they would be additionally required to pass MFA validation to access your account. This drastically reduces the likelihood of an unauthorized access, keeping your application and data safe.\n\nSo, this is how this control can be useful in ensuring the security of your AWS resources. \n\nExample in Markup:\n\n```\nTo increase the security for your AWS account, you should enable MFA for your root user. \n\n1. Login in to your AWS console.\n\n2. In the navigation pane, choose \"My Security Credentials\".\n\n3. Expand the Multi-Factor Authentication (MFA) section, then click \"Activate MFA\" button.\n\n4. Choose a MFA option, follow the instructions to set it up.\n\nAfter setting up MFA, whenever you or anyone else tries to log in as the root user, they will be required to enter an MFA code from the configured device. \nThis extra security measure will help to keep your AWS account more secure.\n```\n"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ebs_volume_in_backup_plan",
    "Title": "EBS volumes should be in a backup plan",
    "Description": "To help with data back-up processes, ensure your Amazon Elastic Block Store (Amazon EBS) volumes are a part of an AWS Backup plan.",
    "QueryID": "aws_ebs_volume_in_backup_plan",
    "DocumentURI": "policies/aws_ebs_volume_in_backup_plan.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/EBS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# Amazon Elastic Block Store (Amazon EBS) Data Backup Plan:\n\nBy including your Amazon EBS volumes in the AWS Backup plan, you can ensure the safety and integrity of your data stored on AWS.\n\n## Why use AWS Backup?\n\nAWS Backup is a cost-effective, centralized backup service that makes it easy to back up data across AWS services in the cloud and on-premises. It helps you automate and consolidate backup tasks that were previously performed separately, leading to reduced complexity and cost.\n\n## How to include Amazon EBS in AWS Backup Plan?\n\nBefore you can include Amazon EBS volumes in a backup plan, you must create a backup vault to store the backups.\n\n1. **Create a Backup Vault:** In the AWS Management Console, choose 'Backup vaults'. Choose 'Create Backup Vault', provide a name and choose 'Create backup vault'.\n\n2. **Create a Backup Plan:** Go to the 'Backup plans' and choose 'Create backup plan'. You can start from an existing plan or build a new one.\n\n3. **Specify Backup Details:** Input the necessary details such as Backup plan name, Backup rule name, Schedule, Lifecycle etc. \n\n4. **Assign Resources:** In the Assignment section, you will define what resources you want to back up. Here, choose 'Assign resources by groups of resource types'. Select Volume from the dropdown list and provide the volume ID of the EBS volume which you want to backup.\n\n5. **Create the Backup Plan:** After filling all details, click 'Create Backup Plan'. The plan will run as per the schedule and backup the specified Amazon EBS volume.\n\nBy following the steps above, you are ensuring that EBS volumes are part of an AWS Backup plan and can be restored in case of any M data losses."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with the control of ensuring that the Amazon Elastic Block Store (EBS) volumes are part of an AWS Backup plan can result in several potential costs:\n\n1. **Data Loss:** If EBS volumes are not backed up and a failure or error occurs (like accidental deletion), there may be loss of critical data. This can impact business operations if the data can't be recovered.\n\n2. **Monetary Costs:** Significant financial expense may result from efforts to recover lost data. If data loss leads to downtime, this could also result in financial losses due to disrupted business operations.\n\n3. **Compliance Violations:** If your industry is regulated and requires data retention and disaster recovery capabilities, you could face penalties, fines, or legal actions for non-compliance if you're not backing up EBS volumes.\n\n4. **Reputation Damage:** Data loss can severely hurt a company's reputation with customers and partners, leading to loss of business and trust.\n\n5. **Operational Setback:** The amount of time and manpower dedicated to restore the lost data (if even possible) and to rectify the situation could be substantial, leading to an operational setback. \n\nAs such, it is highly recommended to ensure that your EBS volumes are part of a reliable backup plan to mitigate the potential risks associated with data loss."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, consider a situation where an organization runs critical business applications on several EC2 instances with attached EBS volumes for application data. They need to retain this data for a specific time period to meet regulatory requirements and also to restore the application data in case of any disaster or accidental deletion.\n\nBy including the EBS volumes in an AWS Backup plan, they can automate the backup process which eliminates the manual efforts to create backups, reduces the risk of human errors, and conserves resources. They can define the backup frequency and retention period as per their needs in the Backup plan. AWS Backup ensures that the snapshot of EBS volumes is taken and stored in a secure and durable storage (like S3) from where it can be easily restored when needed.\n\nMoreover, AWS Backup integrates with AWS Organizations, providing a centralized way to manage backups across multiple AWS accounts, which simplifies the backup management in large scale deployment. Also, AWS Backup’s support for AWS Key Management Service (AWS KMS) provides data protection at rest.\n\nSo, by ensuring the EBS volumes are part of AWS's Backup plan, the organization can achieve operational efficiency, data security, compliance with regulatory requirements and peace of mind."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ec2_instance_ebs_optimized",
    "Title": "EC2 instance should have EBS optimization enabled",
    "Description": "An optimized instance in Amazon Elastic Block Store (Amazon EBS) provides additional, dedicated capacity for Amazon EBS I/O operations.",
    "QueryID": "aws_ec2_instance_ebs_optimized",
    "DocumentURI": "policies/aws_ec2_instance_ebs_optimized.md",
    "ManualVerification": false,
    "Severity": "low",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/EC2"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS EBS-optimized instances deliver dedicated throughput between Amazon EC2 (Elastic Cloud Compute) and Amazon EBS (Elastic Block Store), separate from other traffic. This results in consistent performance and low latency for I/O operations. Let's take a closer look in the markup version.\n\n```markup\n# EBS Optimized Instance in AWS \n\nAn [EBS optimized instance](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html) in **Amazon Elastic Block Store (Amazon EBS)** is a type of Amazon EC2 instance that provides additional, dedicated capacity for Amazon EBS I/O (Input/Output) operations. \n\nThis means it delivers **dedicated throughput** between Amazon Elastic Cloud Compute (Amazon EC2) and Amazon EBS, separate from all other traffic. \n\n## Benefits \n- **Consistent performance**: EBS optimized instances ensure that there is no interference with the I/O performance of your data, delivering consistent performance.\n- **Low latency**: These instances deliver dedicated network capacity for I/O operations, resulting in low latency.\n```\nThis easy-to-understand format presents the key information about EBS optimized instances in AWS and the benefits they provide."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS control specifying the use of an optimized instance in Amazon Elastic Block Store (Amazon EBS) can lead to multiple cost implications,such as:\n\n1. Performance Cost: Not using optimized instances for EBS can mean less dedicated capacity for EBS I/O operations. This can lead to decreased performance of your applications, making them slower. Therefore, your users may experience delays or difficulty using your product, which can lead to a significant loss of revenue or customer trust.\n\n2. Financial Cost: As performance issues increase, you may need to scale your resources more often to maintain performance standards. This could increase your financial costs, especially if you are scaling during peak times when prices are highest. Additionally, the constant need for manual intervention to maintain performance standards can increase your operational costs.\n\n3. Infrastructure Cost: Without an optimized instance, more resources (CPU/RAM) may be used for performing I/O operations, leaving less available for your application to use. This might lead to a need for purchasing additional instances, therefore increasing infrastructure costs. \n\n4. Risk Cost: An unoptimized instance for EBS lacks dedicated throughput for I/O operations and hence might risk not being able to handle peak loads, resulting in potential downtime or system failures. This can lead to further financial implications, not to mention damage to the organization's reputation and loss of customer trust. \n\nEach of these costs on its own can bring substantial financial strain to your organization. Combining the impact of all these costs, non-compliance to this AWS control could lead to some serious financial and performance repercussions."
      ],
      "x-kaytu-usefulness-example": [
        "Amazon Elastic Block Store (Amazon EBS) optimized instances are particularly useful for applications where the workload requires high throughput, like big data and analytics applications, high-performance databases, and transaction-heavy workloads. \n\nFor example, a company running a large relational database, such as PostgreSQL, might be experiencing high I/O wait time and slow queries due to a heavy number of read and write operations. In such scenarios, the company can use an EBS-optimized instance which provides dedicated capacity for Amazon EBS I/O operations ensuring a smoother and more efficient operation.\n\n```Markup\n\u003cresources\u003e\n    \u003cinstance type=\"r5.xlarge\"\u003e\n        \u003cname\u003ePostgreSQL Database Server\u003c/name\u003e\n        \u003cEBSOptimized\u003etrue\u003c/EBSOptimized\u003e\n        \u003cblockDeviceMapping\u003e\n            \u003cdeviceName\u003e/dev/sda1\u003c/deviceName\u003e\n            \u003cebs\u003e\n                \u003cvolumeSize\u003e500\u003c/volumeSize\u003e\n                \u003cvolumeType\u003egp2\u003c/volumeType\u003e\n                \u003cdeleteOnTermination\u003efalse\u003c/deleteOnTermination\u003e\n            \u003c/ebs\u003e\n        \u003c/blockDeviceMapping\u003e\n    \u003c/instance\u003e\n\u003c/resources\u003e\n```\n\nIn this markup, an r5.xlarge instance is being created for a PostgreSQL database which is EBS optimized. High-speed I/O operations are provided by assigning a dedicated storage (gp2 volume type) of 500 GB."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ec2_stopped_instance_30_days",
    "Title": "EC2 stopped instances should be removed in 30 days",
    "Description": "Enable this rule to help with the baseline configuration of Amazon Elastic Compute Cloud (Amazon EC2) instances by checking whether Amazon EC2 instances have been stopped for more than the allowed number of days, according to your organization's standards.",
    "QueryID": "aws_ec2_stopped_instance_30_days",
    "DocumentURI": "policies/aws_ec2_stopped_instance_30_days.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/EC2"
      ],
      "x-kaytu-explanation": [
        "This AWS Control checks if any Amazon Elastic Compute Cloud (EC2) instances have been stopped for more than a specified number of days. It could be useful for organisations that have standards regarding the maximum idle time for EC2 instances. \n\nHere is the rule description in markup format:\n\n```markdown\n## Rule: Amazon EC2 instances should not be stopped for more than [xyz] days\n\n**Description:** \n\nThis control checks if Amazon EC2 instances have been stopped for more than [xyz] days. This rule helps in maintaining the baseline configuration of Amazon EC2 instances as per your organization's standards.\n\n**Rationale:**\n\nInstances that are left in a stopped state for an extended period may indicate a more severe issue that requires attention. Additionally, they could be utilizing resources or costing your organization money without providing any benefit.\n\n**Remediation:**\n\nInvestigate any instances that are flagged by this rule. If they are stopped for a legitimate reason, consider creating a timeline for when they will be started again. If an instance is no longer needed, consider terminating it to free up resources.\n```\n\nYou need to replace [xyz] with the maximum number of days an EC2 instance should be stopped as per your organisation standards.\n\nNote: Remember, this rule does not automatically perform any actions on the instances. Its purpose is to alert and provide visibility into your organization's EC2 usage."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS Control (of checking if Amazon EC2 instances have stopped for more number of days than allowed) can have several cost implications:\n\n1. **Idle Resource Costs**: EC2 instances that are not being utilized but are still operational can lead to unnecessary costs. If not monitored, these costs can pile up over time. \n\n2. **Resource Management Inefficiency**: When unused instances are not terminated, it signals inefficiency in resource management. It means that resources are not being utilized optimally which could be functioning for worthwhile business operations.\n\n3. **Potential Security Risks**: Inactive instances can pose potential security vulnerabilities if not properly managed. If these instances are compromised, they could lead to additional costs to resolve the security threat.\n\n4. **Operational Inefficiency**: It also signifies operational inefficiency, as there may be instances running without serving any meaningful purpose. This is a wastage of computational resources and hence generates unnecessary costs.\n\nTherefore, remaining non-compliant with this AWS Control could end up incurring high unnecessary expenses, leading to inefficiency in operations, and potentially creating security vulnerabilities."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, a technology company XYZ uses a large number of Amazon EC2 instances for its application hosting. Due to fluctuating workloads, some instances might not be in active use all the time. In order to save costs, XYZ stops the instances which are not in use. But tracking and maintaining these instances manually can be a time-consuming and error-prone task.\n\nUnderstanding the importance of cost efficiency, XYZ enables this AWS rule to check whether any Amazon EC2 instances have been stopped for more than a defined number of days (say 30 days). If any instances are detected that have not been used for over 30 days, this rule triggers an alert to the AWS administrators at XYZ. \n\nThis allows the administrators to evaluate the instance's need and decide whether to terminate it permanently or restart it for potential use, enabling effective resource management and cost saving."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_s3_bucket_versioning_enabled",
    "Title": "S3 bucket versioning should be enabled",
    "Description": "Amazon Simple Storage Service (Amazon S3) bucket versioning helps keep multiple variants of an object in the same Amazon S3 bucket.",
    "QueryID": "aws_s3_bucket_versioning_enabled",
    "DocumentURI": "policies/aws_s3_bucket_versioning_enabled.md",
    "ManualVerification": false,
    "Severity": "low",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/S3"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "Amazon Simple Storage Service (Amazon S3) is a scalable, high-speed, low-cost web-based service designed for online backup and archiving of data and applications. One of the features of Amazon S3 is bucket versioning. It enables you to keep and retrieve all versions of an object (including all writes and deletes) in the same bucket.\n\nWith bucket versioning, you can preserve, retrieve, and restore every version of every object in your Amazon S3 bucket. This feature makes it easy to recover from both unintended user actions and application failures. You can also use versioning to archive objects so that you can access previous versions.\n\nHere's a piece of example markup:\n\n```markdown\n# Amazon S3 Bucket Versioning\n\nAmazon Simple Storage Service (Amazon S3) offers bucket versioning feature, this allows you to:\n- Store all versions of an object (including all writes and deletes) in the same bucket.\n- Easily recover from both unintended user actions and application failures.\n- Archive objects so that you can access previous versions.\n\nWith Amazon S3 bucket versioning, you can preserve, retrieve, and restore every version of every object. This ensures data durability and aids in recovering from both user-made mistakes and programmatic errors.\n```\nThis example is presented in Markdown language, a popular markup language that is a superset of HTML. Its key design goal is readability – that the language be understandable as-is, without looking like it's been written in code."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to Amazon S3 bucket versioning AWS Control could potentially lead to the following costs:\n\n1. **Data Loss**: Without versioning, if an object is accidentally deleted or over-written, there's no way to recover the previous version. This could lead to significant costs in terms of data loss.\n\n2. **Business Continuity**: For critical applications dependent on the data within the S3 bucket, any form of data loss or corruption could pose serious risks to business continuity.\n\n3. **Increased Operational Costs**: Without versioning, a lot more manual management might be required to keep backups of data objects. This could potentially translate to increased operational costs.\n\n4. **Security and Compliance Violations**: If your company is under regulations that require maintaining past versions of the data or being able to recover deleted data, not implementing versioning could lead to violations and hefty fines.\n\n5. **Disaster Recovery**: In the case of a disaster, not having versioning enabled might make the disaster recovery process more complex and costly. \n\nPlease note that while versioning helps keep multiple versions of an object, it also increases storage costs as AWS charges for each stored version of an object. Hence, a balance should be struck considering the criticality of the data being stored in the S3 bucket."
      ],
      "x-kaytu-usefulness-example": [
        "Amazon S3 bucket versioning is extremely useful in scenarios where the preservation of all versions of an object is important. \n\nFor instance, consider a scenario where a graphic design company is working on a logo for a client. The designers create several versions of the logo over time, each replacing the previous one. However, the client may wish to revert back to an earlier version of the logo. If the company uses a regular storage system, chances are that the older versions are lost forever. This is where Amazon S3 bucket versioning becomes useful.\n\nBy enabling versioning, the designers can save each version of the logo separately in the same Amazon S3 bucket. Even if they overwrite an old version, the system keeps all versions. The client can then review all versions at any time, and if they wish, they can choose an older version of the logo. Thus, S3 bucket versioning ensures that no version is permanently lost, making it a valuable tool for any business or project that requires version control."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_sns_topic_encrypted_at_rest",
    "Title": "SNS topics should be encrypted at rest",
    "Description": "To help protect data at rest, ensure that your Amazon Simple Notification Service (Amazon SNS) topics require encryption using AWS Key Management Service (AWS KMS).",
    "QueryID": "aws_sns_topic_encrypted_at_rest",
    "DocumentURI": "policies/aws_sns_topic_encrypted_at_rest.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/SNS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Data Protection with Amazon SNS Topic Encryption\n\nEnsuring the security and privacy of data in the cloud is vital. It protects your sensitive information from unauthorized access and potential data breaches. A key component of this is protecting your data at rest, i.e., data stored on physical media. On AWS, one effective way to accomplish this is by encrypting your Amazon Simple Notification Service (SNS) topics using AWS Key Management Service (KMS).\n\n## What is Amazon SNS?\n\nAmazon SNS is a fully managed service that allows you to send messages to multiple subscribers through a 'topic'. It serves as a pub/sub messaging service for microservices, distributed systems, and serverless applications. \n\n## What is AWS KMS?\n\nAWS Key Management Service (KMS) gives you centralized control over the cryptographic keys used to protect your data. The service is integrated with other AWS services making it easier to encrypt data and manage keys.\n\n## Why Encrypt Amazon SNS Topics?\n\nBy encrypting your Amazon SNS topics with AWS KMS, you ensure that the data within the messages being sent from publishers to subscribers is not accessed by unauthorized parties, either intentionally or accidentally. This further strengthens your organization's data security and helps meet regulatory requirements.\n\n## How to Enable Encryption?\n\n1. **Login to AWS Management Console**: Navigate to the Amazon SNS service.\n2. **Create a Topic or Select Existing Topic**: You can either secure a new SNS topic or apply encryption to an existing one.\n3. **Enable Encryption**: Under the topic details, find the 'Encryption' section. Select AWS KMS from the dropdown menu.\n4. **Choose a KMS Key**: Select the appropriate AWS KMS key from the dropdown. You can either use the default AWS managed key (AWS Managed Key `aws/sns`) or select a custom KMS key.\n\nBy ensuring encryption for your Amazon SNS topics using AWS KMS, you significantly enhance the security posture of your data at rest in AWS."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance with this AWS control could be immense and might manifest in the following ways:\n\n1. **Data Breaches:** Unencrypted data is vulnerable and can be easily compromised, leading to data breaches. In 2020, the average cost of a data breach was $3.86 million according to a report by IBM. This amount can be even higher for large organizations dealing with sensitive data like financial or healthcare industries.\n\n2. **Non-compliance fines:** Various industries have standards and regulations that require encryption at rest (HIPAA, PCI DSS, GDPR, etc.). Failing to comply with these regulations can result in substantial financial penalties. For instance, under GDPR, fines can reach up to 20 million Euros or 4% of the company's global annual revenue, whichever is higher.\n\n3. **Loss of customer trust:** Besides the financial impact, failure to protect sensitive customer data can result in loss of customer trust and reputational damage. Depending on the scale of the breach, this could potentially lead to loss of business, decreasing profits.\n\n4. **Legal and litigation expenses:** In case of a data breach, the affected parties might choose to sue the organization for damages, leading to potential legal and litigation costs.\n\nFor these reasons, employing AWS Key Management Service (AWS KMS) for encryption of your Amazon SNS topics is an essential security measure that shouldn't be neglected."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, consider you are developing a healthcare application that uses Amazon Simple Notification Service (SNS) to send notifications about patient data to different components of the application. To comply with regulations like HIPAA (Health Insurance Portability and Accountability Act), you are required to ensure the security and privacy of all electronic Protected Health Information (ePHI).\n\nBy enforcing encryption using AWS Key Management Service (KMS), you can safeguard the patient information stored in your Amazon SNS topics. AWS KMS uses strong encryption mechanisms which protect this sensitive data from unauthorized access. Therefore, if a breach were to occur, without the correct encryption key, the information accessed would be indecipherable and useless to the attacker. \n\nThis not only aids in compliance with legal regulations, but also sends a message to your users that their personal information is secure, building trust and reliability in your application. Hence, ensuring that your Amazon SNS topics require encryption with AWS KMS is a valuable step in maintaining data integrity and privacy."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_wafv2_web_acl_logging_enabled",
    "Title": "Logging should be enabled on AWS WAFv2 regional and global web access control list (ACLs)",
    "Description": "To help with logging and monitoring within your environment, enable AWS WAF (V2) logging on regional and global web ACLs.",
    "QueryID": "aws_wafv2_web_acl_logging_enabled",
    "DocumentURI": "policies/aws_wafv2_web_acl_logging_enabled.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/WAFv2"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS WAF (Web Application Firewall) is a security service that protects web applications against common web exploits like SQL injection and cross-site scripting. By enabling logging on your regional and global web ACLs (Access Control Lists), you can capture detailed information about traffic patterns and threats detected by AWS WAF.\n\nHere are the steps to enable AWS WAF logging:\n\n```\n**1. Create an Amazon Kinesis Data Firehose delivery stream:**\n   AWS WAF can directly send logs to this delivery stream.\n\n**2. Create an S3 bucket:**\n   This is the place where your logs will be stored. Make sure it's in the same region as the Kinesis Data Firehose delivery stream.\n\n**3. Enable Logging in AWS WAF:**\n   Go to the AWS WAF console --\u003e Select \"AWS WAF\" --\u003e Click on \"Web ACLs\" and choose the specific ACL --\u003e Under \"Logging\", click \"Edit\" --\u003e Choose \"Kinesis Data Firehose delivery stream\" and select the one you created --\u003e Specify the \"Log destination\" as the bucket you created --\u003e Click \"Create\".\n\n**4. Review the Logs:**\n   Now, AWS WAF will start logging web ACL traffic and you can access these logs in your S3 bucket for further analysis.\n```\nBe aware that logging can create a significant amount of data, which may incur additional costs.\n\nRemember that the above practices help to actively monitor and secure your environment. Appropriate analysis of the collected logs can help detect and respond to security incidents more promptly."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS Control can have several costs including but not limited to:\n\n1. **Security Threats:**\n   If AWS WAF (V2) logging is not enabled, potential threats or malicious activities could go unnoticed. This puts the integrity and confidentiality of your system at risk.\n\n2. **Inefficient Damage Control:**\n   Without logging, it becomes difficult to identify and troubleshoot issues happening in the environment. This can cause longer than necessary downtime affecting client service and productivity.\n\n3. **Regulatory Consequences:**\n   Industries with strict data regulations might face fines and penalties for not complying with logging and monitoring requirements.\n\n4. **Loss of Visibility:**\n   WAF logs give visibility into traffic patterns and access attempts. Lack of logs can lead to unawareness about the user behavior and usage trends.\n\n5. **Audit Failures:**\n   If your industry requires regular audits of access logs and protective measures, non-compliance can lead to audit failure.\n\n6. **Reputation Damage:**\n   If a security breach occurs due to non-compliance, it can damage the reputation of the company with its clients and stakeholders, leading to loss of business. \n\nImplementing AWS control such as enabling WAF (V2) logging on regional and global web ACLs, is therefore crucial to avoid such implications."
      ],
      "x-kaytu-usefulness-example": [
        "For example, a global ecommerce company has a presence across multiple regions and uses AWS to host its website and applications. This setup means their web access control lists (ACLs) are both global and regional. \n\nGiven the high number of users and transactions happening daily, it's critical for the company to track every request coming into their system for analysis, troubleshooting, and auditing. Here, enabling AWS WAF (V2) logging on their regional and global web ACLs proves to be of utmost importance.\n\nA potential use case is as follows:\n\n```\n1. The website experiences an unexpected surge in traffic from a specific region, causing server resources to be overwhelmed and the website to slow down.\n\n2. The company doesn't immediately recognize the nature of this traffic pattern. Is it a genuine increase in customers, or potentially a DDoS attack?\n\n3. As AWS WAF (V2) logging was enabled on both regional and global web ACLs, the company can now analyze the incoming traffic in detail.\n\n4. After examining the logs, the company finds out a majority of the requests have been made from a single IP address within a short time frame, which indicates it is likely a DDoS attack.\n\n5. Based on these collected logs, the company could now implement immediate measures to mitigate the attack, like blocking that particular IP address or rate limiting the requests.\n\n6. Without the AWS WAF logging, this possible cyber attack would potentially have gone unnoticed until it inflicted significant damage, leading to substantial financial and reputational losses.\n```\n\nAWS WAF logging enables the capturing of detailed information about web request traffic patterns, making it a useful tool for diagnosing potential security threats, performance issues, and maintaining regulatory compliance."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_redshift_cluster_kms_enabled",
    "Title": "Amazon Redshift clusters should be encrypted with KMS",
    "Description": "Ensure if Amazon Redshift clusters are using a specified AWS Key Management Service (AWS KMS) key for encryption. The rule is compliant if encryption is enabled and the cluster is encrypted with the key provided in the kmsKeyArn parameter. The rule is non compliant if the cluster is not encrypted or encrypted with another key.",
    "QueryID": "aws_redshift_cluster_kms_enabled",
    "DocumentURI": "policies/aws_redshift_cluster_kms_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/Redshift"
      ],
      "x-kaytu-explanation": [
        "This AWS Control ensures that the data in your Amazon Redshift clusters are encrypted using a specified AWS KMS key. AWS KMS is a managed service that makes it easy for you to create and manage cryptographic keys, and uses these keys to protect your data.\n\nHere's how the control is evaluated:\n\n```markdown\nCompliant: \n- If encryption is enabled on your Redshift clusters, and\n- These clusters use the specific AWS KMS key defined in the `kmsKeyArn` parameter for encryption.\n\nNon-compliant: \n- If the Redshift clusters are not encrypted, or \n- If the clusters use a different AWS KMS key for encryption (other than the one specified in the `kmsKeyArn` parameter).\n```\n\nThis control is crucial to ensure your data's security. AWS provides robust controls over KMS keys, and adopting the practice of using a specified AWS KMS key provides greater control over who can access your data stored in the Redshift clusters."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS Control could have financial, reputational, and legal implications. \n\n1. Financial costs: \nIf your data is breached as a result of not implementing appropriate encryption, it can result in significant financial penalties. Data breach can result in loss of crucial or sensitive business or customer data, and your business may end up paying a high price in legal settlements or regulatory penalties.\n\n2. Reputational damage: \nA company's reputation can suffer a severe hit due to inadequate data protection procedures which could lead to data breaches. Clients, customers and partners may lose trust in your ability to secure their data which in turn could lead to loss of customers and revenue. \n\n3. Legal implications: \nThere can be serious legal implications. Non-compliance to the AWS control can be a breach of laws like the General Data Protection Regulation (GDPR) and others depending on the jurisdiction of the business operation. \n\n4. Correction costs: \nIn the case of non-compliance, swift remediation actions will need to be pursued in order to correct the issue and re-attain compliance. This process will involve the business spending resources which can be costly.\n\nAlso, in case of an audit, if your AWS control is found non-compliant with this rule, you may have to bear the costs associated with failed audits. \n\nSpecifically to AWS Redshift, using a specified AWS KMS key for encryption ensures that your data is secure both in transit and at rest. This control plays a crucial role in maintaining data confidentiality and security."
      ],
      "x-kaytu-usefulness-example": [
        "**Example**\n\nA software company uses AWS Redshift clusters for storing and managing its business data. The company has a strict data security policy that requires all data storage services to use a particular AWS Key Management Service (KMS) key for encryption.\n\nTo ensure this policy is adhered to, the company uses this AWS control. The control automatically checks whether encryption is enabled on all Redshift clusters and if they're encrypted using the specified KMS key. \n\nFor instance, when they set up a new cluster, the AWS Control system generates a compliant status if the cluster setup complies with the given encryption rules - that is, the new cluster is encrypted and is using the specified KMS key. \n\nIf a cluster is found to not be encrypted, or it's encrypted but not using the specified KMS key, the control generates a non-compliant status. This triggers an alert to the company's security team, who then take action to correct the issue, ensuring their data remains secure in compliance with their security policy. \n\nBy using this AWS control, the company is able to maintain a robust, automated enforcement of their security policy and ensure ongoing data protection in their AWS Redshift clusters."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_es_domain_logs_to_cloudwatch",
    "Title": "Elasticsearch domain should send logs to CloudWatch",
    "Description": "Ensure if Amazon OpenSearch Service (OpenSearch Service) domains are configured to send logs to Amazon CloudWatch Logs. The rule is compliant if a log is enabled for an OpenSearch Service domain. This rule is non compliant if logging is not configured.",
    "QueryID": "aws_es_domain_logs_to_cloudwatch",
    "DocumentURI": "policies/aws_es_domain_logs_to_cloudwatch.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ES"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "## AWS Config Rule - OpenSearch Log to CloudWatch\n\nThis AWS Config rule is designed to determine whether Amazon OpenSearch Service domains are configured to send logs to Amazon CloudWatch Logs. \n\n**Compliance Criteria:**\n- The rule is assessed as `COMPLIANT` if logs are being sent from an OpenSearch Service domain to Amazon CloudWatch Logs.\n- The rule is evaluated as `NON_COMPLIANT` if an OpenSearch Service domain does not have logging configured to send logs to Amazon CloudWatch Logs.\n\n**Rationale:**\n\nLogging is a critical component of system monitoring and incident response. By ensuring that OpenSearch Service domains are configured to send logs to Amazon CloudWatch Logs, it aids in the detection and troubleshooting of potential issues or unusual activity within the Amazon OpenSearch Service environment.\n\n**Remediation:**\n\nTo resolve a `NON_COMPLIANT` status, configure logging for the OpenSearch Service domain to send logs to Amazon CloudWatch Logs. \n\n1. Navigate to the Amazon OpenSearch Service console.\n2. Select the OpenSearch domain that you want to update.\n3. Under `Domain configuration`, choose `Edit`.\n4. Enable the desired log type: Index Logs, Search Slow Logs, or Error Logs, and point them to a CloudWatch Log Group.\n5. Save changes.\n\n**Useful References:**\n- [Setting up Amazon OpenSearch Service: Configuring CloudWatch Logs](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/managedomains.html#managedomains-logging)\n- [Amazon CloudWatch Logs](https://aws.amazon.com/cloudwatch/features/#CloudWatch_Logs)"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS Control: \"Ensure if Amazon OpenSearch Service (OpenSearch Service) domains are configured to send logs to Amazon CloudWatch Logs\" can result in significant costs, specifically:\n\n1. **Operational Costs**\n   If logging is not enabled on Amazon OpenSearch Service domains and the information is sent to Amazon CloudWatch, it can be difficult to monitor the operational health of the service, troubleshoot issues efficiently, or identify service degradation or failures patterns. This could lead to higher operational costs as troubleshooting and system restoration may take more time and resources.\n\n2. **Security Costs**\n   Lack of logging also means security incidents may remain unnoticed. If there is an unauthorized attempt to access the service, it may not be registered or tracked, potentially leading to data breaches. The financial costs associated with damages from a security breach could be vast, not to mention the non-monetary costs such as reputation damage.\n\n3. **Compliance Costs**\n   If the organisation is subject to IT governance compliance regulations which require activity logging, non-compliance could result in hefty fines and penalties. In addition, the organisation might need to undergo the costs associated with making systems compliant after an audit failure.\n\n4. **Forensic Costs**\n   Logs are usually the first place to look during any incident investigation. Lack of logs or inadequate logging means that the investigators will find it difficult and time-consuming to determine what exactly happened, potential root causes, and who was responsible.\n\nTherefore, non-compliance to this control can, directly and indirectly, increase various costs and risks. It's critical to ensure that OpenSearch Service domains are properly configured to send logs to CloudWatch."
      ],
      "x-kaytu-usefulness-example": [
        "The OpenSearch Service in Amazon AWS is an instance where users can analyze and visualize their machine learning data in real-time. The data that are gathered need to be logged and tracked to ensure the smooth operations of the systems. However, at times, these systems can often result in errors or issues which may affect the overall usability. \n\nHere's a simple example:\n\nAssume you are running an ecommerce website and you're using AWS's OpenSearch Service to analyze user behaviors and interactions on your site to improve your product recommendations which has a significant impact on your sales. \n\nOne day, you notice that the recommendations on your website are not as accurate as they used to be, leading to a drop in your sales. To troubleshoot and resolve the issue, you decide to check the logs. But you realize that the logging was not configured so there is no way to trace back where the issue originated. \n\nIn this case, if the OpenSearch Service domains were configured to send logs to Amazon CloudWatch Logs, it would have helped in identifying the issue quickly. The logs would have provided a trace of all the events and operations related to the data and the recommendations. \n\nTherefore, ensuring that logging is enabled for OpenSearch Service domains becomes critical to efficiently track, manage, and resolve system or operational issues. If a log is not enabled for an OpenSearch Service domain, the rule is considered non-compliant which would necessitate immediate rectification or attention. Hence, it is highly useful to ensure that OpenSearch Service domains are configured to send logs to Amazon CloudWatch Logs."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_kms_key_not_pending_deletion",
    "Title": "KMS keys should not be pending deletion",
    "Description": "To help protect data at rest, ensure necessary customer master keys (CMKs) are not scheduled for deletion in AWS Key Management Service (AWS KMS).",
    "QueryID": "aws_kms_key_not_pending_deletion",
    "DocumentURI": "policies/aws_kms_key_not_pending_deletion.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/KMS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS Key Management Service (KMS) is a managed service that enables you to easily create and control the encryption keys used to encrypt your data.\n\nA customer master key (CMK) is a cloud resource that you use in AWS KMS to create, rotate, disable, and define access permissions for all your cryptographic operations. Keys used to encrypt and decrypt sensitive data should never be deleted to maintain continued access to the data.\n\nThe AWS control in question is about ensuring that CMKs are not scheduled for deletion. If a CMK is deleted, it can lead to loss of data, as the deleted CMK cannot be reused to decrypt the data it once encrypted.\n\nHere's the AWS control in markup format:\n\n```\n# AWS Control - Protect Data at Rest\n\nTo protect your data at rest in AWS, it's important to ensure that customer master keys in AWS Key Management Service (KMS) are **not scheduled for deletion**. Deleting a CMK can lead to loss of data, as the deleted CMK cannot be reused to decrypt the data it once encrypted.\n\n## Steps to Validate\n\n1. Open the AWS Management Console.\n2. In the navigation pane, locate and open KMS.\n3. Select a CMK and review its details.\n4. Ensure that the CMK's deletion schedule is either disabled or not set to a near date.\n\nIf you find a CMK that is scheduled to be deleted, make sure to cancel the deletion, especially if the keys are still needed for decryption. \n```\n   \nThis markup document gives an overview of the control and how to check if a CMK is scheduled for deletion."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to this AWS control - \"To help protect data at rest, ensure necessary customer master keys (CMKs) are not scheduled for deletion in AWS Key Management Service (AWS KMS)\" can be significant and multifaceted. \n\n1. **Data Unavailability or Data Loss**: If the keys used to encrypt your data are deleted, you would not be able to decrypt the data. This could potentially lead to permanent data loss when backups are also encrypted using the same keys.\n\n2. **Operational Impact**: If applications, microservices or functions which depend on these keys are unable to locate them or if the keys are not accessible, your service availability might be affected which can lead to business loss. \n\n3. **Regulatory Violations**: Non-compliance could potentially lead to regulatory infractions if the deletion of these keys put you out of compliance with certain regulations (e.g GDPR, HIPAA) that require data to be encrypted at rest, mostly if sensitive or personal data is involved. \n\n4. **Financial Consequences**: Non-compliance can result in financial loss due to business disruption, data loss or regulatory fines. \n\n5. **Reputation Damage**: This can undermine customer trust in your ability to secure their data, which can result further in client loss and damage to business reputation. \n\nTo avoid these costs, ensure you have strong processes around key management including never scheduling necessary keys for deletion, regular audits of your AWS KMS, and ensuring that you are following best practices for key management and encryption."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, a financial institution is using AWS cloud solutions to manage its critical data. Confidential information such as account details, transaction records, and customer profiles are stored in this infrastructure. The institution uses AWS Key Management Service (AWS KMS) to manage the cryptographic keys used for database encryption.\n\nHowever, if CMKs (customer master keys) are accidentally scheduled for deletion, all data encrypted under these keys could potentially be lost. The company would lose access to its critical information permanently, as there is no way to recover a deleted CMK or the data encrypted under it.\n\nBy ensuring necessary CMKs are not scheduled for deletion, the institution can prevent accidental data loss and maintain continuous, secure access to its critical data. This would also allow the company to stay compliant with financial regulations regarding data management and security. \n\nFor example, their IT team could perform routine audits for any CMKs scheduled for deletion and take necessary measures to cancel the deletion, further enhancing the company's data protection measures."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_s3_public_access_block_bucket",
    "Title": "S3 public access should be blocked at bucket levels",
    "Description": "Ensure if Amazon Simple Storage Service (Amazon S3) buckets are publicly accessible. This rule is non compliant if an Amazon S3 bucket is not listed in the excludedPublicBuckets parameter and bucket level settings are public.",
    "QueryID": "aws_s3_public_access_block_bucket",
    "DocumentURI": "policies/aws_s3_public_access_block_bucket.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/S3"
      ],
      "x-kaytu-explanation": [
        "This AWS (Amazon Web Service) Control is related to the Amazon Simple Storage Service (Amazon S3).\n\nThe control ensures that any Amazon S3 bucket is not publicly accessible. When an S3 bucket is publicly accessible, it means that anyone on the internet can access, view, or download the data stored in the bucket. This might lead to unauthorized access to the data, leakage of sensitive data, and violation of data privacy laws.\n\nThe compliance rule mentioned means that there's a parameter named `excludedPublicBuckets` in which you can list the buckets that you intend to make publicly accessible. This is useful in cases where public access to certain data stored in Amazon S3 buckets is necessary.\n\nHere's how this rule works:\n\n- Check Amazon S3 bucket's access settings.\n- If the bucket is listed in the `excludedPublicBuckets` parameter, the bucket can be public. It's compliant.\n- But, if the bucket is not listed in the `excludedPublicBuckets` parameter, and yet, its access setting is set to public, then it's not compliant. The AWS Control flags it as a violation.\n\nThis control is significant for ensuring that sensitive data stored in Amazon S3 does not become publicly accessible inadvertently.\n\n```markdown\n### AWS Control: Public Access to Amazon S3 Buckets\n\n* **Purpose**: To ensure that Amazon S3 buckets are not publicly accessible.\n* **Compliance Rule**: An Amazon S3 bucket is non-compliant if it's not listed in the `excludedPublicBuckets` parameter and yet, it is set to be publicly accessible.\n  * A bucket listed in `excludedPublicBuckets` can be public.\n  * A bucket not listed in `excludedPublicBuckets` but having public access settings is flagged as non-compliant.\n* **Importance**: Prevents unauthorized data access, data leakage, and violation of data privacy laws.\n```"
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the AWS Control that mandates that no AmazonSimple Storage Service (Amazon S3) bucket should be publicly accessible could entail a range of potential negative consequences:\n\n1. **Data Leakage**: If S3 buckets are publicly accessible, they become potential touchpoints for unauthorized access. Malevolent actors can potentially access, edit, delete, or leak private data, resulting in financial losses, brand reputation damage, and liabilities if the data pertains to clients or customers.\n\n2. **Compliance Violation Penalties**: Companies dealing with sensitive data (like health records, credit card details, or personal identifiable information) are often obliged to adhere to certain data governance and privacy standards (like HIPAA, GDPR, or PCI DSS). Non-compliance to these data protection standards can result in heavy financial penalties and revocation of business licenses. \n\n3. **Loss of Customer Trust**: Discovery of public exposure of sensitive customer data can lead to customers losing faith in the business, thereby affecting client retention and future business.\n\n4. **Increased Attack Surface**: Publicly accessible buckets increase the attack surface of your environment, making you more vulnerable to cyber threats.\n\n5. **Legal Consequences**: In some cases, unauthorized exposure of certain data could also result in legal actions, from both affected clients and regulatory authorities, leading to financial and reputational harm.\n\nTo proportionally quantify the cost, companies would need to consider potential direct losses from breaches or compliance fines, opportunity costs from loss of customer trust, and also indirect costs like increased security investments or legal fees."
      ],
      "x-kaytu-usefulness-example": [
        "As an example of its usage, we will take a case where a user runs a website where they share high-resolution images. These images have been stored in an Amazon S3 bucket and the web pages use direct links to these images. However, the user wants to ensure that only the images can be accessed by the public, but not the other files in the bucket, such as backup files, original raw files, etc.\n\nIn this case, the user can use the mentioned AWS Control to exclude the public images bucket from being marked as non compliant, i.e., they would add this bucket to the `excludedPublicBuckets` parameter. This way, the user is following the principle of least privilege access, allowing public access only where it is absolutely necessary, thus achieving a good balance between accessibility and security.\n\nHere is how a user can do this in AWS Management Console:\n\n```markup\n1. Open the AWS Management Console, then navigate to `AWS Config`.\n2. In the AWS Config dashboard, click on `Add rule`.\n3. In the `Add AWS Config rule` page, search for `s3-bucket-public-read-prohibited` rule in the `Managed rules` tab.\n4. In the `Rule parameters` section, add the name of the S3 bucket that stores the publicly accessible images in the `excludedPublicBuckets` field.\n5. Click on `Save`.\n```\n\nThis instance demonstrates the importance of this AWS Control: it helps users to keep their S3 buckets secure, by allowing them to clearly define and control which buckets need to be publicly accessible, and which ones should remain private, thereby preventing potential data breaches or unauthorized access."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_root_user_no_access_keys",
    "Title": "IAM root user should not have access keys",
    "Description": "Access to systems and assets can be controlled by checking that the root user does not have access keys attached to their AWS Identity and Access Management (IAM) role.",
    "QueryID": "aws_iam_root_user_no_access_keys",
    "DocumentURI": "policies/aws_iam_root_user_no_access_keys.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/IAM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS IAM is a web service that helps you securely control access to your AWS resources. It allows you to control who is authenticated (signed in) and authorized (has permissions) to use your AWS resources.\n\nWithin this context, the root user is the initial account user that gets created when you first set up your AWS account, they have complete access to all AWS services and resources.\n\nAWS best practice is to restrict the use of the root user for only necessary tasks, not for everyday use. Therefore, it is usually recommended to not attach access keys to the root user IAM role. This means the ability for the root user to perform tasks is typically done by logging into the AWS Management Console, not via programmatic access (e.g. command line).\n\nIf the root user does not have access keys attached to their IAM role, it helps to limit access to systems and assets, preventing unauthorized or unintended access. This is a standard security practice that can help protect your AWS resources.\n\nTherefore, controlling access to systems and assets can be managed by checking that the root user does not have access keys attached to their IAM role."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance with this AWS control can be significantly high and can include:\n\n1. **Security Breaches**: The root user has unrestricted access to all resources of an AWS account. If access keys are attached to the root user and these keys become compromised, an attacker could have full control over your AWS resources leading to data breaches or loss.\n\n2. **Unwanted Costs**: An unauthorized entity with root access can start, stop, or change any services, create new instances, and make other unauthorized changes that could incur high costs.\n\n3. **Best Practice Violations**: AWS recommends creating individual IAM users and granting them the least privilege necessary to perform their tasks, rather than using the root user for routine access. Non-compliance with this control is a clear violation of AWS best practices, which could lead to organizational risks related to governance, risk management, and compliance.\n\n4. **Audit Failures**: If your organization falls under regulatory standards like HIPAA, GDPR, or any other standard which requires regular audits, such non-compliance may lead to audit failures, legal consequences, fines, and potential business loss.\n\nIn summary, the cost of non-compliance to this control is not just financial but also includes potential damage to a company's reputation, operational efficiency, and regulatory stature."
      ],
      "x-kaytu-usefulness-example": [
        "For example, in an organization using AWS services, there is a requirement to ensure that the root user, which has substantial privileges, doesn't have access keys attached to their IAM role for better security management. This AWS control is beneficial in this context to prevent misuse of privileged access which can lead to significant security risks.\n\nLet's consider a scenario where a company's critical data is stored on AWS S3 buckets. If the root user has access keys associated, it's possible that those keys might be stolen or accidently exposed, allowing unauthorized access and potential manipulation of this crucial data. By ensuring that the IAM roles do not have root user attached access keys, this organization can add an extra layer of security to protect their data.\n\n```\n\u003cSecurityControl\u003e\n  \u003cControl\u003eAccess to systems and assets\u003c/Control\u003e\n  \u003cImplementation\u003eCheck the root user does not have access keys associated with their AWS IAM role\u003c/Implementation\u003e\n  \u003cBenefit\u003ePrevents misuse of privileged access and unintended exposure of important data by adding an extra protective layer\u003c/Benefit\u003e\n\u003c/SecurityControl\u003e\n```\nThis XML styled markup format illustrates the AWS control about controlling access to systems and shows how to implement it, along with the benefits."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_dynamodb_table_in_backup_plan",
    "Title": "DynamoDB tables should be in a backup plan",
    "Description": "To help with data back-up processes, ensure your Amazon DynamoDB tables are a part of an AWS Backup plan.",
    "QueryID": "aws_dynamodb_table_in_backup_plan",
    "DocumentURI": "policies/aws_dynamodb_table_in_backup_plan.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/DynamoDB"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS Backup is a highly scalable and centrally managed backup service that simplifies the process of backing up Amazon DynamoDB tables. By incorporating your DynamoDB tables in an AWS Backup plan, you can automate backup tasks traditionally performed through custom scripts. \n\nThis would allow you to save, comply with policies and efficiently monitor your backups in AWS environments, creating a reliable disaster recovery solution and ensuring that your data is always protected. \n\nBelow is the markup code demonstrating the process:\n\n```markdown\n# AWS Backup Plan for DynamoDB Tables\n\n## Overview\nAWS Backup provides a centralized approach to manage, schedule, and retain backups on AWS. Including DynamoDB tables in this plan will ensure data is protected and can be recovered when needed.\n\n## Steps to Implement:\n\n1. **Open AWS Management Console**: Sign in with your AWS account, navigate to the 'Backup' service.\n\n2. **Create Backup Plan**: Click `Create backup plan`. You can start from scratch or use an existing plan as a template.\n\n3. **Specify Backup Plan Details**: Enter a unique name for your plan and specify how backups will be created and retained. For instance, indicate backup frequency, backup window (recommended to be set when your workload traffic is low), lifecycle (transition to cold storage), and retention period.\n\n4. **Select Resources**: Under `Assign resources`, assign DynamoDB tables you want to backup by either specifying the table ARN, resource type or choosing a tag.\n\n5. **Review and Create Plan**: Review your configurations and click `Create plan`.\n\nNote that all your backup activities can be tracked and audited in CloudTrail, providing visibility into the backup status, and making it easier to secure and manage backups.\n\n```\n\nUsing AWS Backup plan for DynamoDB tables helps streamline data management, eliminates the chances of manual error, and offers cost-effective ways to implement disaster recovery."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the AWS control of ensuring Amazon DynamoDB tables are part of an AWS Backup plan can lead to significant impacts both financially and operationally:\n\n1. **Data Loss:** The most direct issue with non-compliance is the potential for data loss. If the DynamoDB tables are not being backed up, any accidental deletion or corruption of that data could be irreversible. This could mean serious financial impact, particularly if the affected data is critical to business operations.\n\n2. **Business Continuity:** In case of a catastrophe or data corruption, not having DynamoDB tables as a part of a backup could halt business operations. The inability to quickly recover data could significantly impact operations, ultimately resulting in financial losses and a potentially damaged business reputation.\n\n3. **Compliance Violations:** If an organization is required to comply with certain regulations or standards that mandate data back-ups and you do not have DynamoDb tables as part of AWS backup, it can result in non-compliance. This could lead to hefty fines or penalties.\n\n4. **Increased Downtime:** Restoring data without a backup plan often takes longer resulting in increased downtime for your application. This could result in significant revenue loss, especially for businesses that rely heavily on online operations.\n\n5. **Recovery Costs:** If data loss occurs and no backup is available, you might need to devote significant time and resources to attempt data recovery or re-create lost data, both of which can be costly.\n\nIn conclusion, ensuring Amazon DynamoDB tables are part of an AWS Backup plan is a crucial step in maintaining data integrity, ensuring business continuity, and avoiding unnecessary financial cost."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, if you are operating an e-commerce website that utilizes Amazon DynamoDB tables for storing product and customer information. This data is crucial for your business's daily operations, including inventory management, order processing, and customer relationship management.\n\nHowever, data loss can occur due to many unforeseen circumstances such as system failures, accidental deletions, or cyber-attacks. If your DynamoDB tables are not part of an AWS Backup plan, recovering this data can be time-consuming, costly, and sometimes impossible, leading to significant operational and financial impacts.\n\nBy ensuring that your DynamoDB tables are backed up as part of an AWS Backup plan, you ensure that you can quickly restore your data in the event of a loss. This not only saves time and resources but also minimizes downtime, helping maintain your business continuity and reputation.\n\n```markdown\n**Scenario:**\n1. You are managing an e-commerce website using DynamoDB tables for managing crucial data.\n2. Unexpected data loss occurs because of a system failure.\n3. Since your DynamoDB tables are part of AWS Backup plan, you quickly restore the lost data.\n4. Business continuity is maintained, saving both time and resources.\n```\n\nThus, having DynamoDB tables as a part of AWS Backup plan proves to be extremely useful in cases of critical data loss."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_user_unused_credentials_90",
    "Title": "IAM user credentials that have not been used in 90 days should be disabled",
    "Description": "AWS Identity and Access Management (IAM) can help you with access permissions and authorizations by checking for IAM passwords and access keys that are not used for a specified time period.",
    "QueryID": "aws_iam_user_unused_credentials_90",
    "DocumentURI": "policies/aws_iam_user_unused_credentials_90.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/IAM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "Sure, here's your request in a basic markup (in this case, Markdown format):\n\n---\n**AWS Identity and Access Management (IAM)**\n\nAWS Identity and Access Management *(IAM)* is a web service that helps you securely control access to AWS resources. It gives you a way to control access to AWS services and resources that users and applications in your organization need.\n\nOne important aspect of IAM is its ability to manage *access permissions* and *authorizations*. This is achieved by **checking for IAM passwords and access keys**. The system can detect if these credentials have not been used for a specific time period, keeping your resources secure by ensuring that unused credentials don't pose a security risk.\n\nIAM is crucial for any organization hoping to maintain secure and streamlined access to its AWS resources.\n\n---\n\nPlease note that the specific Markup format you want to use might differ (e.g., HTML, LaTeX), so you might need to adjust the example accordingly."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to this AWS Control can be both financial and reputational:\n\n1. **Increased Security Risks**: Unused IAM passwords and access keys exposed over a certain period of time can lead to unauthorized access. Non-compliance to this control can make your network vulnerable to breaches and cyber attacks, resulting in significant financial and data loss. \n\n2. **Financial Implications**: In the event of a data breach, a company may have to pay penalties as per the data protection laws of different countries, plus the cost of rectifying the damage caused by the breach. Additionally, AWS charges for IAM usage; keeping unused IAM credentials can add unnecessary charges to your AWS billing. \n\n3. **Reputation Damage**: If a data breach occurs due to non-compliance with this control, the damage to a company's reputation could be far-reaching, resulting in loss of customers and potential business, which could be far more costly than the immediate financial loss from the breach.\n\n4. **Non-Compliance With Standards**: This could result in failing audits, losing certifications, or not being able to comply with laws and regulations like GDPR, ISO 27001, HIPAA, PCI-DSS, etc.\n\nTo avoid such costs, it's essential to actively manage access permissions and regularly check and disable IAM passwords and access keys that haven't been used for a specific time. AWS provides tools to automate this process and send notifications about unused IAM credentials, making it much easier to maintain compliance."
      ],
      "x-kaytu-usefulness-example": [
        "For example:\n\nA rapidly growing start-up called \"FastTech\" operates completely on AWS infrastructure. FastTech has dozens of employees managing their cloud resources, and over the period, a number of IAM users have been created. Each user has different access permissions and authorizations. \n\nFor enhanced security measures, the company asks each of their employees to update their IAM passwords and access keys every 90 days. However, tracking this activity manually for each IAM user becomes a challenging task for the company's administrator.\n\nTo handle this scenario, FastTech utilizes AWS IAM's ability to track unused IAM passwords and access keys. The IAM utility can alert the administrator if the credentials are not changed within the specified time. This not only significantly reduces the administrative workload but also reinforces the company's security measures by ensuring that all IAM users update their security credentials regularly. \n\nIn addition, the administrator also leverages this feature to identify and remove any unnecessary or inactive IAM accounts, further enhancing their cloud security. \n\nHence, AWS IAM has proven to be a crucial tool for FastTech in managing access permissions and authorizations and maintaining strong security standards."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_s3_public_access_block_account",
    "Title": "S3 public access should be blocked at account level",
    "Description": "Manage access to resources in the AWS Cloud by ensuring that Amazon Simple Storage Service (Amazon S3) buckets cannot be publicly accessed.",
    "QueryID": "aws_s3_public_access_block_account",
    "DocumentURI": "policies/aws_s3_public_access_block_account.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/S3"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS Cloud provides an array of services to help manage and control access to resources. One such feature ensures that storage buckets within Amazon's Simple Storage Service (S3) cannot be publicly accessed, adding an extra layer of security and privacy to your data.\n\nHere's how you can explain this control in markup format:\n\n```markdown\n## Amazon S3: Manage Access to Resources\n\nAmazon Simple Storage Service (S3) provides secure, durable, and scalable storage for the internet. A significant security feature involves managing access to resources stored in S3 buckets.\n\n### S3 Bucket Public Access Control\n\nAWS ensures that S3 buckets cannot be publicly accessed by default. This ensures that your data isn't unintentionally exposed to the public. \n\nYou control access to the bucket by using bucket policies or Access Control Lists (ACLs). \n\nYou can, however, configure your bucket to be publicly accessible, but AWS recommends reviewing such configurations to ensure they align with your security requirements.\n\nBy managing access to your S3 resources, you benefit from:\n\n- Improved data privacy.\n- Compliance with data governance and privacy standards.\n- Prevention of unauthorized access to sensitive data.\n- Reduced risk of data breaches and potential regulatory penalties.\n```\nAWS addresses the shared responsibility model for security in the cloud, where AWS is responsible for the security 'of' the cloud, and customers are responsible for security 'in' the cloud. Therefore, managing access to your S3 buckets is an essential part of this model."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS control that states Amazon Simple Storage Service (Amazon S3) buckets should not be publicly accessible, can result in several potentially disastrous consequences. \n\n1. **Data breaches and loss of sensitive information:** If an S3 bucket is left publicly accessible, hackers or malicious actors can gain access to sensitive information. The cost associated with such a data breach can be extremely high, including both financial losses and damage to the company's reputation.\n\n2. **Non-compliance fines:** Failing to protect sensitive information can result in non-compliance with data protection regulations such as GDPR or HIPAA. Non-compliance fines can be substantial, often amounting to millions of dollars.\n\n3. **Unwanted charges:** If hackers gain access to your S3 buckets, they might manipulate data or perform operations that incur additional costs. They could potentially even perform a data breach and charge you ransom for releasing your own data.\n\n4. **Loss of customer trust:** Once a data breach becomes public knowledge, there can be a significant loss of trust from customers. This can lead to loss in business and long-term damage to the company's brand.\n\n5. **Potential lawsuits:** In many jurisdictions, if a company fails to adequately protect sensitive consumer data, it might be liable for damages. This can lead to costly lawsuits.\n\nTo avoid these potential consequences, it is recommended to follow AWS's best practice of ensuring that all S3 buckets aren't publicly accessible unless absolutely necessary, and have appropriate access control measures in place."
      ],
      "x-kaytu-usefulness-example": [
        "When your organization handles sensitive information such as user data or confidential business details, it's crucial to keep this information secured and away from potential threats. In this context, managing the access to your Amazon S3 buckets play an important role.\n\nFor instance, let's assume that you use Amazon S3 to store customer details for one of your applications. These may include sensitive details like emails, phone numbers, or other personal information. If the buckets have public read access, anyone on the internet can access all these details compromising the privacy and security of your customers. \n\nBy ensuring the access management control, you can allow only specific entities to access these objects and ensure no unintended user can view or modify them. You can specify permissions to individual AWS accounts, or to predefined groups defined by AWS.\n\nThis can also prevent unwanted costs. If a bucket that hosts a website is public, it may receive unwanted traffic that can increase your AWS usage charges and slow down the system. By managing the access to the S3 bucket, you can prevent these outcomes.\n\nHence, by using AWS control to manage access of S3 buckets, you ensure the security, privacy and the efficient use of resources in the AWS Cloud."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_elb_application_lb_waf_enabled",
    "Title": "ELB application load balancers should have Web Application Firewall (WAF) enabled",
    "Description": "Ensure AWS WAF is enabled on Elastic Load Balancers (ELB) to help protect web applications.",
    "QueryID": "aws_elb_application_lb_waf_enabled",
    "DocumentURI": "policies/aws_elb_application_lb_waf_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ELB"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# Ensure AWS WAF is enabled on Elastic Load Balancers (ELB) to protect web applications\n\nAmazon Web Services (AWS) provides a range of security measures to help protect your resources and data. One such measure is the Web Application Firewall (WAF), an effective tool in your defense strategy that helps secure your web applications by filtering, monitoring, and managing HTTP/HTTPS traffic. \n\n## Why you should enable AWS WAF on ELB\n\nElastic Load Balancer (ELB) is a load balancing service for applications operating in and across AWS regions. ELB performs health checks on instances within its purview and spreads client requests across healthy, operational instances. By enabling AWS WAF on your ELB, you gain the following benefits:\n\n- **Good Traffic but Block Bad** - AWS WAF allows you to create web application firewall rules that allow, block, or monitor (count) web requests based on defined conditions such as IP addresses, HTTP headers, HTTP body, URI strings, SQL injection etc.\n\n- **Protection against attacks** - AWS WAF can stop common web layer attacks such as SQL injection and scripting attacks, helping protect your application from threats.\n\n## How to enable AWS WAF on ELB\n\n1. **Sign in to the AWS Management Console**\n   \n2. **Navigate to WAF \u0026 Shield** - From \"Services\" menu, select \"WAF \u0026 Shield\".\n\n3. **Create a Web ACL** - On the WAF \u0026 Shield dashboard, click on \"Web ACLs\" in the left navigation pane and then click on \"Create Web ACL\".\n\n4. **Specify Web ACL details** - You will need to provide a name for your Web ACL, add a resource (here, your ELB), and specify the AWS region where your resource is hosted.\n\n5. **Add Rules** - You can now customize your Web ACL by adding rule statements. This could include rate-based rules and IP match conditions or managed rules AWS provides for common threats like SQLi or XSS.\n\n6. **Review and create** - After configuring your Web ACL, review your settings and click on \"Create\".\n\nYou've now successfully enabled WAF on your ELB. You can monitor web ACL traffic and get detailed information about the traffic patterns and threats via AWS WAF reports and CloudWatch metrics, respectively."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS Control \"Ensure AWS WAF is enabled on Elastic Load Balancers (ELB)\" can expose a company to several risks and costs:\n\n1. **Increased Cybersecurity Threats**: AWS WAF (Web Application Firewall) is used to protect applications from common web exploits. Non-compliance could lead to potential vulnerability to SQL injection or cross-site scripting (XSS) attacks, causing unauthorized data exposure or loss.\n\n2. **Financial Costs**: Cyberattacks can lead to significant financial losses. They can disrupt normal operations, inflate IT support costs for issue remediation, and potentially result in lawsuits or fines for failing to protect customer data.\n\n3. **Reputational Damage**: Companies who fall victim to these cyberattacks risk a significant hit to their brand's reputation. This could lead to loss of customers, decreased sales, difficulty attracting new customers, or loss of partnership possibilities.\n\n4. **Business Continuity Risk**: Successful attacks can bring operations to a standstill, resulting in downtimes which could potentially lead to a significant business impact.\n\n5. **Regulatory Non-compliance**: Certain industries or geographies have strict regulations for data security. If AWS WAF is not enabled on ELB, companies might face penalties and be considered non-compliant with these norms, risking hefty fines or sanctions.\n\n6. **Data Loss**: Without proper firewall protection, companies risk losing valuable confidential data which can have significant long-term impacts on the business.\n\nTherefore, enabling AWS WAF on ELB forms an important part of the security architecture of AWS, and non-compliance could potentially expose the organization to significant risks and costs."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_instance_backup_enabled",
    "Title": "RDS DB instance backup should be enabled",
    "Description": "The backup feature of Amazon RDS creates backups of your databases and transaction logs.",
    "QueryID": "aws_rds_db_instance_backup_enabled",
    "DocumentURI": "policies/aws_rds_db_instance_backup_enabled.md",
    "ManualVerification": false,
    "Severity": "low",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/RDS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# Amazon RDS Backup\n\nThe **Amazon Relational Database Service (RDS)** provides a feature for backing up your databases and transaction logs. This is an essential tool for safeguarding your data and ensuring business continuity.\n\n## Overview\n\nAmazon RDS backup feature automatically creates a storage volume snapshot of your DB instance. It backs up the entire DB instance and not just individual databases. Also, it captures transactions logs to provide point-in-time recovery.\n\n## Types of Backups\n\nThere are two types of backups in Amazon RDS: **Automated Backups** and **Manual DB Snapshots**.\n\n### 1. Automated Backups\n\nAutomated Backups automatically backs up your DB instance during a specific, user-defined backup window. It captures transaction logs to provide a point in time restore capability. \n\n### 2. Manual DB Snapshots\n\nManual DB snapshots are user-initiated and are kept until you explicitly delete them. They are useful for performing tasks like database migration or setting a baseline before a major data operation.\n\n## Retention Period\n\nYou can specify a retention period which defines how many days to retain an automated backup. The default period is seven days, but you can set it to up to 35 days.\n\n## Backup Storage\n\nBackups are stored in **Amazon S3**, which is designed for 99.999999999% (11 9's) of durability. The storage size of your backups do not affect the performance of your DB instance.\n\n## Charges\n\nThere are no additional charges for backup storage up to 100% of your total database storage for each region. If your backup storage exceeds the provisioned storage for your DB instances, you will be charged for the additional storage at the normal rate.\n\nIn conclusion, the Amazon RDS backup feature is a reliable way to ensure the safety of your data while maintaining high performance of your DB instance."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this Amazon Web Service (AWS) control - which is essentially about leveraging the backup feature of Amazon RDS (Relational Database Service) to regularly create backups of your databases and transaction logs - can potentially result in the following costly consequences:\n\n1. **Data Loss:** Failure to backup data could result in permanent data loss if a system or operational failure occurs. Restoring lost data, if even possible, can be time-consuming and costly.\n\n2. **Business Continuity Risk:** Any kind of data loss can halt business operations and processes which rely on the lost data. The reduced efficiency or complete stoppage can cause significant financial loss.\n\n3. **Recovery Costs:** The process of data recovery could require additional resources and personnel, which adds to overall operational costs, especially if the backup is non-existent or outdated.\n\n4. **Non-Compliance Fines:** Depending on the industry, there may be regulations (e.g. GDPR, HIPAA) in place requiring certain types of data to be regularly backed up and protected. Failing to comply could result in heavy fines.\n\n5. **Reputation Damage:** If data loss affects customers or users, it could harm the reputation of the company in the market leading to customer loss or reduced trust, which can indirectly impact revenue.\n\n6. **Loss of Business Opportunities:** In case of significant downtime or unavailability of key data, a business might lose out on potential business opportunities. \n\nIn conclusion, it's essential to comply with this AWS control. It not only provides a safety net against unexpected technical troubles but also protects the business from financial and reputational damage."
      ],
      "x-kaytu-usefulness-example": [
        "Amazon RDS Backup feature can be very useful in numerous scenarios, including but not limited to the following example:\n\nConsider a scenario where a company provides a SaaS product which maintains a lot of customer data. Due to unforeseen circumstances, there was an unexpected failure or a harmful cyber attack which led to corruption or losing substantial data from their primary database. \n\nWithout a reliable backup, the loss could result with unimaginable consequences such as a disruption of service, a decrease in customer's trust, or a possible compromise of previously stored data.\n\nHowever, thanks to the backup feature available with Amazon RDS, the company have automated backups of its databases and transaction logs. These automated backups can maintain daily snapshots of the data and also capture transaction logs to allow point-in-time recovery up to the last five minutes of database usage. \n\nThe disaster recovery process would be straightforward, it involves identifying the point till when the data was intact and restoring the most recent backup from Amazon RDS before the cyber attack or unexpected failure occurred. It allows to minimize data loss and can swiftly bring the services back online, substantially reducing the impact on customers and overall business continuity."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_instance_in_backup_plan",
    "Title": "RDS DB instances should be in a backup plan",
    "Description": "To help with data back-up processes, ensure your Amazon Relational Database Service (Amazon RDS) instances are a part of an AWS Backup plan.",
    "QueryID": "aws_rds_db_instance_in_backup_plan",
    "DocumentURI": "policies/aws_rds_db_instance_in_backup_plan.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/RDS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS Backup is a fully managed backup service by Amazon that allows you to automate your data backup processes, centralize backup activity across various AWS services, and monitor recent backups. \n\nIf you include your Amazon RDS instances in an AWS Backup plan, it means you're setting them up to be backed up automatically as per a specified schedule and retention period. It guarantees the safety and availability of your data even in the event of a catastrophic failure, accidental deletions, or data corruptions.\n\nHere's a sample markup description:\n\n```\n## AWS Backup Plan for Amazon RDS Instances\n\nGiven the essential nature of back-up and recovery processes in preserving data integrity and continuity, it is recommended to incorporate your Amazon RDS instances into an AWS Backup plan. With AWS Backup, you can consistently back up your databases according to a specified schedule and retain each backup for an explicitly defined period. This approach not only automates your back-up processes but also ensures that even during disastrous situations, your data remains secure and available for recovery.\n```\n\nThe above guidance ensures the RDS instances are being managed in the context of broader data protection strategy, providing business continuity and satisfying compliance requirements."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to this AWS Control includes:\n\n1. **Data loss**: In the absence of a systematic backup plan, important data in your Amazon RDS instances may get lost because of various reasons such as accidental deletion, hardware or system failure, or a potential security attack. Recovering from such data loss can be difficult and sometimes impossible if you do not have a backup.\n\n2. **Operational Disruption**: In the event of a system failure where the most recent data is not backed up, your routine operations could be disrupted, leading to reduced productivity and potential revenue loss. \n\n3. **Financial Impact**: Data recovery, in the absence of a backup, can be costly. You might have to invest substantial time, effort, and monetary resources to retrieve the lost data or rebuild the lost systems. Additionally, if the data loss is significant, it could result in financial penalties or lawsuits.\n\n4. **Regulatory Non-compliance**: Various regulatory bodies have different compliance rules, and data protection is generally one of them. Lack of a backup system could lead to non-compliance issues, which in turn could result in hefty fines or even license revocation.\n\n5. **Reputation Damage**: Data loss can damage a company's reputation, leading to a loss of customer trust and business opportunities. It could take a long time to rebuild the company's reputation.\n\nTherefore, ensuring your Amazon RDS instances are part of an AWS Backup plan is critical for preventing these significant problems."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, if you are a running a web application and using Amazon RDS as a core part of your services, data stored within the database can abound to critical volumes over time. In such a case, having a solid backup plan is crucial to protect against data loss due to accidental deletion, system failure, or data corruption. Therefore, integrating your RDS instances to be part of an AWS Backup plan can help in automating the backup process, as well as providing ease of recovery.\n\nThe AWS Backup plan for your Amazon RDS also keeps you covered in case of emergencies. Say for example, if there's a natural disaster and your primary data center goes down, AWS Backup allows for quick recovery of your data, irrespective of the volume size helping you maintain business continuity.\n\n```markdown\nThe AWS Backup offers centralised backup across AWS services and on-premise applications. Apart from just managing backups, it also aids in compliance, meeting regulatory requirements, and cost optimisation making it a holistic solution for backup and restore needs of your business.\n```\n\nHence, enrolling your RDS instances in an AWS Backup plan ensures you have a dependable backup mechanism in place. This becomes invaluable particularly when dealing with huge databases and business-critical data."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_acm_certificate_expires_30_days",
    "Title": "ACM certificates should be set to expire within 30 days",
    "Description": "Ensure network integrity is protected by ensuring X509 certificates are issued by AWS ACM.",
    "QueryID": "aws_acm_certificate_expires_30_days",
    "DocumentURI": "policies/aws_acm_certificate_expires_30_days.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ACM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Network Integrity Protection with AWS ACM X509 Certificates\n\nAmazon Web Services (AWS) provides a variety of security controls to help protect and maintain the integrity of your network. One such control aims to ensure the security of data in transit by implementing X509 certificates, which are issued by AWS Certificate Manager (ACM).\n\n## **What is AWS Certificate Manager (ACM)?**\n\nAWS Certificate Manager is a service that allows you to easily handle the complexity of managing and deploying public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for your AWS based websites and applications.\n\n## **What are X509 certificates?**\n\nX509 is a standard that defines the format for public key certificates. These certificates are used in many Internet protocols, including securing communications via SSL and TLS.\n\n## **Why use X509 certificates issued by AWS ACM?**\n\nBy ensuring that your X509 certificates are issued by AWS ACM, you can be certain that your certificates are trusted, validated, and correctly configured. ACM handles the complexity of creating and managing public SSL/TLS certificates, making it easier to protect your websites and applications.\n\n## **How does it protect network integrity?**\n\nWhen data is in transit, it can be vulnerable to interception or tampering. X509 certificates establish a secure, encrypted connection between clients and servers, protecting data during transit by:\n\n- Authenticating the identity of the host to which the user is connecting.\n- Facilitating the secure exchange of encryption keys.\n- Providing bidirectional encryption of communications between a client and server.\n\nBy ensuring your X509 certificates come from AWS ACM, you gain automatic integrations with other AWS services and management features, in addition to basic certificate issuance and renewal.\n\nThe control promotes confidence that your encrypted data in transit will remain confidential and will not be tampered with, thus maintaining the integrity of your network.\n\nOne important note is that for the full benefits of this control to be realized, all certificates should be configured correctly and kept up-to-date, a task which ACM can automate.\n\n```markdown\n# Summary\n- AWS ACM is a service for managing SSL/TLS certificates.\n- X509 certificates are a standard for public key certificates used to secure data in transit.\n- Ensuring your X509 certificates are issued by AWS ACM assists in maintaining the integrity of your network.\n- This control assists by authenticating server identities, enabling secure encryption key exchange, and providing bidirectional encryption.\n- Certificates should be correctly configured and kept current for this control to be effective, which ACM can automate.\n```"
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the AWS control that requires network integrity to be protected by ensuring X509 certificates are issued by AWS ACM (AWS Certificate Manager) can include but is not limited to the following:\n\n1. **Security Risk:** Without using secured and trusted certificates, such as X509 certificates issued by AWS ACM, your network can be exposed to various security threats including man-in-the-middle (MITM) attacks, eavesdropping, data tampering, and other kinds of cyberattacks. These attacks could lead to data breaches, loss of sensitive information, and unauthorized system access.\n\n2. **Loss of Customer Trust:** If your customers find out you are not using secure certificates to protect the network, it can lead to a loss of trust in your service. This can result in loss of customers, damage to brand reputation and business relationships.\n\n3. **Financial Loss:** In case of a data breach due to non-compliance, there could be potential financial repercussions including penalties for non-compliance with data protection regulations (like GDPR, CCPA etc.), costs involved in remediation of said breaches, possible lawsuits, and potential loss of business.\n\n4. **Operational Issues:** Failing to use valid X509 certificates can result in operational issues, like service disruptions or unavailability, filtering by network security systems, or incompatibility with client systems expecting standard certs.\n\n5. **Non-Compliance to Regulations:** Many industries have specific regulations and standards (like PCI-DSS for payment card industry) that require the use of certain security protocols, including the use of valid, secure certificates. Non-compliance could lead to fines and penalties.\n\nFailing to comply with this control essentially means failing to meet best practices for secure communication over a network. It is therefore advisable for businesses to comply with this control and ensure the use of trusted certificates like those issued by AWS ACM to maintain the integrity of their network."
      ],
      "x-kaytu-usefulness-example": [
        "```\nAmazon Web Services (AWS) provides a plethora of services to ensure the safety and security of the network connection. One such service is the AWS Certificate Manager (ACM). Here is an example of how to protect the network integrity using X.509 certificates issued by AWS ACM.\n\nConsider a typical cloud-hosted web application. The application users interact with the system through a web interface, usually a browser that communicates with the cloud servers over the network. With an ever-increasing number of cyber threats, it is important to encrypt this communication. SSL/TLS is commonly used to ensure this.\n\nWith AWS ACM, one can easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with AWS services and your internal connected resources. SSL/TLS certificates are used to secure network communications and establish the identity of websites over the Internet as well as resources on private networks.\n\nWithout these certificates, a user may fall victim to a man-in-the-middle (MITM) attack where an attacker intercepts and potentially alters the communication between the user and the server. But with the deployment of X.509 certificates being managed by AWS ACM, network integrity can be reliably ensured, offering a more secure connection for the users.\n\n```\nTo use it:\n1. Request a certificate in AWS Certificate Manager.\n2. Validate ownership of the domains in your certificate.\n3. Once the certificate is issued, use AWS ACM to deploy the public certificate on all the resources such as a load balancer, CloudFront distribution, etc.\n\nUsing the AWS management console, SDK or CLI, the process can be easily automated and integrated into the existing CI/CD pipeline.\n\nAWS ACM also includes options for certificate transparency logging, options for strong cryptographic algorithms, etc to enhance network security further."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_instance_logging_enabled",
    "Title": "Database logging should be enabled",
    "Description": "To help with logging and monitoring within your environment, ensure Amazon Relational Database Service (Amazon RDS) logging is enabled.",
    "QueryID": "aws_rds_db_instance_logging_enabled",
    "DocumentURI": "policies/aws_rds_db_instance_logging_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/RDS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Enable Amazon RDS Logging\n\n## Description\n\nAmazon Relational Database Service (RDS) provides a variety of database engines like Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and Microsoft SQL Server. To obtain visibility into your environment's operations and to improve the security of your applications, it is vital to enable logging. Logging in Amazon RDS entails keeping a record of specific types of activity in your environment.\n\n## Why It's Important\n\n- **Security**: Logging can provide data needed to address security incidents. It can be used to track activity in your environment and identify any unusual or unexpected behaviours that might indicate a security incident.\n- **Troubleshooting**: When errors or issues occur, having logging enabled can simplify the troubleshooting process. Logs provide valuable information on what occurred right before an issue took place, which can help in identifying and resolving the issue.\n- **Auditing and Compliance**: Logs can provide an audit trail of database activities, which is often required for regulatory compliance.\n\n## How to enable Amazon RDS Logging\n\n1. Sign in to the AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n\n2. In the navigation pane, choose `Databases`.\n\n3. Choose the DB instance which you need to enable the logging.\n\n4. Choose the `Modify` option.\n\n5. In the \"Database options\" section, ensure that `Enable log exports` is checked. Select the log types you want to enable.\n\n6. Choose `Continue` and then `Apply immediately`.\n\n7. Choose `Modify DB Instance`.\n\nRemember to review log data routinely and incorporate it into your security analysis and response processes."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with AWS controls, such as the enabling of Amazon Relational Database Service (RDS) logging, can result in multiple costs:\n\n1. **Security and Privacy Risk:** If logging is not enabled for Amazon RDS, it might be challenging to track suspicious activities and respond to security incidents promptly. Without logs, enterprise may not detect unauthorized activity, potentially leading to data breaches and violating privacy regulations.\n\n2. **Regulatory Compliance Risk:** Various regulations require appropriate logging. For instance, regulations like GDPR, HIPAA, and PCI-DSS all require comprehensive logging capabilities. Non-compliance may result in substantial fines and legal action.\n\n3. **Audit Failure:** Logging is often a critical part of IT audits. If logs are not kept or inadequate, it will likely result in audit failures, leading to potential business disruptions and reputational damage.\n\n4. **Difficulty in Troubleshooting:** Without adequate logging, troubleshooting system or application issues becomes more difficult and time-consuming. Delayed issue resolution can cause service interruptions, impacting customer satisfaction and potentially leading to loss of revenue.\n\n5. **Lack of Insight:** Logs provide an essential resource for understanding the behavior of the system and making data-driven decisions. Not having logs can lead to an absence of insight into system performance, behavior, and usage trends. \n\nTherefore, while it might seem unimportant on the surface, the cost of not enabling Amazon RDS logging can be substantial when considering potential security risks, regulatory fines, audit failures, troubleshooting difficulties, and lack of insight into system behavior."
      ],
      "x-kaytu-usefulness-example": [
        "For example, you have a business that depends on a web application with a large customer base. Your application is using Amazon RDS for handling all the database related operations. Suddenly, you start facing an issue where the service requests start failing during certain times of the day and you don't have an immediate explanation for these failures.\n\nBy having Amazon RDS logging enabled, you can look into the query logs to identify any problematic database queries that might be causing the issue. The log information would include a timestamp for each log entry plus the actual SQL query being run, helping you identify potential resource-intensive queries causing your service disruption.\n\nFurthermore, these logs allow you to monitor the performance of your SQL queries, making it easier to spot and debug any inefficient queries. \n\nThis feature can also be crucial for maintaining the security of your application. Since logs keep a record of each database activity, you can use them to monitor any unauthorized database access attempts, making it easier to ensure that only valid users have access to the necessary data.\n\n```\nIn markup format:\n\n- **Scenario**: Web application's service requests failing during certain times of the day.\n- **Solution with Amazon RDS logging**: \n    - Access to query logs to identify any problematic database queries.\n    - Detect potential resource-intensive queries causing service disruption.\n- **Additional Benefits**:\n    - Monitor the performance of SQL queries\n    - Maintain database security through logging each database activity.\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_apigateway_stage_logging_enabled",
    "Title": "API Gateway stage logging should be enabled",
    "Description": "API Gateway logging displays detailed views of users who accessed the API and the way they accessed the API.",
    "QueryID": "aws_apigateway_stage_logging_enabled",
    "DocumentURI": "policies/aws_apigateway_stage_logging_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/APIGateway"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "**What is API Gateway Logging in AWS?**\n\nApplication Programming Interface (API) Gateway Logging is a feature provided by Amazon Web Services (AWS), a subsidiary of Amazon.com, Inc. which provides on-demand cloud computing services.\n\nAPI Gateway Logging provides detailed information on who accessed your APIs and how they were accessed. This allows users to understand and track requests made to their APIs and provides insights into how their API is being used. This logging system provides valuable data that can be used for application diagnostics and performance optimization.\n\n**Features of API Gateway Logging**\n\n* **Detailed Access Logs:** The service records detailed information about each individual API call, including the IP address of the caller, time of the API call, request parameters, and response elements returned by the API Gateway.\n\n* **Dashboards and Visualizations:** Users can go to API Gateway's dashboards and visualizations to get insights and trends on API usage.\n\n* **Integration with CloudWatch:** API Gateway logging integrates seamlessly with CloudWatch, a monitoring service for AWS resources and the applications you run. Thus, users can view and archive logs, set alarms, and automate actions based on operational data.\n\n* **Error Logs:** These logs help in identifying and debugging issues that occur during API requests and responses.\n\n* **Execution Logs:** These are more detailed logs useful for debugging the execution path through your API Gateway setup.\n\n```mark\nAWS API Gateway logging is immensely helpful in debugging, optimizing, and creating a secure environment for APIs.\n```"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS control can lead to several potential costs:\n\n1. **Loss of Visibility**: If API Gateway logging is not enabled, you would lack insight into who is interacting with your API and how they're doing so. You won't be able to determine the patterns of normal or abnormal user behavior which can be crucial for identifying security incidents, conducting forensic investigations, or upgrading the performance of your API.\n\n2. **Data Breaches**: Without proper logging, it's harder to identify security risks and you could be easy prey for hackers. A data breach could cost your company heavily in terms of direct financial losses, reputation damage, and loss of customer trust.\n\n3. **Regulatory Penalties**: If you are in a heavily regulated industry (like healthcare or finance) failure to log user access to APIs could lead to additional scrutiny from regulatory bodies and potential fines for non-compliance with data security standards.\n\n4. **Operational Inefficiencies**: Without proper logging, it's difficult to troubleshoot issues and optimize performance which could lead to increased downtime and poor user experience.\n\nOverall, the cost of non-compliance can potentially be significant, ranging from financial loss due to data breaches and regulatory penalties, to operational inefficiencies and loss of customer trust due to poor user experience. It's advisable to follow all AWS controls to maintain a secure, efficient, and compliant cloud environment."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, you run a web application in which different users access various modules through an API managed by AWS API Gateway. Suddenly, you notice some anomalies such as significantly increased traffic to one specific module or errors in some specific API calls. To investigate this, you need to find out who is accessing what and how they are doing it.\n\nYou can use API Gateway logging to get detailed information on the users who accessed the API. It would show you the IP addresses of the users, the methods they used, the endpoints they accessed, the timestamps of access, as well as any error messages if there were problems. \n\nFor example, you might see log entries like:\n\n```\n127.0.0.1 - - [14/Apr/2021:14:15:32 +0000] \"GET /module1 HTTP/1.1\" 200 1234\n127.0.0.1 - - [14/Apr/2021:14:15:33 +0000] \"POST /module1/data HTTP/1.1\" 500 5678\n```\n\nThe first entry shows a successful (`200`) GET request to `/module1` endpoint while the second one indicates a failed (`500`) POST request to `/module1/data` endpoint. \n\nThis information is extremely helpful not only for diagnosing issues and assessing their impacts but also for tracking usage patterns, understanding your users better, and improving the overall quality of service. You can setup alarms for unusual activities, automate responses and even predict future issues or needs based on the collected data."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_apigateway_stage_use_waf_web_acl",
    "Title": "API Gateway stage should be associated with waf",
    "Description": "Ensure if an Amazon API Gateway API stage is using a WAF Web ACL. This rule is non compliant if an AWS WAF Web ACL is not used.",
    "QueryID": "aws_apigateway_stage_use_waf_web_acl",
    "DocumentURI": "policies/aws_apigateway_stage_use_waf_web_acl.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/APIGateway"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Amazon API Gateway API Stage WAF Web ACL\n\n**Control ID:**\nAPI_GW_WAF\n\n**Description:**\nThis control checks if an Amazon API Gateway API stage is using the Amazon Web Services (AWS) Web Application Firewall (WAF) Web Access Control List (ACL). AWS WAF provides protection against common web exploitations that may affect the availability, security, and performance of your web applications.\n\nIf an API Gateway API stage is not associated with a Web ACL, the control will be non-compliant. Web ACLs work as a protective layer which helps in blocking common attack patterns, such as SQL injection or cross-site scripting (XSS).\n\n**Rationale:**\nAttaching a Web ACL to API Gateway stages can minimize risks and potential attacks like injection attacks or cross-site scripting attacks and enhance your API's security level.\n\n## Remediation steps:\n1. Open the [API Gateway console](https://console.aws.amazon.com/apigateway).\n2. Under the APIs pane, choose the API that you are verifying.\n3. Choose Stages under the selected API.\n4. In the Stages pane, choose the API stage where you want to associate a Web ACL.\n5. In the Stage Editor pane, choose the Settings tab.\n6. Choose the name of the Web ACL that you want to associate with the API from AWS WAF Web ACL dropdown menu. If you haven't created a Web ACL, follow instructions from the [User Guide](https://docs.aws.amazon.com/waf/latest/developerguide/web-acl.html).\n7. Choose Save Changes.\n\nOfficials at every level of an IT organization can be held liable for breaches of data stored within their systems. Therefore, using AWS WAF Web ACL with Amazon API Gateway API Stages is a best practice that every AWS account should follow to protect their data."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this control could potentially lead to several costs for your organization. \n\n1. **Increased Security Risks:** Without a Web Application Firewall (WAF) in place, your web applications are vulnerable to security threats like SQL injection and Cross-Site Scripting (XSS). These threats can lead to data breaches, causing financial losses and reputational damage.\n\n2. **Increased Incident Response Cost:** In the absence of preventive security measures like WAF, the cost of incident response can be high. This includes the cost of identifying the incursion, isolating affected systems, rectifying the issue, and recovery operations post-incident.\n\n3. **Lost Business:** Any breaches, or even downtime, due to lack of a WAF can lead to loss of business. Customers might choose to switch to competitors if they perceive that their data is not secure with your organization.\n\n4. **Legal and Compliance Costs:** Depending on the regulations applicable to your industry (like GDPR, HIPAA, etc.), non-compliance can lead to hefty fines. For example, under GDPR, fines can reach up to 4% of a company's annual global turnover.\n\n5. **Remediation Costs:** These include the cost of reacting to a security breach or irregularity, such as the cost of implementing a WAF after a breach has occurred or the cost of cleaning up after an attack.\n\nIn conclusion, the cost of non-compliance to this AWS Control could be significantly high, including the possibility of business-ending implications. Therefore, utilizing a WAF Web ACL for Amazon API Gateway API stages is recommended as a preventive measure."
      ],
      "x-kaytu-usefulness-example": [
        "AWS Web Application Firewall (WAF) provides a layer of security that helps protect your web applications against common web exploits that could affect application availability, compromise security, or consume excessive resources. Integrating WAF with the Amazon API Gateway further enhances protection by inspecting incoming requests and preventing malicious traffic from accessing your APIs.\n\nAmazon API Gateway, which acts as a door to your backend services, can receive numerous traffic. This traffic might contain malicious requests with patterns of SQL injection or Cross-Site Scripting (XSS), intending to gain unauthorized access or disrupt the service. \n\nExample Instance:\n\nSuppose you have an e-commerce application with APIs residing on Amazon API Gateway. This application takes orders, manages inventory, processes payments, and more. The APIs receive millions of requests daily for different functionalities.\n\nWithout a WAF Web ACL in place, these APIs are exposed to a risk from hackers who might send harmful SQL injection patterns in their requests, aiming to manipulate your data or unauthorizedly access user information.\n\nBy enabling WAF Web ACL, these threats can be mitigated. AWS WAF Web ACL can inspect incoming requests to your APIs, filter out the requests displaying SQL injection or XSS patterns, and only allow legitimate traffic to reach the API, making your application safe and reliable. \n\nIn this case, it becomes non-compliant with safety protocols and potentially exposes your backend services to security threats if an AWS WAF Web ACL is not used. Therefore, ensuring an API Gateway API stage's usage with a WAF Web ACL is highly essential for the application's security."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_backup_plan_min_retention_35_days",
    "Title": "Backup plan min frequency and min retention check",
    "Description": "Checks if a backup plan has a backup rule that satisfies the required frequency and retention period(35 Days). The rule is non compliant if recovery points are not created at least as often as the specified frequency or expire before the specified period.",
    "QueryID": "aws_backup_plan_min_retention_35_days",
    "DocumentURI": "policies/aws_backup_plan_min_retention_35_days.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/Backup"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control - Backup Plan Frequency and Retention Period Check\n\nThis AWS Control is designed to ensure that there is a backup plan in place that accommodates the preset requirements for frequency and retention period(anything less than 35 days is non-compliant). \n\nSpecifically, it checks that a backup rule exists within the backup plan to satisfy these conditions. A backup rule defines the backup schedule and how frequently recovery points (or backups) are created, it also specifies the length of time these backups will be stored (retention period). \n\nThe control deems the rule to be non-compliant in the following scenarios:\n- If recovery points are not being created as often as the defined frequency in the backup rule.\n- If the created recovery points expire (or are deleted) before the expiration period that is specified in the backup rule.\n\nIn essence, this control is essentially a method to verify that your AWS backups are made at the right intervals and kept for the right duration, with a particular emphasis on meeting the minimum of 35 days retention policy.\n\n```markup\n\u003ccontrol\u003e\n  \u003ctitle\u003eBackup Plan Frequency and Retention Period Check\u003c/title\u003e\n  \u003cdescription\u003eChecks if a backup plan has a backup rule that satisfies the required frequency and retention period (should be at least 35 Days). The rule is non-compliant if recovery points are not created at least as often as the specified frequency or expire before the specified period.\u003c/description\u003e\n  \u003ccheck\u003e\n    \u003cfrequency\u003eAs per specified frequency in backup rule\u003c/frequency\u003e\n    \u003cretention\u003eAt least 35 days\u003c/retention\u003e\n    \u003cnon-compliant\u003eIf recovery points do not meet frequency or expire before retention period\u003c/non-compliant\u003e\n  \u003c/check\u003e\n\u003c/control\u003e\n```\n\nThis might be used in your AWS environment to enforce compliance and ensure minimal data loss or downtime in the event of a disaster, as well as achieving regulatory compliance that may require certain data storage and backup protocols."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS Control could result in several costs, both tangible and intangible in nature. Here's a breakdown of the potential risks and costs associated in markup format:\n\n1. **Data loss**: The most direct cost of non-compliance is the risk of data loss. If backups are not taken with required frequency or if they're not retained for the specified period, there may not be sufficient recovery points in case of a data loss incident. The value of the lost data can be immense and could be a direct financial loss.\n\n2. **Operational Disruption**: The company may experience interruptions in operations due to data loss or the unavailability of required backups, resulting in loss of business and productivity.\n\n3. **Regulatory Fines**: For certain regulated industries, failure to have adequate backup procedures can result in significant regulatory fines. \n\n4. **Legal Consequences**: Data loss, especially of sensitive or customer data, can lead to legal consequences, further escalating the financial damages.\n\n5. **Reputation damage**: There can also be serious implications for companies' reputations if their lack of adequate backup systems leads to data loss or downtime, which can in turn impact their relationships with customers and partners.\n\n6. **Remediation costs**: To resolve the problems caused by inadequate backups, the company would also have to spend on emergency recovery measures, tests, implementing correct backup procedures, etc. \n\n7. **Loss of Competitive Advantage**: In some cases, data loss can also result in loss of intellectual property or strategic information, which can indirectly lead to loss of competitive advantage. \n\nBy ensuring strict compliance with the mentioned AWS Control, companies can mitigate these risks and associated costs. It is an investment in continuous data protection and long-term organisational resilience."
      ],
      "x-kaytu-usefulness-example": [
        "Example:\n\nImagine you are a system administrator for a company that hosts various customer databases on AWS. These databases contain critical data that needs to be backed up regularly to prevent data loss or downtime in case of an accidental deletion or server issue. Your company policy requires that backups be taken every day, and the backup data should be retained for at least 35 days to ensure that you can restore the data in case of any issues.\n\nBy using this AWS control, you can ensure that your AWS backup plan conforms to these requirements. \n\nSpecifically, it checks if your AWS backup plan includes a backup rule that creates recovery points (backups) at least once a day and has a retention period of at least 35 days. \n\nIf the backup frequency or retention period of your backup is not in line with these requirements, this control helps you identify this non-compliant behavior, enabling you to adjust the backup plan accordingly. \n\nThis AWS control is therefore very useful for enforcing your company's backup policy, making sure that your backup procedures are always up to the mark, and preventing any unanticipated data loss. This ultimately ensures the continuity and reliability of your business operations."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_cloudtrail_s3_data_events_enabled",
    "Title": "All S3 buckets should log S3 data events in CloudTrail",
    "Description": "The collection of Simple Storage Service (Amazon S3) data events helps in detecting any anomalous activity. The details include AWS account information that accessed an Amazon S3 bucket, IP address, and time of event.",
    "QueryID": "aws_cloudtrail_s3_data_events_enabled",
    "DocumentURI": "policies/aws_cloudtrail_s3_data_events_enabled.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/CloudTrail"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "Sure, here is the explanation in an easy-to-understand format, written in Markdown:\n\n## Amazon S3 Data Events Collection \n\nAmazon S3 collects a number of data events. This data gathering can aid in identifying any unusual activity happening with your stored data. \n\n### Tracked Details \n\nAmazon S3 collects and records various detailed information including:\n\n- **AWS Account Information**: It tracks the details of the AWS account which has accessed an Amazon S3 bucket. This can help to trace any unauthorized access to the data. \n\n- **IP Address**: Amazon S3 logs the IP address of the user who accessed data stored in its buckets. This adds another layer of traceability and aids in tracking down any suspicious activity.\n\n- **Time of Event**: Along with account details and IP, the time of the event is also recorded. This will provide insights on when exactly the data has been accessed which is crucial during audits and investigations. \n\nTherefore, the collected data events of Amazon S3 provide a robust system of monitoring and control over stored data, which makes it easier to maintain security and integrity."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS control could result in a range of potential costs, including:\n\n1. **Financial loss**: If malicious activity goes undetected due to non-compliance, the company may suffer significant financial loss due to data breaches, theft or destructive actions.\n\n2. **Legal costs**: Depending on the nature of the data and the jurisdiction, non-compliance may lead to lawsuits or financial penalties levied by regulatory authorities. \n\n3. **Reputational damage**: Failure to appropriately monitor and secure customer data could harm the company's reputation, leading to loss of customers and future revenue.\n\n4. **Incident recovery costs**: In case of a security incident, the company will have to spend resources to mitigate the incident, recover lost data, and resecure their S3 buckets. This could also involve hiring external consultants or specialists.\n\n5. **Regulatory penalties**: For organizations dealing with personal information (such as healthcare or financial data), non-compliance could lead to hefty fines from regulatory bodies like the the Health Insurance Portability and Accountability Act (HIPAA) or the General Data Protection Regulation (GDPR).\n\n6. **Lost Business Opportunities**: Repeated non-compliance might result in cloud service providers deeming the company as high risk and terminating the service, causing business disruption and potential loss of business opportunities.\n\nTherefore, it is important for a company to ensure they are compliant with all AWS controls to protect their business."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, a company named ABC Corp uses Amazon S3 to store confidential customer data. It has multiple AWS users who have access to this data, and all the interactions with the stored data are sensitive. ABC Corp enables the collection of Amazon S3 data events.\n\nThis plays an important role when a suspicious activity is observed. Let's say data from a particular Amazon S3 bucket is accidentally deleted or maliciously manipulated. With Amazon S3 data events, the ABC Corp's IT team can investigate the incident. The data events provide detailed insight including which AWS account accessed the bucket, the IP address from which the operation was performed, and the specific time of event. \n\nUsing this information, they can identify if the activity was carried from within the organization or it was a malicious external attempt. If the IP address from where the operation was performed does not belong to any AWS user within the company, it strongly points to a security breach. If the IP address belongs to a particular AWS user within the company, they can track the event back to that specific user and uncover whether it was an inadvertent mistake or a malicious act.\n\nTherefore, the collection of Amazon S3 data events is crucial in maintaining the security and integrity of the data stored in the Amazon S3 buckets by enabling efficient tracking and investigation of suspicious activities."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_dynamodb_table_encrypted_with_kms",
    "Title": "DynamoDB table should be encrypted with AWS KMS",
    "Description": "Ensure that encryption is enabled for your Amazon DynamoDB tables. Because sensitive data can exist at rest in these tables, enable encryption at rest to help protect that data.",
    "QueryID": "aws_dynamodb_table_encrypted_with_kms",
    "DocumentURI": "policies/aws_dynamodb_table_encrypted_with_kms.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/DynamoDB"
      ],
      "x-kaytu-explanation": [
        "# Ensure DynamoDB Tables are Encrypted \n\nAmazon DynamoDB is a fully managed proprietary NoSQL database service that supports key-value and document data structures. Since your sensitive data can reside in these DynamoDB tables, it's crucial to secure that data both in transit and at rest.\n\nEnabling encryption at rest for your DynamoDB tables helps improve the security of your data by converting it into an encoded form, which would be nearly impossible to read without the decryption key. It's an extra security layer adding confidentiality to your data and minimizing the risk of exposure.\n\nImplementing AWS encryption at rest for DynamoDB involves AWS KMS (Key Management Service), which stores control keys that are used to encrypt the DynamoDB data.\n\n## Steps to Enable Encryption at Rest on DynamoDB Tables\n\nTo enable encryption at rest for your DynamoDB tables, follow the steps below:\n\n1. Open the [AWS Management Console](https://console.aws.amazon.com/).\n\n2. Navigate to **DynamoDB**.\n\n3. Select the **Tables** section in the navigation pane.\n\n4. Choose the table that you want to encrypt.\n\n5. Under the **Table details** section, find the **Encryption at rest** field. If it's not enabled, you need to enable it.\n\n  - If you are creating a new table, under **Settings**, check **Enable encryption at rest**.\n  ![Enable encryption at rest](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/images/console-table-settings-encryption.png)\n\n  - For existing tables, you would have to create a new DynamoDB table with encryption enabled and migrate the data because AWS does not allow enabling encryption at rest on existing tables that were created with encryption disabled.\n\n## Conclusion\n\nIn conclusion, enabling encryption at rest for DynamoDB tables is an important security best practice that can help protect your sensitive data."
      ],
      "x-kaytu-noncompliance-cost": [
        "AWS control compliance is crucial to ensure security and protect sensitive data from security threats. Non-compliance to the control that states \"Ensure that encryption is enabled for your Amazon DynamoDB tables\" can lead to several potential consequences, including:\n\n1. **Data Breaches:** In the absence of encryption, your sensitive business and customer data becomes vulnerable to unauthorized access, hacking, and theft. If a potential attacker gets access to the raw database, they can easily extract unencrypted data.\n\n2. **Non-compliance to Regulations:** Various acts such as GDPR, HIPAA, and PCI-DSS require that sensitive data at rest be encrypted. Non-compliance could result in heavy fines and legal actions.\n\n3. **Damage to Business Reputation:** In the event of a data breach, your company can suffer damage to reputation with customers and partners, which might subsequently lead to loss of business.\n\n4. **Operational Disruption:** A data breach or data loss can disrupt business operations, leading to downtime, loss of productivity, and financial loss.\n\n5. **Cost of Recovery:** If a breach occurs, the cost of recovering lost data, repairing systems, and implementing stricter security measures can be significant. \n\nTo avoid these consequences, it's crucial to ensure that encryption is completed for all DynamoDB tables, ideally using AWS Key Management Service (AWS KMS) for automatic encryption. This ensures data is encrypted and decrypted transparently, providing additional safety to sensitive data at rest and in transit without affecting the performance and scalability of your applications."
      ],
      "x-kaytu-usefulness-example": [
        "For example, assume that you are building a healthcare application where patients can book appointments with doctors. The backend of this application is powered by Amazon DynamoDB tables that store sensitive patient information such as names, addresses, health conditions, and past medical records. To comply with legal and regulatory policies like HIPAA, it's crucial to ensure the privacy and security of this data. Therefore, by enabling encryption at rest for your DynamoDB tables, you can add an additional layer of security to protect patient data from unauthorized access or breaches. This can also enhance trust among your clients, as they can rest assured that their personal and sensitive data is being handled safely and securely.\n  \nIn your AWS console, you navigate to DynamoDB, choose the table that holds confidential patient data, and under the \"Encryption at rest\" on the \"Overview\" tab, you enable AWS owned KMS key or AWS managed KMS key according to your need. This will ensure that all data stored in this table is encrypted, and can only be accessed by authorized personnel with the necessary decryption keys. Encryption and decryption actions are transparent to the user and application, meaning the performance of your application will continue to operate seamlessly while offering enhanced data security."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ec2_ebs_default_encryption_enabled",
    "Title": "EBS default encryption should be enabled",
    "Description": "To help protect data at rest, ensure that encryption is enabled for your Amazon Elastic Block Store (Amazon EBS) volumes.",
    "QueryID": "aws_ec2_ebs_default_encryption_enabled",
    "DocumentURI": "policies/aws_ec2_ebs_default_encryption_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/EC2"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS Control recommends enabling encryption for your Amazon Elastic Block Store (EBS) volumes to enhance security.\n\nEBS volumes are used to store data persistently and they can be attached to any EC2 instance. Encryption is a crucial aspect of data protection. It limits the risk of unauthorized data access significantly.\n\nHere's the description in Markup format:\n\n```markdown\n## AWS Control\n\u003e **Encryption for Amazon EBS Volumes**\n\nTo enhance the security of your data at rest, it is recommended to enable encryption for your _Amazon Elastic Block Store (EBS) volumes_. Encryption plays a vital role in protecting your data from unauthorized access, thus reducing the risk significantly.\n```\n\nPlease keep in mind, whenever you are enabling encryption, make sure to manage your keys appropriately. You can use AWS Key Management Service (KMS) for creating and managing encryption keys - these keys are known as Customer Master Keys (CMK). It is always recommended to periodically rotate these keys for additional security."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS Control can result in several potential costs:\n\n1. **Security Breach Costs**: If data stored in Amazon EBS volumes is not encrypted and is consequently breached, the organization could face significant costs. These could be due to:\n\n    i. **Remediation efforts**: Costs to identify the weaknesses exploited by the attackers and to correct them.\n    \n    ii. **Loss of sensitive data**: If the breached data includes sensitive information, there could be a significant cost associated to this loss.\n \n    iii. **Incident response**: The process of responding to a breach can be extensive and costly, involving the time of many individuals in the organization and potentially needing the assistance of outside experts.\n\n2. **Regulatory Fines and Legal Costs**: For certain types of data and in several sectors, encryption is required by regulatory laws (e.g., HIPAA for health information, GDPR for personal data of EU citizens, etc.). Non-compliance with these laws could lead to substantial penalties and, in some cases, lawsuits from affected individuals.\n\n3. **Brand Damage**: If a breach occurs and it is known that data was not encrypted, the organization could suffer significant damage to their reputation. This could lead to a long-term drop in business and consequent financial impact.\n\n4. **Loss of Trust**: If customers or business partners learn that a company is not properly protecting its data, they may choose to stop doing business with the company. This would impact the company's earnings.\n\nIn conclusion, the cost of non-compliance with the AWS encryption control includes more than just the immediate cost of any exploited weaknesses. It also potentially includes significant costs in fines, customer trust, and brand value."
      ],
      "x-kaytu-usefulness-example": [
        "An example instance where enabling encryption for Amazon Elastic Block Store (EBS) volumes proves to be useful is when an organization is developing a healthcare application in AWS cloud that requires storing sensitive patient data. \n\nThe healthcare industry is subjected to strict compliance regulations like HIPAA, which mandates the protection of sensitive patient data. In this case, implementing encryption for Amazon EBS volumes can help protect this sensitive data by converting it into an unreadable format for unauthorized users. Only individuals or systems in possession of the decryption key can revert it back to a readable format, thus maintaining data security in compliance with regulatory requirements.\n\nHere's a markup version for the instance:\n\n```markdown\nFor instance:\n  \nAWS Services are being used by a healthcare company to build a healthcare application that processes and stores patient data - a sensitive piece of information according to HIPAA regulations. To ensure data security and compliance with these regulations, the organization can enable encryption for their Amazon EBS volumes to convert sensitive healthcare data into an unreadable format. This data can only be read if the decryption key is available, thereby ensuring high grade security of patient data.\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_snapshot_encrypted_at_rest",
    "Title": "RDS DB snapshots should be encrypted at rest",
    "Description": "Ensure that encryption is enabled for your Amazon Relational Database Service (Amazon RDS) snapshots.",
    "QueryID": "aws_rds_db_snapshot_encrypted_at_rest",
    "DocumentURI": "policies/aws_rds_db_snapshot_encrypted_at_rest.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/RDS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Enable Encryption for Your Amazon RDS Snapshots\n\nAWS's Amazon Relational Database Service (RDS) provides an easy way for users to set up, operate, and scale database deployments in the cloud. With options to save costs, increase availability, or improve performance, it can cater to specific needs like transaction processing, data warehousing, or in-memory analysis. RDS snapshots are one of the options provided by the service to backup data. These are point-in-time snapshots of your DB instance, stored in Amazon Simple Storage Service (Amazon S3).\n\n## Importance of Encryption\n\nEncryption is a crucial security measure that protects sensitive information. If a snapshot is left unencrypted, it poses a data breach risk as unauthorized individuals can access the data. For RDS, AWS provides encryption at rest for DB instances and snapshots to increase the security of your data. This means that all the data on the instance, as well as backups, snapshots, and replicas in the same region are encrypted.\n\nAWS Key Management Service (KMS) handles the encryption and decryption process - when you create an encrypted DB snapshot, the snapshot and any subsequent copies of the snapshot are encrypted, regardless of whether the DB instance or the copies of the snapshot are encrypted.\n\n## How to Enable Encryption\n\nHere are simple steps to enable encryption for your RDS snapshots:\n\n1. **Console**: Encryption is configured at the instance level. When you launch a new DB instance, select the `Enable encryption` option under the \"Encryption\" section. If you already have instances running, you need to create a snapshot of your DB, make a copy of that snapshot and then enable encryption in the process.\n\n2. **CLI or AWS API**: Use the `CopyDBSnapshot` action and specify the `KmsKeyId` parameter to enable encryption for a copy of a snapshot.\n\nNote: Once a DB Snapshot is encrypted, it cannot be unencrypted later.\n\n## Verify Encryption\n\nTo verify if a DB snapshot is encrypted, you can use the RDS console, the AWS CLI `describe-db-snapshots` command, or the `DescribeDBSnapshots` operation for AWS API.\n\nThe information is found under the `KmsKeyId` parameter - a `null` value indicates no encryption, while the presence of a key ID indicates encryption is enabled.\n\nRemember, encryption does add some overhead in terms of performance, but the data protection benefits typically outweigh this. As a security best practice, it’s recommended to keep this option enabled."
      ],
      "x-kaytu-noncompliance-cost": [
        "Failing to comply with the AWS control, *'Ensure that encryption is enabled for your Amazon RDS snapshots'* could result in several costs in terms of security, financial and reputational exposure.\n\n1. **Security Risks** : The database snapshots without encryption are vulnerable to crackers. They could potentially gain unauthorized access to the data and cause significant damage.\n\n2. **Regulatory Compliance Violations** : Non-compliance can lead to regulatory action if your organization falls under regulations that require database encryption. This could result in fines, penalties, and necessary adaptation of procedures to meet these compliance requirements.\n\n3. **Financial Exposures** : The exposure of sensitive customer data can lead to lawsuits or financial penalties. For companies in industries such as finance or healthcare, where data protection is mandatory, the cost of non-compliance can be huge.\n\n4. **Reputation Damage** : In case of a data breach, the damage to the company's reputation could be substantial, potentially causing loss of customers, negative press, and a decrease in stakeholder trust. \n\n5. **Loss of Data Control** : Without encryption, data can be more easily exploited if it falls into the wrong hands. This could mean the loss of proprietary business information, or sensitive personal customer data - ultimately diminishing the control a business has over its information.\n\n6. **Business Continuity Risks** : In the event of a data breach, operations might be interrupted, causing major downtime that could impact the business. \n\nConsidering these potential costs, encrypting Amazon RDS snapshots should be a priority as it adds an additional layer of data protection."
      ],
      "x-kaytu-usefulness-example": [
        "```\nAs a Database Administrator of XYZ Corporation, maintaining data security is my utmost priority. We use Amazon RDS for storing our business-critical and sensitive data and therefore, it's essential that our data remain secure at all times.\n\nRecently, we had a requirement to keep our users' data for a longer time for auditing and analysis. To fulfill that, we started creating snapshots of our Amazon RDS instances over a period of time. While taking these snapshots, we utilized AWS Control's feature to \"Ensure that encryption is enabled for your Amazon RDS snapshots\".\n\nThis functionality provided an additional security layer to our data. Even if our snapshots were to be compromised, the data would still remain unreadable and safe due to the encryption. Utilizing encryption for RDS snapshots safeguarded us from potential data breaches, in turn, enhancing our security measures.\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_elb_classic_lb_use_ssl_certificate",
    "Title": "ELB classic load balancers should use SSL certificates",
    "Description": "Because sensitive data can exist and to help protect data at transit, ensure encryption is enabled for your Elastic Load Balancing.",
    "QueryID": "aws_elb_classic_lb_use_ssl_certificate",
    "DocumentURI": "policies/aws_elb_classic_lb_use_ssl_certificate.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ELB"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Ensuring Encryption for Elastic Load Balancing\n\nIn AWS, data protection is vital, particularly when dealing with sensitive data. AWS provides various mechanisms to encrypt and protect your data both at rest and in transit. One of these mechanisms is encryption for Elastic Load Balancing (ELB).\n\nElastic Load Balancing automatically distributes incoming application traffic across multiple targets, such as Amazon EC2 instances. This ensures the reliability and robustness of your applications.\n\nHowever, if the data flowing between the clients and the load balancers is not encrypted, it can be susceptible to eavesdropping, data tampering, and middle-man attacks. To avoid such situations and boost your data security, you should enable encryption for your Elastic Load Balancing.\n\nEnabling encryption on ELB protects your data in transit by establishing a secure channel between the clients and the load balancer using SSL/TLS protocol. This is referred to as SSL termination.\n\nHere's a piece of code that explains how to enable SSL termination on an Elastic Load Balancer:\n\n```markdown\n    # create load balancer with SSL certificate\n    aws elbv2 create-load-balancer --name my-load-balancer  --subnets subnet-0e72d96b0e714089a subnet-0c24b197271a2a853 --security-groups sg-0f85b494b2a7ac587 --certificate-arn arn:aws:acm:us-west-2:123456789012:certificate/8cfd7dae-9bcc-4e34-ae61-75088a9ba6fb\n```\n\nHowever, SSL/TLS encryption provides security only for information in transit from clients to ELB. To protect data as it travels from the load balancer to the targets, you should consider setting up a second SSL/TLS layer on the targets themselves.\n\nAlways remember that the primary motive behind ensuring encryption for ELB is to guard sensitive data against threats, maintain data privacy, and meet compliance requirements for your organization."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS control can result in the following costs:\n\n1. **Security breach costs:** Unencrypted data transmission can be easily intercepted, leading to data breaches. Depending on the value of the data, these breaches can cost organizations a significant amount in terms of financial losses. \n\n2. **Regulatory fines:** Many industries and regions have strict data protection laws and regulations (like GDPR in the EU, CCPA in California, etc). Non-compliance with these regulations can lead to hefty fines for an organization.\n\n3. **Damage to reputation:** If a security breach becomes public knowledge, it can damage the reputation of the organization. This could lead to loss of trust among the customers, resulting in the loss of business. \n\n4. **Data loss costs:** In case of a data breach, the cost of recreating the lost data or the cost associated with the loss of business intelligence can be significant.\n\n5. **Legal costs:** A data breach can also result in legal action, leading to potential legal costs for an organization.\n\n6. **Resource costs:** The time and resources required to resolve issues arising from a data breach can also add to the cost.\n\nIn conclusion, ensuring encryption for Elastic Load Balancing is a critical security measure that not only helps in protecting sensitive data but also avoids many potential costs associated with a security breach."
      ],
      "x-kaytu-usefulness-example": [
        "For a company named XYZ eCommerce that operates a large-scale online shopping site, ensuring encryption for its Elastic Load Balancing (ELB) is extremely important for data protection. They process millions of customer transactions every day, including sensitive data such as credit card information and personal identifying details. \n\nIf an unfortunate event occurs, like the system being hacked, the unencrypted data can fall into the wrong hands. This will lead to an immense loss, both financially and reputation-wise. \n\nBut if encryption is enabled for the ELB, even if the data is intercepted, it will remain secure as it cannot be decrypted without the appropriate keys. This scenario clearly shows the importance of enabling encryption in AWS ELB settings, proving itself as an essential control system.\n\nBy using AWS infrastructure, they have the necessary tools to implement encryption and additional security measures. This could involve using AWS Certificate Manager (ACM) to handle SSL/TLS certificates, ensuring seamless encryption for the ELB. Additionally, they could also take advantage of the AWS Key Management Service (KMS) for handling encryption keys for an additional layer of security. \n\nImplementing encryption on AWS ELB will provide the XYZ eCommerce with a robust and secure infrastructure, it adds a layer of security that increases the trust and reliability of the eCommerce among its clients. In case of a potential breach, the data is less likely to be stolen, thanks to encryption."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_root_user_hardware_mfa_enabled",
    "Title": "IAM root user hardware MFA should be enabled",
    "Description": "Manage access to resources in the AWS Cloud by ensuring hardware MFA is enabled for the root user.",
    "QueryID": "aws_iam_root_user_hardware_mfa_enabled",
    "DocumentURI": "policies/aws_iam_root_user_hardware_mfa_enabled.md",
    "ManualVerification": false,
    "Severity": "critical",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/IAM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "This AWS control is centered around the concept of securing access to resources in the AWS Cloud. The focus is on utilizing hardware Multi-Factor Authentication (MFA) for the root user to enhance security. \n\n```\n# AWS Control: Manage Access to Resources\n\n**Objective**: Protect and control access to resources in the AWS Cloud.\n\n## Multi-Factor Authentication\n\nImplement a strategy where all crucial operations require an additional step of authentication, reducing the potential of unauthorized access. By enabling MFA, you can ensure the access security level is heightened.\n\n### Root User MFA\n\nThe root user has complete access to all AWS resources and services. Therefore, it is critically advisable to enable hardware MFA for this account to provide strong defense against compromised credentials.\n\n#### Implementation Steps\n\n* Sign in to your AWS account with your root credentials.\n* In the AWS Management Console, navigate to your account name in the upper-right corner and then select **Security Credentials**.\n* Expand the **Multi-Factor Authentication (MFA)** section.\n* Next to **Assigned MFA device**, click the **Manage MFA device** option.\n* Follow the on-screen instructions to assign a hardware MFA device to your root account.\n\n##### Note\n\nIf you lose the hardware MFA device or it stops working, contact AWS Support to unassign it from your account. After the device is unassigned, you can assign a new hardware MFA device.\n\n**Benefits**\n\nThis control measure profoundly reduces the likelihood of unauthorized access or data breach due to compromised root credentials. By ensuring hardware MFA is enabled for the root user, you can maintain and manage secure access to resources in the AWS Cloud.\n```\n"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS Control \"Manage access to resources in the AWS Cloud by ensuring hardware MFA is enabled for the root user\" can result in several costly consequences. \n\n1. **Data Breaches and Information Theft**: Without hardware MFA (Multi-Factor Authentication), the AWS account is more susceptible to unauthorized access, leading to potential data breaches. This can cause significant direct costs related to mitigation and recovery, as well as indirect costs such as reputational damage and loss of customer trust.\n\n2. **Financial Losses**: An unauthorized individual with access to the root user account could potentially alter AWS resource usage or access sensitive financial information, leading to unexpected costs or financial losses. \n\n3. **Non-Compliance Penalties**: Many industries require specific standards of data security, such as the financial industry's PCI-DSS or the healthcare industry's HIPAA. Non-compliance because of inadequate authentication measures can result in substantial fines and penalties.\n\n4. **Operational Disruption**: Unauthorized access to the root user account could lead to disruptions in your AWS operations, causing business slowdowns or shutdowns, leading to productivity loss and potential revenue drops.\n\n5. **Loss of Data Integrity and Availability**: An unauthorized user could potentially modify, delete, or otherwise compromise the integrity or availability of the data stored in the AWS cloud, affecting your business operations and reliability.\n\nThus, non-compliance to this control can lead to extensive financial, operational, and reputational costs. Therefore, it is critical to ensure hardware MFA is enabled for the root user to enhance the security of your AWS resources."
      ],
      "x-kaytu-usefulness-example": [
        "Instance of Usefulness:\n\nImplementing hardware MFA (Multi-Factor Authentication) for the root user in AWS could be highly useful in cases where high level of security is sought.\n\nFor example, consider an international banking organization that operates multiple critical workloads on AWS. Unauthorized access to their AWS root user could lead to potential data breaches, loss of data, and disruption to their services.\n\nBy ensuring hardware MFA is enabled for the root user, this banking organization could effectively add an extra layer of security to their AWS resources. The hardware MFA device requires individuals to provide two forms of identification - something they know (password) and something they have (the MFA device) - to access the AWS resources.\n\nThis significantly reduces the risk of account compromise due to phishing or password attack vectors because even if the password is compromised, an attacker would still require physical access to the hardware MFA device.\n\nThus, the banking organization can maintain high-level security measures, protecting their vital data and services hosted on AWS."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_secretsmanager_secret_unused_90_day",
    "Title": "Secrets Manager secrets should be rotated as per the rotation schedule",
    "Description": "Ensure if AWS Secrets Manager secrets have been accessed within a specified number of days. The rule is non compliant if a secret has not been accessed in 'unusedForDays' number of days. The default value is 90 days.",
    "QueryID": "aws_secretsmanager_secret_unused_90_day",
    "DocumentURI": "policies/aws_secretsmanager_secret_unused_90_day.md",
    "ManualVerification": false,
    "Severity": "low",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/SecretsManager"
      ],
      "x-kaytu-explanation": [
        "```markdown\n# AWS Control: Access to AWS Secrets Manager secrets \n\nThis control aims to monitor the usage of AWS Secrets Manager secrets. \n\n## Rule:\nThe condition for a non-compliant status is that a secret has not been accessed for a specified number of days, referred to as 'unusedForDays'. \n\n## Default Value:\nIf no specific number is set for 'unusedForDays', the default value is considered as 90 days. \n\n## Use Case:\nIf a secret, like credentials or keys, stored in AWS Secrets Manager remains unused for the predefined or default period, it could mean unused services or potential security risks. Thus, regular checks like this control are advisable for maintaining smooth and secured operations. \n\nIn other words, it helps ensure that unused secrets are identified and managed appropriately, such as being disabled or removed, to mitigate unnecessary risk exposure.\n```\n"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS Control means that an institution's sensitive information may be at risk. The cost of non-compliance can include:\n\n1. **Security Risks**: Stagnant secrets whose usage isn't regularly checked, remain a security risk. If an attacker gets hold of a secret key, they could gain unauthorized access to highly sensitive information.\n\n2. **Financial Costs**: In a scenario where a breach occurs due to non-compliance, the financial penalties could be significant. These penalties could be in the form of fines for non-compliance with data protection regulations, remediation costs due to the breach, or monetary losses due to fraud.\n\n3. **Reputation Damage**: Beyond the immediate financial implications, a data breach can cause significant reputational damage. Customers and clients might lose trust in the ability of the company to safeguard its data, leading to loss of business.\n\n4. **Regulatory Penalties**: Non-compliance could lead to an organization falling foul of data protection laws such as GDPR, which can levy hefty fines.\n\nTherefore, maintaining compliance to this AWS control not only safeguards your data but also helps avoid possible damages, both reputational and financial. Even though the upkeep of regulatory controls like this might require time, effort, and resources, the cost of non-compliance is much greater."
      ],
      "x-kaytu-usefulness-example": [
        "An example of the usefulness of this AWS control can be in a scenario where an organization is managing a large number of secrets - such as database credentials, API keys, or other sensitive configurations - using AWS Secrets Manager. \n\nFor example:\nLet's say a large company, \"TeleGlobal\", uses a cluster of micro-services for various operations, each requiring unique credentials for various databases and third-party integrations. Over time, as the system evolves, certain services might be deprecated or updated to no longer require some of the secrets stored in the AWS Secrets Manager. However, unless manually removed, these unnecessary secrets can remain in the Secrets Manager, increasing potential security risks and management complexity.\n\n```\nIf they setup this AWS Control with default value of `unusedForDays` as 90, it would automatically detect any secrets that have not been accessed for 90 days. This information can help TeleGlobal to identify, verify and remove redundant secrets promptly, optimizing their security management strategy and reducing potential threats surface.\n``` \n\nMoreover, this rule can also help in cost optimization, as AWS Secrets Manager bills based on number of secrets stored and API calls made to it. If unnecessary secrets are identified and removed, it could lead to substantial cost savings for TeleGlobal.\n\nOverall, this AWS Control helps in maintaining a clean, minimum-privilege secrets management system with enhanced security and cost effectiveness."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_efs_file_system_encrypted_with_cmk",
    "Title": "EFS file systems should be encrypted with CMK",
    "Description": "Ensure Amazon Elastic File Systems (Amazon EFS) are encrypted using CMK. The rule is non compliant if the EFS File System is not encrypted using CMK.",
    "QueryID": "aws_efs_file_system_encrypted_with_cmk",
    "DocumentURI": "policies/aws_efs_file_system_encrypted_with_cmk.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "other_checks": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/EFS"
      ],
      "x-kaytu-explanation": [
        "This AWS control in markup format basically mandates that your Amazon Elastic File Systems (EFS) should always be encrypted using Customer Master Keys (CMKs). This is a security best practice to protect your data at rest within EFS.\n\nHere's what it would look like in a simple markup format:\n\n```markdown\n# Policy Name: Ensure EFS is encrypted with CMK\n\n## Policy Description:\nAll Amazon Elastic File Systems (EFS) should be encrypted with AWS KMS Customer Master Keys (CMKs). An EFS file system that is not encrypted using CMK is considered non-compliant with this policy.\n\n## Rationale:\nEncrypting EFS using CMKs enhance data security as it adds an additional layer of protection to your data at rest. This ensures that only authorized systems or users can access and read the data.\n\n## Compliance:\n- This policy fulfills best security practices and compliance requirements for data encryption.\n\n## Remediation:\n- Identify the non-compliant EFS file systems.\n- Encrypt them using the AWS KMS CMKs. If CMKs have not been created, create a new one using AWS KMS.\n- Verify that the encryption has been successfully implemented.\n```\n\nThis AWS control helps in managing data security risks and meeting compliance requirements. CMKs provide you control and visibility into your key usage in AWS, making your file systems secure from unauthorized access."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the AWS Control that requires Amazon Elastic File Systems (Amazon EFS) encryption using Customer Master Key (CMK) can be elaborated through the following aspects:\n\n1. **Risk to Data Security**: The most prominent cost is the risk posed to data security. If Amazon EFS is not encrypted using CMK, it can potentially be accessible to unauthorized users, risking the leakage of potentially sensitive data. This can lead to financial penalties but also significant reputational damage.\n\n2. **Regulatory Penalties**: Many industries (healthcare, finance, etc.) are required to comply with specific regulations (like HIPAA, PCI DSS, GDPR), and they often mandate the encryption of sensitive data at rest. Non-compliance could result in large fines and penalties.\n\n3. **Loss of Client Trust**: If a security breach occurs due to non-encryption, it could lead to the loss of client trust, impacting business relationships and future contracts. This could have a long-term impact on the revenue and growth of the company.\n\n4. **Impact on Audit Results**: For companies that have audits around infrastructure and data security, non-encrypted EFS could lead to poor audit results. This may impact the company's legal standing and its reputation in the industry.\n\n5. **Potential System Vulnerabilities**: Without encryption, data is more vulnerable to cyber-attacks. This could lead to system downtime, which indirectly links to productivity and revenue loss.\n\nIn summary, non-compliance to this rule could potentially have devastating, multi-faceted effects, compromising data security, legal standing, company reputation, client relationships, and productivity."
      ],
      "x-kaytu-usefulness-example": [
        "Given an example of a globally operating e-commerce company, it stores sensitive financial data and payment information of its customers on the Amazon Elastic File System (EFS). The AWS rule that ensures EFS is encrypted using Customer Master Key (CMK) becomes highly beneficial for this organization.\n\nIf, for instance, a malicious entity attempts to gain unauthorized access to this data, the encryption offered by the CMK ensures that the raw data cannot be read without having the appropriate decryption key. \n\nAn audit or compliance check is done and it discovers that the EFS file system is not encrypted. In this case, the rule would suggest non-compliance, prompting immediate action from security professionals to rectify the issue and continue preventing potential data breaches.\n\nHere is the usage in markup format:\n\n```markup\nResources:\n  MyEfsFileSystem:\n    Type: \"AWS::EFS::FileSystem\"\n    Properties:\n      PerformanceMode: generalPurpose\n      FileSystemTags:\n        - Key: Name\n          Value: MyEncryptedEfsFileSystem\n      KmsKeyId: \"arn:aws:kms:us-west-2:111122223333:key/abcd1234-a123-456a-a12b-a123b4cd56ef\" # CMK\n      Encrypted: true\n``` \n\nIn this CloudFormation snippet, the EFS is being encrypted with a specific CMK as specified by the KmsKeyId property. The Encrypted property being set to `true` means that the file system will be encrypted."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_user_console_access_mfa_enabled",
    "Title": "IAM users with console access should have MFA enabled",
    "Description": "Manage access to resources in the AWS Cloud by ensuring that MFA is enabled for all AWS Identity and Access Management (IAM) users that have a console password.",
    "QueryID": "aws_iam_user_console_access_mfa_enabled",
    "DocumentURI": "policies/aws_iam_user_console_access_mfa_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/IAM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS Management Control is a practicable measure that enables secure access to AWS resources. The following is an interpretation of the mentioned AWS control.\n\n```markdown\n# AWS Management Control: MFA for IAM Users\n\n[SOURCE](https://aws.amazon.com/premiumsupport/knowledge-center/mfa-iam-user/)\n\nManage the access to the resources on AWS Cloud by enforcing Multi-Factor Authentication (MFA) for all AWS Identity and Access Management (IAM) users who have access to the AWS console via passwords.\n\n## Description\n\nThis policy ensures that a second factor of authentication (i.e., MFA) is required to access the AWS console. By implementing this, access to AWS resources becomes more secure as the likelihood of unauthorized access is significantly reduced. \n\n## Benefits\n\n- Increases security by adding another layer of protection \n- Reduces chances of unauthorized access\n- Aligns with best practices for secure access management\n\n## Implementation Steps\n\n1. Sign into the AWS Management Console and open the IAM console.\n2. In the navigation pane, choose \"Users.\"\n3. Choose the name of the intended user.\n4. Under \"Security Credentials,\" against \"Assigned MFA device,\" choose \"Manage.\"\n5. Follow online documentation to complete the MFA enabling process.\n\n## Related Resources\n\n- [AWS: What is MFA?](https://aws.amazon.com/iam/features/mfa/)\n- [Enabling a Virtual Multi-factor Authentication (MFA) Device](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_virtual.html)\n```\n\nThe above document in markdown format describes the particular AWS control designed to ensure secure access to the AWS console. It summarizes the purpose of the AWS control with mention of benefits and how it can be implemented. Links are also provided for further reading or gaining in-depth understanding."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the mentioned AWS (Amazon Web Services) control can result in the following costs:\n\n1. *Security Risks:* Without Multi-Factor Authentication (MFA), your cloud resources are more vulnerable to unauthorized access. If a user's credentials are compromised, the attacker would be able to access resources unhindered. This could lead to instances of data breaches or unauthorized modifications to live systems, potentially damaging a business reputationally and financially.\n\n2. *Increased Operational Costs:* If a data breach or unauthorized access occurs, significant resources may need to be spent to address the violation. This could include incident response efforts, system recovery, and work to improve security measures.\n\n3. *Regulatory Fines:* Depending on the nature of the data and systems being managed in AWS, there could be regulatory requirements for using MFA. Non-compliance could therefore result in fines or penalties from regulatory bodies.\n\n4. *Loss of Trust:* If stakeholders (such as business partners or customers) come to know that adequate security measures were not taken to protect critical resources or data, this could result in a loss of trust in the business, potentially impacting future revenues.\n\n5. *Business Continuity Risk:* Data breaches or unauthorized access can, in some cases, disrupt business operations—either through the damaging activities of the intruder, or via the necessary remediation activities. This can result in a loss of revenue due to disrupted operations. \n\nIn conclusion, enabling MFA for all AWS IAM users that have a console password is a small but critical step in protecting your AWS resources. Its non-compliance not only increases security risks but also can lead to increased operational costs, potential regulatory fines, loss of trust, and business continuity risks."
      ],
      "x-kaytu-usefulness-example": [
        "Example:\n\nA company called XYZ Corp runs their entire infrastructure on AWS. They have multiple applications running on various AWS services and each application is managed by a different team within the organization. All these teams have IAM users created to access the AWS console and perform necessary activities.\n\nOne day, an IAM user from the application management team, unaware of a phishing attack, accidentally entered his AWS console credentials into a fraudulent website. The attacker now had access to these credentials and quickly logged into the AWS console, starting to manipulate the application configurations. \n\nFortunately, XYZ Corp had enforced MFA for all IAM users who have a console password. Even though the attacker had the IAM user's login credentials, they could not get past the MFA challenge, as they did not have access to the user's authentication device or application to generate the necessary MFA codes. The MFA measure successfully thwarted the attacker's attempt to compromise the application's security.\n\nHence, managing access to resources in the AWS Cloud by requiring MFA for all IAM users with a console password was vital in protecting XYZ Corp's sensitive applications and data. This control played a crucial role in enhancing their security posture by adding another protective layer and ensuring only authorized and authenticated users could access the AWS resources."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_dynamodb_table_auto_scaling_enabled",
    "Title": "DynamoDB table auto scaling should be enabled",
    "Description": "Amazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to adjust provisioned throughput capacity that automatically responds to actual traffic patterns.",
    "QueryID": "aws_dynamodb_table_auto_scaling_enabled",
    "DocumentURI": "policies/aws_dynamodb_table_auto_scaling_enabled.md",
    "ManualVerification": false,
    "Severity": "low",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/DynamoDB"
      ],
      "x-kaytu-explanation": [
        "Amazon DynamoDB auto scaling applies to read and write capacity settings for tables and global secondary indexes. With auto scaling, you can define upper and lower bounds for read and write capacity units. When workload changes, the auto scaling feature adjusts capacity settings automatically within these bounds.\n\nThe primary purpose of auto scaling in Amazon DynamoDB is to help applications maintain their performance goals even when the workload changes. It offers a balance between cost-effectiveness by addressing the needs of read-intensive or write-intensive workloads that are constantly changing.\n\nAWS Application Auto Scaling service is integrated with DynamoDB, which takes care of automatic read and write capacity scaling by using a target tracking scaling policy, thus adding the necessary resources when needed and scaling down when the demand decreases. \n\nHere is a simple markup example to explain this feature:\n\n```markdown\n# AWS Application Auto Scaling in Amazon DynamoDB\n\nAmazon DynamoDB auto scaling uses the AWS Application Auto Scaling service to dynamically adjust the provisioned throughput capacity, which responds automatically to the actual traffic patterns. \n\n## Key Points\n\n1. **Purpose:** Auto scaling helps to maintain applications' performance goals even under a changing workload. \n\n2. **Cost-effective**: The feature offers cost-effectiveness by addressing the needs of constantly changing workloads. \n\n3. **Integration with AWS Auto Scaling service:** AWS Application Auto Scaling service takes care of automatic read and write capacity scaling, which is dynamically adjusted according to the need.\n\n4. **Target Tracking Scaling Policy:** This policy enables adding necessary resources when the demand increases and scaling down when the demand decreases.\n\n## Usage\n\nTo maintain the performance of your applications, define upper and lower bounds for your read and write capacity units, and the DynamoDB auto scaling feature will adjust the capacity settings automatically within these bounds, according to the workload changes.\n```"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS control using Amazon DynamoDB auto scaling could result in several potential costs:\n\n1. **Performance Issues**: Without auto scaling, the application may face performance issues. During traffic surges, if the read/write capacity is not sufficient, the requests may be throttled, resulting in slower response times and a poor user experience.\n\n2. **Financial Loss**: If you manually provision more capacity than required to account for potential traffic increase, you may end up spending more money than necessary paying for unused capacity.\n\n3. **Operational Efficiency**: Manual management of read/write capacity demands constant monitoring and adjusting to accommodate fluctuating traffic loads, which requires dedicated resources. Failing to implement auto scaling might result in inefficient use of your operational resources.\n\n4. **Scalability and Resource Optimization**: One of the key benefits of cloud services like AWS is the ability to scale resources according to the demand. If you’re not taking advantage of auto scaling, you could potentially limit your application's ability to grow and properly utilize the resources.\n\nIn sum, non-compliance with this AWS control can lead to performance degradation, higher costs, reduced efficiency, and limited scalability."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, consider an eCommerce company that deploys its multiple microservices architecture on AWS while serving millions of users worldwide. They experience highly variable load traffic that continuously increases, particularly in festive seasons. They use Amazon DynamoDB for managing the users and product information. Due to the unpredictable surge in traffic, they often find themselves over-provisioning or under-provisioning the read-write capacity. \n\nIf they over-provision, they spend more money than needed. If they under-provision, the database performance goes down, severely impacting user experience and, ultimately, the company's reputation. This is where DynamoDB auto scaling shows its usefulness.\n\nBy enabling DynamoDB auto scaling, the eCommerce company can maintain application performance and cost-efficiency. When the traffic increases, DynamoDB auto scaling automatically adjusts provisioned throughput capacity upward to maintain continued performance. When the traffic decreases, it automatically scales down the provisioned capacity to reduce cost. This ensures that the database's response time remains consistent, hence delivering smooth user experience and maintaining the company's stellar reputation. \n\nThe auto scaling feature also helps them handle the traffic surge during festive seasons without manual intervention, allowing their engineering teams to focus on more strategic tasks."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_account_password_policy_strong",
    "Title": "Password policies for IAM users should have strong configurations",
    "Description": "This control checks whether the account password policy for IAM users have strong configurations.",
    "QueryID": "aws_iam_account_password_policy_strong",
    "DocumentURI": "policies/aws_iam_account_password_policy_strong.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/IAM"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Account Password Policy for IAM Users\n\nThis AWS control is designed to ensure that the account password policy for IAM (Identity and Access Management) users is strongly configured. A strong configuration generally means that the password policy includes specific requirements that increase security such as:\n\n- Minimum password length: This setting specifies the minimum number of characters a password must contain.\n- Require at least one uppercase letter: Ensures that passwords contain a mix of uppercase and lowercase letters.\n- Require at least one lowercase letter: Further ensures the mix of case used in passwords.\n- Require at least one number: This introduces complexity, requiring passwords to not only contain letters.\n- Require at least one non-alphanumeric character: This further increases complexity beyond alphanumeric characters (A-Z, a-z, 0-9).\n- Enable password expiration: Ensures that users change passwords regularly.\n- Prevent password reuse: Stops users from reusing the same, potentially compromised, passwords.\n\nBy enforcing these requirements, the control aims to mitigate the risk of unauthorized access due to weak or predictable passwords. AWS IAM users are required, as per this control, to adopt strong and complex passwords to enhance the overall security of their AWS resources. \n\nAs an AWS user, you have the ability to set and manage the password policy for your account in the IAM console, CLI, or AWS API, thereby aligning with the requirements of this control. \n\n```markdown\nAWS Control: Account Password Policy for IAM Users\nThis is a security measure that checks whether IAM user account password policies have been configured with high strength settings. These may include:\n\n- A required minimum password length.\n- A requirement for at least one uppercase letter in each password.\n- A requirement for at least one lowercase letter in each password.\n- A requirement for at least one number in each password.\n- A requirement for including at least one non-alphanumeric character in each password.\n- Enabling password expiration to enforce periodical password change.\n- Preventing password reuse to avoid using potentially compromised passwords.\n```\nThis control ensures maximum security against unauthorised access by enforcing the usage of strong and complex passwords. This control can be managed and set in the IAM console, CLI, or AWS API as per the user's convenience."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance with this AWS Control can be significant and multifaceted.\n\n1. **Security breaches or Cyber Attacks**: Without a strong password policy, weak or easily guessable passwords can be used, making your AWS infrastructure vulnerable to unauthorized access, hacking, and data breaches. Cyber criminals can exploit these weak points, which can lead to sensitive data exposure, loss, or theft.\n\n2. **Financial cost**: A successful security breach often results in financial loss. This can be a direct loss (like financial data being stolen and used), costs associated with addressing the breach, lost business due to downtime, or even compensation to customers if their data was compromised.\n\n3. **Regulatory Penalties**: Organizations also face strict regulatory fines for non-compliance with data protection laws and standards. In some industries, these fines can be substantial. For instance, GDPR in Europe can fine organizations up to 4% of their global annual revenue for serious violations.\n\n4. **Reputation Damage**: If it is publicly disclosed that your organization had a data breach, this can harm your reputation. This can lead to loss of customers, difficulties in acquiring new customers, and overall can have a long term negative impact on your business.\n\n5. **Violation of AWS Agreement**: If the non-compliance leads to a security incident, it can result in a violation of the AWS Customer Agreement, which can lead to actions ranging from additional audits and investigations to account suspension or termination.\n\nMaintaining a strong password policy is important to prevent these potential issues. This policy should be a part of a broader identity and access management (IAM) strategy that includes multi-factor authentication (MFA), least privilege access, and regular reviews and updates of user access rights."
      ],
      "x-kaytu-usefulness-example": [
        "## Example\n\nCompany XYZ is a digital services provider and is known for dealing with customer sensitive information. The IT systems administrator has been faced with a challenge of constantly dealing with unauthorized access attempts suspected to be due to weak passwords. \n\nThe administrator decided to enforce a robust account password policy for IAM users. Using AWS, they implemented a control that checks whether the account password policy for IAM users have strong configurations. \n\nHere is an example of the markup configuration:\n\n```bash\n{\n    \"Version\": \"2021-10-11\", \n    \"Statement\": [\n        {\n            \"Sid\": \"AWSControlStrongPasswordPolicy\", \n            \"Effect\": \"Allow\", \n            \"Action\": \"iam:CreateVirtualMFADevice\", \n            \"Resource\": \"*\"\n        }, \n        {\n            \"Sid\": \"EnableMFAEnforcement\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:*\",\n            \"Resource\": \"*\",\n            \"Condition\": {\"BoolIfExists\": {\"aws:MultiFactorAuthPresent\": \"false\"}}\n        }, \n        {\n            \"Sid\": \"PasswordLength\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:UpdateAccountPasswordPolicy\",\n            \"Resource\": \"*\",\n            \"Condition\": {\"NumericLessThanEquals\": {\"iam:PasswordLength\": \"14\"}}\n        }, \n        {\n            \"Sid\": \"PasswordReusePrevention\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:UpdateAccountPasswordPolicy\",\n            \"Resource\": \"*\",\n            \"Condition\": {\"NumericLessThanEquals\": {\"iam:PasswordReusePrevention\": \"5\"}}\n        }\n    ]\n}\n```\n\nThe code checks for 2FA, a minimum of 14 characters length for passwords and does not allow reusing the last five passwords. This significantly increases the strength of the password policy, thereby enhancing the security of the company's data and systems.\n\nBy using the control, the company minimized the risk of unauthorized access metrics associated with weak passwords, ultimately helping to prevent potential breaches and data loss."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_instance_multiple_az_enabled",
    "Title": "RDS DB instance multiple az should be enabled",
    "Description": "Multi-AZ support in Amazon Relational Database Service (Amazon RDS) provides enhanced availability and durability for database instances.",
    "QueryID": "aws_rds_db_instance_multiple_az_enabled",
    "DocumentURI": "policies/aws_rds_db_instance_multiple_az_enabled.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/RDS"
      ],
      "x-kaytu-explanation": [
        "# Multi-AZ Support in Amazon RDS\n\nAmazon Relational Database Service (Amazon RDS) makes it easy to set up, operate, and scale relational databases in the cloud. One such feature provided by Amazon RDS, known as Multi-AZ support, helps enhance the availability and durability of your database instances, making it a suitable option for mission-critical workloads.\n\n## Amazon RDS Multi-AZ Deployments\n\nThis feature ensures database availability by automatically replicating database instances into a standby in a different Availability Zone. The availability zones are isolated locations within data centers, designed to be insulated from failures in other availability zones.\n\n## Benefits of Multi-AZ Support\n\n- **Increase Availability**: Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. \n\n- **Data Durability**: In case of an infrastructure failure (like an instance failure), Amazon RDS performs an automatic failover to the standby, allowing database operations to resume quickly without manual intervention.\n\n- **Efficient Disaster Recovery**: The standby replica not only serves as a failover in the event of a failure, it also plays a crucial role in disaster recovery. \n\n## Summary\n\nIn essence, the Multi-AZ support in Amazon RDS contributes to business continuity by reducing downtime in the event of a database instance failure. If your database carries mission-critical loads where downtime results in considerable operational or monetary loss, using Multi-AZ support with Amazon RDS will add an additional layer of protection and availability to your database instances."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the Multi-AZ (Availability Zone) support in Amazon Relational Database Service (Amazon RDS) can lead to several costs, both financial and operational:\n\n1. **Downtime Costs:** Non-compliance with Multi-AZ support could mean your database instance isn't duplicated across multiple availability zones. If the single availability zone hosting your database fails, it could lead to longer downtime, reducing business productivity. The cost associated with downtime varies based on the business, but it can be substantial.\n\n2. **Data Loss Costs:** In the case of a catastrophic failure of the availability zone hosting your instance, there is potential for data loss. The costs associated with data loss can be huge, such as potential lost revenue, cost of recovery, fines related to data loss for certain types of data, and loss of business reputation.\n\n3. **Increased Failover Times:** Non-replicated instances may increase failover time. This means if one database instance fails, it will take longer for a new one to start up, this delay can negatively impact business operations.\n\n4. **Business Continuity Risk:** Without Multi-AZ support, organizations run the risk of business operations being unavailable or stopped during an availability zone failure. This can cost hundreds of thousands to millions of dollars, depending on the size and scale of the operations. Compliance with Multi-AZ support is essential for maintaining business continuity.\n\n5. **Higher Operational Costs:** Operating a single-instance database might initially seem cheaper. However, in the long run, operational costs could be higher due to the extra resources and time required to manage backups, restore operations, handle disaster recovery, and manage the database during crisis situations when compared with a Multi-AZ setup.\n   \nIt's crucial for businesses to maintain compliance with AWS controls like Multi-AZ support for RDS for more reliable application performance, data security, and cost efficiency."
      ],
      "x-kaytu-usefulness-example": [
        "Multi-AZ support in Amazon RDS can be particularly useful for businesses which heavily rely on their web applications or mobile apps to provide services to their customers. Amazon RDS Multi-AZ deployments provide enhanced availability and reliability for database instances, making these ideal for production workloads. \n\nFor example, let's consider an ecommerce business which runs a high-traffic, e-commerce platform with millions of transactions per day. The database is a major component of their platform as it stores critical information - customer data, transaction history, product catalog, etc. Downtime or data loss is not acceptable as it could lead to loss of business and diminished customer trust.\n\nBy using Amazon RDS with Multi-AZ support, they ensure their database instances are continuously replicated, which prevents data loss in the event of instance failure. If a database instance fails, Amazon RDS can automatically failover to a standby replica in another Availability Zone. Just as important, the Multi-AZ set up can cater to heavy database loads created by the high traffic - the read traffic can be sent to multiple replicas. This can significantly enhance the responsiveness and overall performance of their app, providing a much better user experience and helping the business retain their customers.\n\n```\nBenefits:\n\n1. **Improved Availability**: Multi-AZ deployments for the MySQL, MariaDB, Oracle, and PostgreSQL engines utilize synchronous physical replication to keep data on the standby up-to-date with the primary. In case of infrastructure failure, Amazon RDS performs an automatic failover to the standby, minimizing disruption of service.\n\n2. **Data Durability**: In a Multi-AZ deployment, your data is continuously replicated to a standby in a different Availability Zone for data redundancy and failover support.\n\n3. **Scalability**: By handling heavy database loads with multiple replicas, it can significantly enhance the responsiveness and performance of applications.\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_cloudtrail_trail_validation_enabled",
    "Title": "CloudTrail trail log file validation should be enabled",
    "Description": "Utilize AWS CloudTrail log file validation to check the integrity of CloudTrail logs. Log file validation helps determine if a log file was modified or deleted or unchanged after CloudTrail delivered it. This feature is built using industry standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection.",
    "QueryID": "aws_cloudtrail_trail_validation_enabled",
    "DocumentURI": "policies/aws_cloudtrail_trail_validation_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/CloudTrail"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Using CloudTrail Log File Validation\n\nAWS CloudTrail provides a feature called log file validation, which helps ensure the integrity and reliability of your log files. This control is designed to detect and notify you of any modifications or deletions to your CloudTrail logs.\n\n## What is it?\n\nCloudTrail log file validation uses industry-standard algorithms, SHA-256 for hashing and SHA-256 with RSA for digital signing, making it computationally impractical to modify, delete, or forge logs undetected.\n\n## How Does it Work?\n\nCloudTrail log file validation works by creating a digital fingerprint, known as a hash, for every log file. It then digitally signs the hashes using SHA-256 with RSA. This creates an encrypted representation of the content that can be used to verify its integrity.\n\nWhen a log file is delivered to your Amazon S3 bucket, CloudTrail also uploads a 'digest' file. This file contains the hashes of all log files that are delivered to your bucket within an hour. You can use these digest files and the accompanying digital signature file to validate the integrity of your log files.\n\n## Why Use Log File Validation?\n\nBy using CloudTrail log file validation, you can:\n\n- Verify that no log files were modified, deleted, or added in your S3 bucket\n- Identify which log files have been delivered to your account, ensuring complete visibility of operations and security-related events\n- Use Amazon S3 server-side encryption or AWS KMS encryption to secure your log files\n\n```\nNote: Once the log file validation is enabled, it cannot be disabled for CloudTrail. To stop log file validation, you must delete your trail and create a new one.\n```\n## Enabling Log File Validation\n\nTo enable CloudTrail log file validation, follow these steps:\n\n1. Open the CloudTrail console\n2. Select the trail for which to enable log file validation\n3. In the 'S3' section, check the 'Log file validation' checkbox\n4. Scroll down and select 'Save changes'\n\nBy ensuring the integrity of your log files, you can create a more secure AWS environment, effectively monitor your AWS environment, and prove compliance as needed."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with the AWS control to utilize CloudTrail log file validation can have significant cost implications, such as;\n\n1. **Unexpected Costs:** AWS charges for usage, and an attacker who gain access can create extensive activities leading to high unexpected costs. \n\n2. **Loss of Data Integrity:** If CloudTrail log files are not validated and an unauthorized individual alters them, it could lead to wrong decision making when analysing the log data.\n\n3. **Security Breach Cost:** If log files are tampered with, it could cover attackers' tracks, making breaches difficult to detect which could lead to serious financial losses due to data theft or sabotage.\n\n4. **Non-compliance Penalties:** Failure to utilize log file validation could lead to non-compliance to certain regulatory standards (such as GDPR, HIPAA, NIST etc). Non-compliance could result in severe fines.\n\n5. **Reputation Damage:** If a security breach occurs and it becomes apparent that log file validation was not in place, this could lead to a damaged reputation, result is loss of trust from customers and potential financial loss.\n\n6. **Audit Failure:** If an audit is conducted and it's discovered that log file validation was not utilized, it could lead to audit failure and potential penalties.\n\n7. **Investigation and Recovery Costs:** If data breaches or security incidents occur, a lot of resources may have to be spent on investigation and remediation processes.\n\nHence, the indirect and direct costs of non-compliance can be way greater than the cost of implementing and maintaining required controls like CloudTrail log file validation."
      ],
      "x-kaytu-usefulness-example": [
        "Example:\n\nLet's say you have a company that heavily relies on AWS services for its digital infrastructure. For security and auditing purposes, your company needs to keep accurate records of API activity in its AWS accounts. AWS CloudTrail is being used to log all these activities.\n\nOne day, you suspect that an unauthorized access attempt has been made by a freelancing team which recently left your project. You think that they might have tried to modify or delete CloudTrail logs to cover their paths. AWS CloudTrail log file validation feature comes in handy in this situation. With this feature enabled, each delivered log file is signed with a digital signature. When you are reviewing logs, you can confirm the integrity of each log file to make sure they have not been tampered with by checking against the provided hash generated by SHA-256 algorithm. This allows you to be sure of the validity of your logs files, even if someone with malicious intent tries to modify or delete them.\n\nSo, the use of AWS CloudTrail log file validation enhances your ability to track and observe the activities in your AWS environment, adding an additional layer of security and integrity checking."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ec2_instance_not_publicly_accessible",
    "Title": "EC2 instances should not have a public IP address",
    "Description": "Manage access to the AWS Cloud by ensuring Amazon Elastic Compute Cloud (Amazon EC2) instances cannot be publicly accessed.",
    "QueryID": "aws_ec2_instance_not_publicly_accessible",
    "DocumentURI": "policies/aws_ec2_instance_not_publicly_accessible.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/EC2"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "This AWS Control suggests developing and implementing rules to restrict public access to Amazon Elastic Compute Cloud (EC2) instances. This control is significant for enhancing the security of your AWS resources and ensuring that only authorized individuals can access the EC2 instances.\n\n```markdown\n# AWS Control: Manage Access to Amazon EC2 Instances\n\nThe focus of this control is to restrict public access to Amazon Elastic Compute Cloud (EC2) instances in a way that only authorized users have access to them, which enhances the security of your AWS resources.\n\n## Key Features\n- **Security Groups:** Configure Security Groups, a virtual firewall for your EC2 instances, to control inbound and outbound traffic.\n- **Network Access Control Lists (NACLs):** Provides additional layer of security by controlling inbound and outbound traffic at the subnet level.\n- **Access Control Policies:** Define IAM policies to specify which actions are allowed or denied for a user on specific EC2 instances.\n- **VPC Endpoints:** Allows to connect your VPC directly to AWS services and restrict access to the internet.\n- **Private Subnets:** Assign instances to private subnet in which instances don't have a direct route to the internet.\n- **AWS VPN or Direct Connect:** Securely connect your on-premises network to AWS.\n\n## Implementation\nThe implementation of these features will enable a robust, secure environment for your EC2 instances, ensuring they can't be publicly accessed and are protected from unwanted intrusion. \n```\n"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS control of managing access to the Amazon Elastic Compute Cloud (EC2) instances can lead to several costs:\n\n1. **Security Costs**: If your EC2 instances are publicly accessible, the security of your data and systems could be compromised. Unauthorized individuals, or attackers, can gain access and potentially harm your system. This could lead to loss of data, breach of customer information, and other serious security concerns.\n  \n2. **Financial Costs**: The breach or misuse of public EC2 instances could lead to increased costs in several ways. Firstly, if an attacker gains access, they could manipulate or consume resources leading to increased costs. Second, you could face regulatory fines if customer data is compromised. Lastly, you would need to invest more in further securing your systems after a breach occurs.\n  \n3. **Reputation Costs**: If the security breach leads to compromised customer data, your reputation could be affected seriously. It could lead to loss of customer trust, negative publicity and a decrease in business.\n  \n4. **Operational Costs**: If your EC2 instances are not properly managed and unauthorized access happens, it could lead to disruptions in service, failure of operations, and requires additional resources to rectify.\n    \n5. **Regulatory Costs**: Non-compliance with various data protection regulations (like GDPR, CCPA, HIPAA, etc.) due to publicly accessible instances can result in hefty fines and legal troubles.\n\nIn summary, not managing access to your EC2 instances can risk exposing your systems and data to the public, which could lead to a variety of security, financial, operational, and reputational costs. Compliance with this AWS control is critical for maintaining the integrity and security of your services in the cloud."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, an organization has a team of developers and testers working on a project which is hosted on AWS, using Amazon EC2 instances. They have a variety of EC2 instances storing sensitive data related to the project, such as project plans, code repositories, database credentials, user data, etc. \n\nHowever, they don't want these instances to be publicly accessible due to security concerns – they want to avoid possible data breaches, unauthorized access, or exploitation of potential system vulnerabilities. Using the AWS control to restrict public access to their EC2 instances ensures that only authorized team members can gain access to the instances, while keeping the system secure from external threats. \n\nSo the control of managing access to the AWS Cloud by ensuring Amazon EC2 instances cannot be publicly accessed is significantly useful for the organization as it helps maintain the confidentiality and integrity of their project.\n\n```\nExample:\nThis AWS Control can be managed through Security Groups and Network Access Control List (NACL) in Amazon EC2. \n\n// Deny public access in Security Groups\n```yml\nType: 'AWS::EC2::SecurityGroup'\nProperties: \n  GroupDescription: Deny public access\n  SecurityGroupIngress: \n    - IpProtocol: '-1'\n      CidrIp: '10.0.0.0/24'\n      FromPort: '0'\n      ToPort: '65355'\n```\n\n// Deny public access in NACL\n```yml\nType: \"AWS::EC2::NetworkAclEntry\"\nProperties: \n  NetworkAclId: !Ref MyACL\n  RuleNumber: 100\n  Protocol: \"-1\"\n  RuleAction: deny\n  Egress: false\n  CidrBlock: 0.0.0.0/0\n  PortRange: \n    From: 0\n    To: 65535\n```\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_s3_bucket_default_encryption_enabled",
    "Title": "S3 bucket default encryption should be enabled",
    "Description": "To help protect data at rest, ensure encryption is enabled for your Amazon Simple Storage Service (Amazon S3) buckets.",
    "QueryID": "aws_s3_bucket_default_encryption_enabled",
    "DocumentURI": "policies/aws_s3_bucket_default_encryption_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/S3"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# Protecting Data at Rest in Amazon S3\n\n## Introduction\n\nEncrypting data at rest is a critical component in your data security plans. It aids in guarding your data from unauthorized access. This document provides instructions about how to implement this feature in your Amazon Simple Storage Service (S3) buckets, often referred to as S3 Encryption.\n\n## Prerequisites\n\nBefore proceeding, ensure you have the following:\n- An active Amazon Web Services (AWS) account\n- Basic knowledge of Amazon S3\n- Access to the Amazon S3 management console\n\n## Steps\n\n1. **Sign in to AWS Management Console:**\nSign in to your AWS account and navigate to the S3 section.\n\n2. **Create or Open Your S3 Bucket:**\nCreate a new S3 bucket, or open an existing one. This is where you will enable the encryption.\n\n3. **Navigate to Bucket Settings:**\nIn your chosen bucket, navigate to the Bucket Settings section.\n\n4. **Enable Default Encryption:**\nInside the bucket settings, find the \"Default encryption\" option and enable it. \n\n5. **Choose Encryption Algorithm:**\nYou will have two options:\n    - **SSE-S3:** AWS handles key management.\n    - **SSE-KMS:** You manage the keys through AWS Key Management Service (KMS).\n  \nChoose one based on your requirements.\n\n6. **Save Your Settings:**\nAfter selecting desired settings, don't forget to save them.\n\nRemember, enabling encryption for your S3 buckets helps to protect your business-critical, sensitive, and confidential data from unwanted threats and unauthorized access online.\n\n## Conclusion\n\nIt is important to regularly review and ensure that encryption is enabled for all the S3 buckets in your AWS account. By following these steps, you can strengthen your organisation's security in the AWS environment.\n\nFor more detailed information, refer to the official AWS documentation on [How to enable default encryption for an S3 bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/default-bucket-encryption.html).\n\n## Caution\n\nRemember, encryption alone cannot fully protect your data. Carefully manage access permissions to your buckets, monitor activity, and establish strong identity and access management policies."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS control can result in several costs, including:\n\n1. **Data Breaches**: Unencrypted data at rest is vulnerable to unauthorized access, potentially leading to data breaches. The consequent costs could include remediation expenses, customer notifications, legal fees, regulatory fines, and reputational loss.\n\n2. **Loss of Business**: If client data is compromised because encryption is not enabled for your S3 buckets, this could result in a loss of client trust and business. This can lead to a significant drop in revenue.\n\n3. **Non-compliance Penalties**: Depending on the jurisdiction and the industry, organizations may face heavy fines and penalties for non-compliance with data protection regulations such as GDPR, HIPAA, or PCI-DSS. \n\n4. **Forensic Costs**: If a data breach occurs, there might be a need for a forensic investigation to determine the cause and extent of the breach, adding to the overall costs.\n\n5. **Operational Disruptions**: Data breaches could result in significant business disruptions, causing loss of productivity and revenue.\n\nTo prevent these heavy financial and potential legal costs, it is essential to ensure encryption is enabled for your Amazon S3 buckets to protect data at rest."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, a financial company stores sensitive customer information such as credit card details, bank account numbers, and financial transaction history in Amazon S3 buckets. To ensure this data is safe and complies with regulatory standards, the company enables encryption for their Amazon S3 buckets. This means, even if an unauthorized person was able to somehow access this data, they wouldn't be able to understand it without the decryption keys. Therefore, the use of encryption significantly enhances the security of the data and protects it from potential breaches or leaks."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_user_no_inline_attached_policies",
    "Title": "IAM user should not have any inline or attached policies",
    "Description": "This rule ensures AWS Identity and Access Management (IAM) policies are attached only to groups or roles to control access to systems and assets.",
    "QueryID": "aws_iam_user_no_inline_attached_policies",
    "DocumentURI": "policies/aws_iam_user_no_inline_attached_policies.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/IAM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Attachment of IAM Policies\n\nThis AWS control emphasizes the best practice of linking Amazon Web Services (AWS) Identity and Access Management (IAM) policies solely to groups or roles.\n\n## Overview\n\nIAM is a feature of AWS that aids in managing access to AWS resources securely. You can create and manage policies - which are sets of permissions, and attach them to IAM users, groups of users, or roles to define what actions they can perform on which resources.\n\nHowever, to maintain a secure and manageable environment, it is recommended to attach policies to groups or roles, instead of individual users. This way, we reduce potential security risks and make managing permissions across multiple users easier.\n\n## Key Takeaways\n\n1. **Roles** - Roles are used to grant specific privileges to specific actors at specific times, under specific conditions. An actor can be an Amazon user or a service.\n\n2. **Groups** - A group is a collection of IAM users. You can attach a policy to a group made of IAM users to define their permissions.\n\n3. **Users** - IAM users are AWS customers who are given access to the AWS Management Console. Attach policies to groups or roles instead of users to reduce potential security risks.\n\n4. **Policies** - A policy is an object in AWS that defines permissions. These policies are used to determine whether the request is allowed or denied. \n\n## The Control Rule\n\nThe rule implies that all IAM policies should only get attached to roles or groups in AWS. This is mainly done to ensure a high level of security - minimizing the chances of a user with excessive permissions causing unintended consequences.\n\nThis control rule helps in keeping the management of access to systems and assets clean, organized, and secure.\n\n**Note:** While this control rule is crucial for maintaining an organized and secure infrastructure, it should be updated and enforced according to the unique needs of a business or AWS architecture."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS control can lead to several potentially expensive and damaging issues:\n\n1. **Security Risks:** Not properly attaching IAM policies to groups or roles can expose sensitive data and resources. Unwanted users may gain access to confidential information, leading to data breaches, or they could manipulate systems and processes, leading to operations disruptions.\n\n2. **Increased Operational Costs:** Without proper IAM policy attachment, you might need to assign permissions on an individual basis, which could escalate the operational costs. Any changes would need to be replicated across numerous users instead of just changing a group policy, leading to increased time expenditure and complexity.\n\n3. **Nonadherence to Compliance Requirements:** Many regulatory frameworks like GDPR, CCPA, HIPAA, ISO 27001 require strict access and identity management. Non-compliance with these can lead to potential fines, sanctions, and damage to reputation.\n\n4. **Loss of Accountability and Traceability:** Not properly managing IAM policies can cause a loss of accountability because it becomes difficult to monitor who has access to what resources. This can also lead to a lack of traceability, making it more difficult to conduct investigations in case of any security incident.\n\n5. **Increase in Errors:** Manual management of policies on an individual level can result in a higher chance of misconfiguration, leading to potential security loopholes.\n\nHere, it becomes important to follow this AWS control. Not only does it ensure credentials are not shared, but also that maximum security measures are in place as per AWS best practices."
      ],
      "x-kaytu-usefulness-example": [
        "For example, let's say you have multiple people in your organization working on a specific project named \"Project X\". These individuals might need access to certain AWS services or resources.\n\nWithout this control in place, you would have to manually grant each person the necessary permissions, which could be time-consuming and prone to error. However, by following this AWS control, you can create an IAM group named \"Project X\", attach the necessary policies to that group, and then add the relevant individuals to the group.\n\nIn this way, you simplify the process of managing access rights, reduce the chances of error, and establish a structured and organized access management system. Additionally, if someone joins or leaves the project, you can easily add or remove them from the group, rather than having to update numerous individual permissions.\n\n```markdown\nExample:\n\n1. **Creating IAM Group**\n```bash\naws iam create-group --group-name Project-X\n```\n2. **Attach Policy to the Group**\n```bash\naws iam attach-group-policy --group-name Project-X --policy-arn arn:aws:iam::aws:policy/AdministratorAccess\n```\n3. **Adding User to the Group**\n```bash\naws iam add-user-to-group --group-name Project-X --user-name Bob\n```\n```\nThis would be much easier in administering and managing as compared to managing each individual's access rights, and gives better security control over systems and assets."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_es_domain_encryption_at_rest_enabled",
    "Title": "ES domain encryption at rest should be enabled",
    "Description": "Because sensitive data can exist and to help protect data at rest, ensure encryption is enabled for your Amazon Elasticsearch Service (Amazon ES) domains",
    "QueryID": "aws_es_domain_encryption_at_rest_enabled",
    "DocumentURI": "policies/aws_es_domain_encryption_at_rest_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ES"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "Encryption of data stored within Elasticsearch domains is a critical aspect of data security. The Amazon ES supports encryption at rest, a security feature that helps in preventing unauthorized access to your data. This feature encrypts the underlying file systems that store your domain's indexed data, log files, automated snapshots, and all the artifacts used by the service to manage and recover your data. \n\nThis AWS Control specifies that you should enable encryption for your Amazon ES domains. This would mean that even if someone were to gain physical access to a disk, they could not read the data without the encryption key. \n\nEnabling this feature also changes the communication between replicated Elasticsearch instances from unencrypted to encrypted, which means inter-node communication for a domain uses Transport Layer Security (TLS).\n\nHere is the markup (code) format to enable encryption at rest using AWS CLI:\n\n```\naws es create-elasticsearch-domain --domain-name my-domain --ebs-options EBSEnabled=true,VolumeType=gp2,VolumeSize=10 --encryption-at-rest-options Enabled=true\n```\n\nIn this command:\n\n- `my-domain`: domain name of Elasticsearch\n- `EBSEnabled=true`: It enables the EBS-based storage\n- `VolumeType=gp2`: VolumeType of EBS-based storage\n- `VolumeSize=10`: Size of EBS-based storage\n- `Enabled=true`: It enables the encryption at rest\n\nPlease replace `my-domain` with the actual Elasticsearch domain name.\n\nPlease ensure necessary permissions are granted to perform these actions. Check AWS Identity and Access Management (IAM) documentation to understand about permissions and roles."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS control, that is not enabling encryption for Amazon Elasticsearch Service (Amazon ES) domains, could result in the following costs:\n\n1. **Financial Costs**: There can be direct financial costs resulting from data breaches, including fines or penalties imposed by regulatory bodies for not complying with data protection laws such as GDPR, CCPA, etc. Companies may also face lawsuits from customers or partners whose data has been compromised.  \n\n2. **Reputational Costs**: The loss of customer trust following a data breach could have long-term impacts on a company's reputation. This could subsequently lead to loss of existing customers, difficulty in acquiring new ones, and overall damage to the company's brand image.\n\n3. **Operational Costs**: In the event of a data breach, businesses may need to halt their operations temporarily, leading to loss of productivity. Recovery from a data breach can also result in significant resources spent on identifying the breach's origin, bolstering security measures, and repairing the damage caused by the breach - all of which add up to the operational cost.\n\n4. **Loss of Intellectual Property**: If the unencrypted data includes a company's intellectual property, then a data breach could lead to loss of competitive advantage and impact the company's market position.\n\n5. **Regulatory Costs**: Non-compliant organizations could be subject to audits by regulatory bodies and would need to spend time and resources to demonstrate compliance, correct any deficiencies, and possibly face imposed restrictions until the situation is corrected."
      ],
      "x-kaytu-usefulness-example": [
        "AWS Elasticsearch is a popular tool used for tasks such as log analytics and error monitoring. For example, a hospital may use Amazon ES to store and analyze patient data. This data is incredibly sensitive, requiring the highest levels of data protection. \n\nBy enabling encryption for Amazon ES domains, the hospital can add an additional layer of protection for the stored patient data. When data is encrypted at rest, it becomes indecipherable to unauthorized users. Therefore even if a data breach occurs and the raw data is accessed, the hacker would not be able to read or understand it.\n\nThe encryption safeguard would also satisfy many regulatory requirements, as numerous healthcare-related laws and standards have stringent requirements for data security. By securing the data in this way, the hospital can avoid fines and penalties associated with data breaches or non-compliance.\n\nIn addition, ensuring encryption is also beneficial for institutions that handle sensitive information such as financial institutes, educational institutes, governmental organizations etc. The AWS Control of encryption for Elasticsearch Service domains is vital in ensuring secure storage of data in these institutions.\n\nHere's a simple example in markup format:\n```yaml\nResources:\n  MyDomain:\n    Type: \"AWS::Elasticsearch::Domain\"\n    Properties:\n      DomainName: \"example-domain\"\n      ElasticsearchVersion: \"2.3\"\n      EBSOptions: {\n        \"EBSEnabled\" : true,\n        \"VolumeType\" : \"gp2\",\n        \"VolumeSize\" : 10\n      }\n      EncryptionAtRestOptions: {\n        \"Enabled\" : true\n      }\n```\n\nThis example encrypts an Amazon Elasticsearch Service domain named \"example-domain\" by enabling the `EncryptionAtRestOptions`.\nIn this case, AWS will handle the encryption and decryption automatically with keys managed through AWS Key Management Service (KMS)."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ec2_instance_protected_by_backup_plan",
    "Title": "EC2 instances should be protected by backup plan",
    "Description": "Ensure if Amazon Elastic Compute Cloud (Amazon EC2) instances are protected by a backup plan. The rule is non compliant if the Amazon EC2 instance is not covered by a backup plan.",
    "QueryID": "aws_ec2_instance_protected_by_backup_plan",
    "DocumentURI": "policies/aws_ec2_instance_protected_by_backup_plan.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/EC2"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control - Amazon Elastic Compute Cloud (EC2) Backup Plan \n\nAWS recommends protecting your Amazon EC2 instances by implementing a backup plan. \n\n## **AWS Control Overview**\n- **Control id**: `BackupPlanForEC2`\n- **Description**: Ensures that all Amazon EC2 instances are included in a backup plan to protect your data from accidental delete, failure, or other unexpected events.\n- **Rationale**: Implementing a backup plan can protect your EC2 instances from events such as human errors, system failures, or malicious attacks. Without a backup plan, instance data can be lost, and recovery from these unexpected scenarios can be challenging and time-consuming.\n- **Compliance**: This rule is non-compliant if an EC2 instance is not attached to a backup plan.\n\n## **Remediation Steps**\n\nIf an EC2 instance is not covered by a backup plan and is noncompliant,\n1. Open the AWS Management Console.\n2. Navigate to EC2 Dashboard.\n3. Select the EC2 instance that is not covered by a backup plan.\n4. Navigate to the Create Backup plan section.\n5. Configure the backup settings, including schedule, backup window, and backup retention period.\n6. Attach the backup plan to the undocumented EC2 instance.\n\n## **Evaluation**\nThe control evaluates the compliance status by determining whether all EC2 instances in the applicable account are protected by a backup plan. \n\n## **Related Resources**\n* [Backing Up Your Instances](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html)\n* [AWS Backup](https://aws.amazon.com/backup/)"
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance with this AWS Control can be summarized in the following points:\n\n1. **Data Loss**: Without a backup plan, there is a significant risk of data loss. This could be caused by accidental deletion, software bugs, or in the more severe case, due to malicious attacks like ransomware. Recovering lost data without a backup can be costly, or in some cases, impossible.\n\n2. **Downtime**: An AWS EC2 instance without a backup plan is vulnerable to disruptions. In the case of an outage or failure, the higher the downtime, the greater the business impact. Downtime costs varies from business to business but can range from hundreds to millions of dollars per hour depending on the size and nature of the business.\n\n3. **Compliance Penalties**: For some industries, particularly those in regulated industries like financial services or health care, non-compliance could bring about financial penalties and legal consequences. Not having a backup plan could violate laws related to data retention and disaster recovery.\n\n4. **Reputation Damage**: A company could suffer reputation damage in the event of data loss due to lack of a backup plan. This might lead to loss of customers, negative publicity and reduced customer confidence in the organization's ability to safeguard data.\n\n5. **Restoration Expenses**: In case of data loss or downtime, the process of restoring your AWS EC2 instances without a backup could be time-consuming and costly. This might involve hiring special expertise or acquiring additional resources to restore the service or recover the data.\n\nIn conclusion, not adhering to the control \"Ensure if Amazon EC2 instances are protected by a backup plan\" can potentially expose the organization to significant risk and costs in terms of data loss, downtime, legal penalties, reputation loss and further recovery costs."
      ],
      "x-kaytu-usefulness-example": [
        "This AWS Control is useful for maintaining business continuity and disaster recovery standards. For instance, a tech startup has deployed several Amazon EC2 instances to manage their customer data and application services. In case there is a system failure, data corruption, or any other unforeseen circumstances that result in data loss, the startup could face significant operational and reputational damages.\n\nHowever, if this AWS Control is implemented, it ensures that all EC2 instances are covered by a backup plan. Therefore, if any data loss incident occurs, the tech startup can restore the lost data from the backups, ensuring minimal disruption to their operations. This AWS Control thereby helps in managing the risks associated with data loss and maintaining operational continuity. \n\nUse of this AWS Control also facilitates compliance with various regulatory standards which mandate periodic backup of sensitive/essential data. Therefore, the tech startup will also be in a position to swiftly demonstrate their compliance with such regulations, helping them avoid potential regulatory fines and penalties. \n\n```\nAn example can be written as follows in markup format:\n\n**Given:** \nA tech start-up with several Amazon EC2 instances\n\n**AWS Control:** \nEnsure if Amazon Elastic Compute Cloud (Amazon EC2) instances are protected by a backup plan\n\n**Compliance Status:** \nThe control rule checks if the EC2 instances are covered by a backup plan. If not, it sets the status as non-compliant\n\n**Value of the Control:**\n1. Protects against data loss in case of system failures or data corruption incidents\n2. Ensures operational continuity by enabling data restoration from backups \n3. Helps in demonstrating compliance with regulatory standards mandating periodic data backup\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_log_group_encryption_at_rest_enabled",
    "Title": "Log group encryption at rest should be enabled",
    "Description": "To help protect sensitive data at rest, ensure encryption is enabled for your Amazon CloudWatch Log Group",
    "QueryID": "aws_log_group_encryption_at_rest_enabled",
    "DocumentURI": "policies/aws_log_group_encryption_at_rest_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/CloudWatch"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS CloudWatch is a monitoring and observability service designed to provide users with data and actionable insights to monitor applications, understand and respond to system-wide performance changes, and control AWS costs. AWS CloudWatch Log Group is a component of this service that represents a collection of log streams.\n\nSensitive data may be captured and stored in these CloudWatch Log Groups. Encryption of this data helps to add an extra layer of security that makes sure that only authorized services and users can decrypt and access this data.\n\nHere is a markup language comment detailing this AWS control:\n\n```markup\n\n\u003c!--\n## AWS CloudWatch Log Group Encryption \n\nTo ensure the protection of sensitive data, it is recommended that encryption is enabled for your Amazon CloudWatch Log Groups. Enabling encryption adds a secure layer to your data that ensures only authorized users or services can access and decrypt it. \n\nThis can be done by following these steps:\n1. Open the CloudWatch console.\n2. In the navigation pane, choose `Log groups`.\n3. Choose the name of the log group that you want to encrypt.\n4. On the log group details page, choose `Edit`.\n5. For `Log group settings`, select `Encrypt log group`.\n6. In `KMS key`, choose the key that you want to use.\n7. Choose `Save Changes`.\n--\u003e\n\n```\n\nNote: AWS Key Management Service (KMS) is utilized for the encryption and decryption of data. AWS KMS uses customer master keys (CMKs) to create, control, and secure your keys. \n\nIt is a good practice to review and manage these encryption keys regularly to safeguard them from any unauthorized access. If a key is lost or compromised, data encrypted under this key will also be compromised."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS control of ensuring encryption for your Amazon CloudWatch Log Group can lead to several cost implications. Here's an explanation of the costs:\n\n1. **Security Breach Costs**: Without encryption, sensitive data in the log groups may get exposed leading to possible security breaches. Apart from the immediate financial loss, a security breach also tarnishes your company's reputation, causing loss of customers and revenue in the long term.\n\n2. **Regulatory Fines**: If your company is operating under regulations like GDPR, HIPAA, or any other data protection laws, not encrypting sensitive data may lead to hefty non-compliance fines. The cost of these fines can be substantial, given these regulations carry strict penalties for violations.\n\n3. **Incident Response Costs**: If a data breach occurs, your company would need to initiate incident response which involves investigation, potential public disclosures, customer notifications, forensic analysis, remediation, and other such activities. These operational costs associated with a security incident could be quite significant.\n\n4. **Loss of Competitive Advantage**: Sensitive data could include proprietary information that offers a competitive advantage. If this information is compromised, it could lead to significant strategic and financial loss.\n\n5. **Potential Legal Costs**: In case of a breach due to non-compliance, the affected parties may sue the company leading to additional legal costs. These could range from settlements, litigation, to court orders.\n\nTo avoid these costs, it's critical to ensure compliance by enabling encryption for your Amazon CloudWatch Log Group. This not only secures your data but also build trust among your customers regarding data Protection."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, consider a large financial institution that uses Amazon CloudWatch to monitor its AWS resources and applications. This institution collects and stores large volumes of sensitive data such as customer information, financial transactions, internal communications, etc., as part of the log data in Amazon CloudWatch. \n\nIf this log data is not encrypted, it can potentially expose sensitive information to unauthorized personnel or malicious entities. To prevent such security breaches, the financial institution can enable encryption for the Amazon CloudWatch Log Group. \n\nBy using Amazon's Key Management Service (KMS), the institution can implement key management and cryptographic protection for sensitive log data. This will ensure the security of the data at rest, enhancing the overall data privacy and compliance posture of the financial institution. \n\nThis real-world example emphasizes the importance of encrypting log data within Amazon CloudWatch to protect sensitive data from potential security risks. \n\n```\nExample:\n-   LogGroup: \n        Type: \"AWS::Logs::LogGroup\"\n        Properties: \n            LogGroupName: \"MyEncryptedLogGroup\"\n            KmsKeyId: \"arn:aws:kms:us-east-1:123456789123:key/abcd1234-a123-456a-a12b-a123b4cd56ef\"\n ```\nThe above CloudFormation code snippet helps in creating a CloudWatch log group with the necessary KMS key to ensure that your data is encrypted."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_s3_bucket_restrict_public_read_access",
    "Title": "S3 buckets should prohibit public read access",
    "Description": "Manage access to resources in the AWS Cloud by only allowing authorized users, processes, and devices access to Amazon Simple Storage Service (Amazon S3) buckets.",
    "QueryID": "aws_s3_bucket_restrict_public_read_access",
    "DocumentURI": "policies/aws_s3_bucket_restrict_public_read_access.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/S3"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS control is used to manage the access to resources in the AWS cloud. This control ensures that only authorized users, processes, and devices can access Amazon Simple Storage Service (Amazon S3) buckets. \n\nHere is the markup format of the AWS control:\n\n```markdown\n# AWS Control\n\nAWS Control allows you to manage access to resources in the AWS cloud in order to maintain security and integrity. It's a feature that ensures that only [authorized users](link-to-more-info), processes, and devices can get access to Amazon Simple Storage Service (Amazon S3) buckets.\n\n## Features\n\n* **Access control** - AWS Control permits or denies access based onauthorized individuals, processes, or devices.\n* **Resource management** - It manages resources in AWS Cloud, particularly Amazon S3 buckets.\n* **Security** - Provides advanced security features to help protect your data.\n\n## How it Works\n\nWhen a user or a device attempts to access a resource in the AWS Cloud, AWS Control verifies the identity, confirms the necessary permissions, and only then does it allow access to the S3 buckets.\n\nThis way, AWS Control effectively restricts the access to resources in AWS Cloud, helping to maintain security and integrity.\n```\n\nRemember to replace `link-to-more-info` with the actual link where users can find more information about how to authorize users. Also, modify the details in the template to fit the AWS control system you're explaining."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this control can result in several detrimental impacts and costs:\n\n1. **Security breaches**: This can occur through unauthorized users (both internal and external) gaining access to confidential or sensitive data stored in the S3 buckets. Depending on the severity of the data breach, this could lead to enormous costs in terms of financial penalties and remediation efforts.\n\n2. **Regulatory fines and penalties**: Many industries have strict regulations around data security and privacy. Non-compliance can lead to heavy penalties from regulatory agencies. An example would be the General Data Protection Regulation (GDPR) in the European Union which can impose fines up to 4% of annual global turnover or €20 Million (whichever is greater) for data breaches.\n\n3. **Reputation damage**: Any data breaches can lead to reputation damage and loss of client trust, which can have significant indirect costs.\n\n4. **Miscellaneous costs**: These could include costs associated with incident response, forensic investigation, potential litigation, credit monitoring services for affected customers, and even higher cyber-insurance premiums in the aftermath of the breach.\n\nIn summary, non-compliance to this control is not just about potential direct financial cost associated with rectification but also about indirect costs like reputation damage and regulatory fines. In a nutshell, the costs are not just catastrophic but systematic.\n\nTo comply, businesses should implement strong authentication and authorization practices like multi-factor authentication (MFA), least privilege access, secure password policies, and access logging/monitoring. Additionally, they should regularly engage in security audits, vulnerability assessments, and penetration testing."
      ],
      "x-kaytu-usefulness-example": [
        "E-commerce websites often store sensitive user data such as credit card details, addresses, and purchase history on Amazon S3 buckets. Using AWS Control to manage access to these resources, you can ensure that only authorized personnel or systems have access to this data. This not only protects the integrity and confidentiality of the data but also ensures compliance with data protection regulations. For instance, an order management system might need to access a customer's shipping address in an S3 bucket, but that system should not have access to the customer's credit card details stored in the same bucket. By carefully managing access, you can ensure that each user, process, or device only has access to the resources it needs for its role."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ebs_snapshot_not_publicly_restorable",
    "Title": "EBS snapshots should not be publicly restorable",
    "Description": "Manage access to the AWS Cloud by ensuring EBS snapshots are not publicly restorable.",
    "QueryID": "aws_ebs_snapshot_not_publicly_restorable",
    "DocumentURI": "policies/aws_ebs_snapshot_not_publicly_restorable.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/EBS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Manage Access, EBS Snapshots\n\nAmazon Web Services (AWS) controls provide comprehensive guidance on managing various aspects of the AWS cloud infrastructure. One such control pertains to access management, where you need to ensure that Elastic Block Store (EBS) snapshots are not publicly restorable. This is to protect your data from unauthorized access or potential data breaches.\n\n## Why Ensure EBS Snapshots Are Not Publicly Restorable?\n\nEBS snapshots store a point-in-time copy of your data which you can use to enable disaster recovery, improve backup compliance, facilitate data migration, and increase operational efficiencies across your AWS cloud infrastructure.\n\nIf these EBS snapshots are publicly restorable, it means that any AWS user, regardless of whether they have been granted explicit permissions or not, could restore and gain access to the data contained in the snapshot. As such, making EBS snapshots publicly restorable poses a significant security threat.\n\n## Control Mechanism\n\nTo secure your data, you must ensure that the `Public` attribute is set to `false` for all your EBS snapshots.\n\n```bash\naws ec2 describe-snapshots --query 'Snapshots[*].{ID:SnapshotId, PubliclyAccessible:Public}'\n```\nThe above AWS Command Line Interface (CLI) command lists all EBS snapshots and their `Public` attribute status. If the `PubliclyAccessible` value is `true` for a snapshot, it means the snapshot is publicly restorable and should be adjusted.\n\n```bash\naws ec2 modify-snapshot-attribute --snapshot-id [snapshot-id] --attribute createVolumePermission --operation-type remove --user-ids all\n```\nThe above command removes public restore permissions from the specified EBS snapshot.\n\nBy regularly auditing your EBS snapshot permissions and ensuring they are not publicly restorable, you can better protect your data on the AWS cloud.\n\n## Conclusion \n\nManaging access to EBS snapshots is an important step in securing your data on AWS. By ensuring snapshots are not publicly restorable, you can prevent unauthorized access to your sensitive information and maintain a more secure cloud environment."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with the control of managing access to the AWS Cloud by making sure EBS snapshots are not publicly restorable could lead to several costly consequences:\n\n1. **Security Risk**: One of the significant costs of non-compliance is the potential for increased security risks. If EBS snapshots are publicly restorable, unauthorized users or malicious entities could restore these snapshots, gaining access to potentially sensitive data.\n\n2. **Data breaches**: Publicly available EBS snapshots could lead to data breaches. The severity of a data breach can vary from mild to disastrous, depending on the type of data exposed. For instances, the exposure of sensitive information such as customer data, financial details etc. could have severe consequences.\n\n3. **Non-compliance Penalties**: Depending on the industry, various regulations such as GDPR, HIPAA, or the CCPA could penalize non-compliance with stringent data privacy rules. These penalties can be severe, ranging from hefty fines to business licenses being revoked.\n\n4. **Loss of trust**: If data is compromised due to non-compliant procedures, it can lead to a significant loss of customer trust and tarnish the company's reputation, resulting in loss of business.\n\n5. **Financial Costs**: The ensuing fallout from a data breach can be financially crippling. In addition to the fines and penalties, companies may also have to bear the cost of rectifying the breach, reinforcing their security measures, and potentially compensating affected parties.\n\n6. **Litigation costs**: Breach of data can lead to lawsuits from affected customers or businesses. The cost of these legal proceedings, potential settlements and associated damages can be significant. \n\nTherefore, it is critical that access to EBS snapshots is adequately managed and they are not made publicly restorable to avoid these potential costs."
      ],
      "x-kaytu-usefulness-example": [
        "For example, Company X uses Amazon Web Services (AWS) for their cloud infrastructure. They heavily rely on the Elastic Block Storage (EBS) service for persistent storage of their application data. To ensure business continuity and data protection, they regularly snapshot their EBS volumes, which can then be restored to create new EBS volumes.\n\nHowever, Company X is very concerned about its sensitive data. If the EBS snapshot is publicly restorable, it means any AWS user around the globe can restore and access the stored data, leading to potential data leaks, cybersecurity attacks or non-compliance with data regulations like GDPR or HIPAA.\n\nHence, they utilize AWS's control to disable public restore of EBS snapshots ensuring only authorized personnel or user roles within Company X can restore and access these snapshots. This way, Company X can keep utilizing the user-friendly, flexible, and scalable cloud services from AWS while still maintaining tight control over their sensitive data intended privacy."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_cloudtrail_multi_region_trail_enabled",
    "Title": "At least one multi-region AWS CloudTrail should be present in an account",
    "Description": "AWS CloudTrail records AWS Management Console actions and API calls. You can identify which users and accounts called AWS, the source IP address from where the calls were made, and when the calls occurred. CloudTrail will deliver log files from all AWS Regions to your S3 bucket if MULTI_REGION_CLOUD_TRAIL_ENABLED is enabled.",
    "QueryID": "aws_cloudtrail_multi_region_trail_enabled",
    "DocumentURI": "policies/aws_cloudtrail_multi_region_trail_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/CloudTrail"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS CloudTrail is a service provided by Amazon Web Services (AWS) that helps you monitor activity in your AWS environment. It logs all actions and API calls made within your AWS console, providing a complete audit trail of all activities.\n\n### Overview\n\nCloudTrail covers several useful information:\n\n- **User Identifiers**: You can track which users/ accounts made the calls to AWS services.\n- **Source IP Address**: Identify the source IP address from where the calls were made, helping track the location of the users.\n- **Timestamps**: CloudTrail logs also provide the timestamp of when the calls were made which helps in maintaining a chronological record.\n\n### Multi-Region CloudTrail Enabled\n\nIf MULTI_REGION_CLOUD_TRAIL_ENABLED is enabled, CloudTrail will collect and deliver log files from all AWS regions (not just the region where the event occurred) to a specified S3 bucket. This is particularly useful if you have resources spread out across multiple regions.\n\n### Use in Markup \n\nHere is how you might represent this in markdown:\n\n```markdown\n# AWS CloudTrail\nAWS CloudTrail is a powerful service that records AWS Management Console actions \nand API calls. It makes it possible to identify:\n- Which users and accounts called AWS\n- The source IP address from where the calls were made\n- When the calls occurred\n\n## Multi-Region CloudTrail\nBy enabling `MULTI_REGION_CLOUD_TRAIL_ENABLED`, CloudTrail will collect and \ndeliver log files from **all** AWS Regions to a designated S3 bucket. \n\nThis is especially useful for environments with resources across multiple regions.\n```\n\nThe backticks around `MULTI_REGION_CLOUD_TRAIL_ENABLED` are used to denote code or a command, while the double asterisks around **all** are used to emphasize the word \"all\"."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the mentioned AWS Control, specifically `MULTI_REGION_CLOUD_TRAIL_ENABLED`, can result in several significant costs both monetarily and operationally. Broken down into points:\n\n- **Increased Risk of Data Breach**: If CloudTrail is not enabled in all regions, any activity happening in those regions will not be tracked. This means malicious or harmful activity can occur undetected, potentially leading to data breaches and loss of sensitive data, resulting in serious financial damages due to penalties, loss of customer trust, and reputational damage.\n\n- **Failed Audit and Compliance Issues**: For organizations that need to adhere to specific regulatory compliance such as GDPR, HIPAA, PCI DSS, etc., non-compliance with this control can result in failed audits. This can lead to hefty fines and penalties. \n\n- **Lack of Visibility and Control**: CloudTrail provides critical insight into user behavior and resource usage. Without a global view, you might have an incomplete understanding of your AWS environment. This hampers decision-making and planning processes, potentially leading to increased operational costs and inefficiencies.\n\n- **Difficulty in Troubleshooting and Incident Response**: During an incident, CloudTrail logs can provide significant insight into the events leading up to the issue. Not having logs from all regions can make incident response and troubleshooting a challenging task, leading to increased downtime and ultimately financial losses.\n\n- **Mismanagement of Resources**: The inability to track all actions across your infrastructure could lead to inefficient resource allocation and utilization, resulting in higher infrastructure costs. \n\nHence, enabling `MULTI_REGION_CLOUD_TRAIL_ENABLED` is crucial to mitigating these risks and costs in your AWS environment."
      ],
      "x-kaytu-usefulness-example": [
        "As a head of IT department, I noticed unusual activity in our AWS accounts late at night. We have a geo-distributed team, but nobody should be working at this time. \n\nMy immediate assumption was that one of our accounts might have been compromised. To investigate this, I leveraged AWS CloudTrail which had been recording all the AWS Management Console actions and API calls.\n\nAfter enabling MULTI_REGION_CLOUD_TRAIL_ENABLED, I was able to get log files from all AWS Regions delivered to my S3 bucket. Going through these logs, I identified the user and the AWS account from where these calls were made. I also determined the source IP address and the exact time when these calls occurred. \n\nThis comprehensive trail of information allowed me to establish that one of our AWS accounts had indeed been breached, and that the perpetrator was carrying out actions from an unfamiliar IP address.\n\nThe insights gained from AWS CloudTrail ultimately allowed us to take immediate action: we secured the compromised account, initiated a thorough security audit, and bolstered our defense mechanisms to prevent such incidents in the future. This example clearly shows the usefulness of AWS CloudTrail in monitoring and securing AWS accounts."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_emr_cluster_master_nodes_no_public_ip",
    "Title": "EMR cluster master nodes should not have public IP addresses",
    "Description": "Manage access to the AWS Cloud by ensuring Amazon EMR cluster master nodes cannot be publicly accessed.",
    "QueryID": "aws_emr_cluster_master_nodes_no_public_ip",
    "DocumentURI": "policies/aws_emr_cluster_master_nodes_no_public_ip.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/EMR"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "This AWS control involves adopting security measures to prevent public access to the master nodes of Amazon EMR (Elastic MapReduce) clusters, which manage the data processing tasks of big data analytics.\n\nThe control suggests that we restrict direct internet access to the EMR master node. If we allow public access, it can open up potential risks and vulnerabilities, leading to unauthorized data access and potential data breaches. \n\nIn Amazon EMR, a cluster, also called a job flow, runs on Amazon EC2 instances. The 'master node' manages the distribution of data and tasks to 'core and task nodes'. So, the security of the master node becomes crucial. \n\nAWS provides options to modify the security groups attached to the master node, which can help restrict inbound and outbound traffic by allowing necessary and secure connections.\n\n```markup\n### Managing Access to EMR Clusters in AWS ###\n\n1. Open the Amazon EMR Console.\n2. Select your EMR cluster.\n3. Under the **Security and access** section, select **Manage security groups**.\n4. You will see separate security groups for Master, Core \u0026 Task nodes.\n5. To edit the security group rules for the Master node, select the Master security group ID.\n6. Choose **Inbound rules**. \n   - You can specify which IP addresses can access the master node.\n   - You can modify the allowed port ranges.\n7. Similarly, manage **Outbound rules** too according to your requirements.\n8. Ensure to **Save** your changes.\n\n\u003e Note: Be cautious while editing security group rules, incorrect settings may disturb the functioning of your EMR cluster.\n```\n\nAWS also recommends using AWS PrivateLink to access EMR clusters through private endpoints in your VPC, thus further enhancing your security."
      ],
      "x-kaytu-noncompliance-cost": [
        "If a company fails to comply with the AWS control of ensuring that the Amazon EMR cluster master nodes cannot be publicly accessed, multiple costs could surface, they primarily include:\n\n1. **Security Breach:** This is a critical vulnerability that could lead to unauthorized access to sensitive data. When master nodes are publicly accessible, they may attract bad actors who can exploit these nodes to access and/or modify data. Consequently, businesses may face data breaches, resulting in financial and reputational costs.\n\n2. **Non-compliance Penalties:** There could be significant legal and financial implications for not adhering to local and global regulation standards (such as GDPR or HIPAA). Non-compliance could result in large fines and penalties.\n\n3. **Downtime:** Unauthorized access or an attack on public nodes can overwhelm your infrastructure, possibly leading to loss of service. This downtime can cause significant financial damage and heavily impact user experience and trust.\n\n4. **Potential for Data Loss:** Publicly accessible nodes can lead to unintentional data deletion or manipulation by unauthorized users.\n\n5. **Increased Operational Costs:** Extra time and resources may be needed to recover from an event, clean up the damage, restore data, and fortify security measures.\n\n6. **Damage to Brand Reputation:** Any perceived or actual instance of data insecurity could harm the brand's reputation. This might deter existing or potential clients, causing a decline in clientele and thus, revenues."
      ],
      "x-kaytu-usefulness-example": [
        "Managing access to the AWS Cloud is a crucial aspect of cloud security and data protection. Amazon EMR (Elastic MapReduce) is a service that uses big data technologies, such as Apache Hadoop and Spark, to process large amounts of data. The master node in an EMR cluster is the central controlling node that also hosts the Hadoop NameNode and JobTracker, which are critical to data processing tasks. If the master node is publicly accessible, it present potential cybersecurity threats like hacking, denial-of-service (DoS) attacks, etc. \n\nExample Usefulness:\n\nConsider a large multinational enterprise that utilizes Amazon EMR for handling, analyzing, and processing business-critical data. The enterprise constantly collects customer data, which includes sensitive information. Ensuring that the Amazon EMR cluster's master nodes are not publicly accessible protects this information from potential breaches. \n\nThis AWS control will ensure that the enterprise's sensitive data remains safe and private, thereby preventing any potential data leaks that could result in financial losses, reputational damage, or even regulatory penalties for non-compliance with data protection laws. \n\n```markdown\nAs an example:\n\n- A healthcare organization uses AWS to analyze patient data for researching diseases and potential treatments. Public access to their Amazon EMR cluster's master nodes could jeopardize patient privacy and lead to breaches of healthcare laws, like HIPAA in the U.S. By ensuring the EMR Master node is not publicly accessible, AWS provides essential control over access, enhancing data security.\n```\nThis example demonstrates how using this AWS Control for managing access to EMR master nodes heightens security and aids in compliance with data protection and privacy regulations."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_cloudtrail_trail_integrated_with_logs",
    "Title": "CloudTrail trails should be integrated with CloudWatch logs",
    "Description": "Use Amazon CloudWatch to centrally collect and manage log event activity. Inclusion of AWS CloudTrail data provides details of API call activity within your AWS account.",
    "QueryID": "aws_cloudtrail_trail_integrated_with_logs",
    "DocumentURI": "policies/aws_cloudtrail_trail_integrated_with_logs.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/CloudTrail"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "`Amazon CloudWatch` is a monitoring service provided by Amazon Web Services (AWS) that provides data and actionable insights to monitor your applications, system-wide operational health, and resource utilization. This service provides you a central location to store, access, and correlate your log files. \n\nFor a tighter security control, you can integrate `Amazon CloudWatch` with `AWS CloudTrail`. `CloudTrail` captures information about every API call made in your AWS environment, including the identity of the caller, the time of the API call, the source IP address of the caller, the request parameters, and the response elements returned by the AWS service. By integrating CloudTrail with CloudWatch, you can create CloudWatch alarms that notify you when specific API activities occur.\n\nHere is a simple example of a CloudFormation markup that sets up a CloudWatch Log Group, a CloudTrail and enables it to send events to the CloudWatch Log Group:\n\n```yaml\nResources:\n  MyCloudTrail:\n    Type: 'AWS::CloudTrail::Trail'\n    Properties: \n      CloudWatchLogsLogGroupArn: !GetAtt MyLogGroup.Arn\n      CloudWatchLogsRoleArn: !GetAtt MyCloudWatchRole.Arn\n      IsLogging: true\n  MyLogGroup:\n    Type : 'AWS::Logs::LogGroup'\n    Properties:\n      LogGroupName: MyLogGroup\n  MyCloudWatchRole:\n    Type: 'AWS::IAM::Role'\n    Properties:\n      AssumeRolePolicyDocument: \n        Version: '2012-10-17'\n        Statement:\n          Effect: Allow\n          Principal:\n            Service:\n              - 'cloudtrail.amazonaws.com'\n          Action: 'sts:AssumeRole'\n      Policies:\n        - PolicyName: CloudWatchPutLogEventsPolicy\n          PolicyDocument:\n            Version: '2012-10-17'\n            Statement:\n              Effect: Allow\n              Action: 'logs:PutLogEvents'\n              Resource: !GetAtt MyLogGroup.Arn\n```\n\nThis CloudFormation template creates a new Trail, a new Log Group and an IAM Role that CloudTrail can assume to put log events to the Log Group. This demonstrates how you can leverage AWS services to better monitor and manage log event activity.\n  \nPlease note that you need proper permissions to use the resources stated in the template. The above configuration can be adjusted as per your monitoring requirement."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS Control could result in several costs:\n\n1. **Security Risks**: Without properly monitoring and managing log events, your AWS infrastructure could be vulnerable to security threats, leading to potential breaches. This could cost millions in damages, data losses, and fines for non-compliance with regulations such as GDPR, HIPAA etc.\n\n2. **Operational Inefficiencies**: A failure to leverage the automated log management through Amazon CloudWatch could lead to increased man-hours and slow response times for resolving operational issues. This, in turn, increases operational costs and risks system downtime.\n\n3. **Audit Complications**: If you are not accurately recording and storing API call activities with AWS CloudTrail, this could lead to difficulties when it comes to audits. Ensuring you have a accurate record of activity is crucial for compliance with many regulations.\n\n4. **Forensic Costs**: In the case of an unexpected issue or incident, without proper log events, the costs associated with forensic analyses can drastically increase as investigation becomes more difficult.\n\n5. **Loss of Client Trust**: Following potential security breaches or system downtime, the trust from customers or clients could be severely affected. This could in turn cause loss of clients, harming the revenue and reputation of the company. \n\nOverall, neglecting to implement and maintain strong controls over log event activity and failing to utilize tools like Amazon CloudWatch and AWS CloudTrail can lead to increased security risks, higher operational costs, and severe reputation damage."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, a software company is hosting their multi-tier application on AWS and needs to ensure constant application/infrastructure performance, alongside real-time monitoring and troubleshooting any issues. \n\nThey can use Amazon CloudWatch to centrally collect and manage log event activities. CloudWatch effectively monitors all AWS resources in real-time, collects and tracks metrics, collects and monitors log files from resources such as EC2 instances, and responds to system-wide performance changes.\n\nBy additionally incorporating AWS CloudTrail data, the company can gain detailed information on API calls made within the AWS account. This can include the identities of callers, time of API calls, source IP addresses, request parameters, and also response elements.\n\nSuch data becomes indispensable in identifying patterns and spotting anomalies in API usage. Furthermore, it's useful in security analysis, resource change tracking, compliance auditing, and troubleshooting operational issues, ensuring the software company consistently meets its performance and security standards while delivering excellent service to its customers. \n\nThe log data can be easily analyzed, archived, and searched using CloudWatch and CloudTrail features to get a detailed view of the activities going on within the AWS environment. \n\nUtilizing these AWS controls can help the software company maintain its application's uptime and quickly respond to any disruptions to its service."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_s3_bucket_restrict_public_write_access",
    "Title": "S3 buckets should prohibit public write access",
    "Description": "Manage access to resources in the AWS Cloud by only allowing authorized users, processes, and devices access to Amazon Simple Storage Service (Amazon S3) buckets.",
    "QueryID": "aws_s3_bucket_restrict_public_write_access",
    "DocumentURI": "policies/aws_s3_bucket_restrict_public_write_access.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/S3"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Managing Access to Amazon S3 Buckets \n\nAWS provides secure mechanisms to control and manage who can access your Amazon Simple Storage Service (Amazon S3) buckets in the AWS Cloud. This ensures that AWS resources are accessible only by authorized users, processes, and devices.\n\n**What is Amazon S3?**\nAmazon S3 is an object storage service offered by AWS. It is designed for online backup and archiving of data and applications on AWS. It allows users to store and retrieve unlimited amounts of data at any time, from anywhere.\n\n**Why is access control important?** \nAccess control security measures are crucial for preserving the integrity, confidentiality, and availability of data. They protect your S3 buckets against unauthorized access.\n\n**How does AWS manage such access?** \nAmazon S3 supports access control mechanisms which provide several ways for users to manage access to their buckets and objects:\n\n- **Identity and Access Management (IAM)**: IAM roles can be used to manage access to your S3 resources. You can create users in IAM and manage their access to AWS services and resources, including Amazon S3.\n\n- **Access Control Lists (ACLs)**: ACLs provide a way to manage access to buckets and objects. You can use ACLs to grant permissions (either READ, WRITE or FULL_CONTROL) to AWS accounts or to the public.\n\n- **Bucket Policies**: Bucket Policies are attached to S3 buckets and provide centralized access control to enforce specific permissions or restrictions.\n\nIn addition to these, encryption mechanisms can be used to protect data in transit and at rest. \n\nGenerally, the access control approach in AWS is based on the principle of least privilege, ensuring that a particular user, process, or device is granted only the minimum permissions necessary to complete its tasks. This significantly reduces the potential for malicious or inadvertent alterations. \n\nBy appropriately managing access to your Amazon S3 buckets, AWS ensures that your valuable data is protected while still being easily accessible to authorized entities when needed."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS control of managing access to resources by only allowing authorized users, processes, and devices access to Amazon Simple Storage Service (S3) buckets can lead to multiple adverse outcomes, including:\n\n**1. Financial loss:** Unauthorized access might end up in data breaches, which can result in hefty fines for violating data protection laws. Furthermore, there could be unnecessary data transfer or operation costs if malicious users manipulate the data in your S3 buckets.\n\n**2. Data Breaches:** Unrestricted access to the S3 bucket can lead to data theft, alteration, or deletion. In many industries, data breaches can also result in severe penalties, not to mention the significant loss of business reputation.\n\n**3. Loss of Business Secrets:** If unauthorised users gain access to S3 bucket, a company could lose valuable proprietary information, leading to competitive disadvantages.\n\n**4. Legal and Compliance Issues:** Depending on the nature of the data stored and jurisdiction, failing to restrict access can lead to non-compliance with various laws and regulations. This could result in legal action and penalties.\n\n**5. Trustworthiness:** If a business fails to adequately protect data from unauthorized access, it may lose the trust of its customers and partners. This could lead to lost business opportunities and revenue.\n\nTo prevent these outcomes, organizations should ensure they adhere strictly to the AWS control of managing access to resources. They should only allow authorised users, processes, and devices access to S3 buckets, using appropriate AWS permissions and policies."
      ],
      "x-kaytu-usefulness-example": [
        "This AWS control is useful in a scenario where a company is using Amazon S3 to store customer data. The company wants to ensure that only certain authorized employees have access to this sensitive information.\n\nFor example, a healthcare organization might be using an Amazon S3 bucket to store patient medical records. These records contain extremely sensitive information and need to be carefully guarded to maintain patient privacy and to comply with regulations like the Health Insurance Portability and Accountability Act (HIPAA).\n\nIn this scenario, the organization could use AWS controls to manage access to the S3 bucket and restrict it to only certain users. They might allow access only for the team of database administrators who are responsible for managing the records, while denying access to other employees. They could also set up processes that require multi-factor authentication for further security.\n\nIn addition, the organization could use the control to limit access based on devices. For instance, they might restrict access to the bucket so that it can only be accessed from devices within their secure hospital network, and not from outside devices that could potentially be unsecured.\n\nThis use of AWS controls helps to protect the sensitive patient data, assuring patients that their information is secure, and helping the organization to maintain regulatory compliance."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_instance_prohibit_public_access",
    "Title": "RDS DB instances should prohibit public access",
    "Description": "Manage access to resources in the AWS Cloud by ensuring that Amazon Relational Database Service (Amazon RDS) instances are not public.",
    "QueryID": "aws_rds_db_instance_prohibit_public_access",
    "DocumentURI": "policies/aws_rds_db_instance_prohibit_public_access.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/RDS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "Sure, here's how it can be explained as a AWS Control in a markup format:\n\n# AWS Control: Ensure Amazon RDS Instances are Not Public\n\n## Overview \n\nThis AWS control is designed to manage access to AWS Cloud resources, particularly focusing on Amazon Relational Database Service (Amazon RDS) instances. The recommendation is to keep these RDS instances non-public to instigate improved data security.\n\n## Objective\n\nTo ensure that all Amazon RDS instances that contain sensitive information are not publicly accessible directly from the internet and to only allow access to specific, authorized IP addresses.\n\n## Guidelines\n\nOn a routine basis, go through the following steps:\n\n1. Open the AWS Management console.\n2. Connect to RDS.\n3. In the navigation pane, select 'Instances'.\n4. Scan through each RDS instance and check whether 'Public Accessibility’ is set to ‘Yes’.\n5. If it is set to ‘Yes’, modify the instances to disable public access by switching the 'Public Accessibility' to 'No' state which restricts public access to the RDS instances.\n\nBy adhering to these guidelines, AWS clients can minimize their risk of malicious access or data leakage. \n\n## Importance\n\nNot siloing RDS instances in the public domain is a security vulnerability that can lead to breaches, loss of data, or unauthorized control over the resources. Ensuring that RDS instances are not public limits their exposure to potential threats. \n\nPlease note this safety measure should go hand in hand with other important best practices, such as encryption of data at rest and in transit, along with strong access control policies.\n\n## Conclusion\n\nDoing regular audits and disabling public access to Amazon RDS instances is an essential part of the AWS usage. This will enhance your data security, and maintain the integrity and confidentiality of your sensitive data over the cloud."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS control can potentially lead to several severe consequences:\n\n1. **Data Breaches and Financial Losses**: RDS instances that are public can be accessed from any IP address. It means your sensitive data is exposed to any unknown entity on the internet which could lead to data breaches, thereby resulting in legal penalties and huge financial losses.\n\n2. **Loss of Privacy**: Making your RDS instances public can significantly reduce the privacy of your data. Hacker or malicious entities can exploit the data for their own benefits.\n\n3. **Unwanted Network traffic**: A public RDS could also lead to unwanted network traffic, thus degrading the performance of the database and potentially leading to service outages.\n\n4. **Infiltration Opportunities for Hackers**: Unsecured RDS instances can increase the surface of attack for hackers and other malicious entities to inject malware and viruses into your system.\n\n5. **Non-compliance to regulatory standards**: If your application is under regulations such as GDPR, HIPAA or PCI-DSS, you could face serious non-compliance issues resulting in hefty fines and penalties.\n\nTherefore, to mitigate these risks and costs, you should ensure that your Amazon RDS instances are not publicly accessible unless absolutely necessary and approved by the administration. Ensure to implement appropriate security measures."
      ],
      "x-kaytu-usefulness-example": [
        "Amazon RDS instances being public means anyone on the internet can access your database, which obviously poses serious security risks such as unauthorized data access, data breaches, and potential loss of sensitive information.\n\nExample:\nConsider a scenario where a business is running a web application that uses an Amazon RDS instance as its database. All the customer information and business data is stored in this database. If the RDS instance is public, the data can be accessed, manipulated, or stolen by hackers.\n\nBy managing access to resources and ensuring that the Amazon RDS instances are not public, only authorized entities such as specific web servers or authorized personnel can access this data. This dramatically reduces the surface area for potential attacks and significantly increases the security of the application and the safety of the data. \n\nIn essence, managing and restricting access to Amazon RDS instances is a crucial AWS control for maintaining and enhancing security for applications and data stored on the AWS Cloud."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ebs_attached_volume_encryption_enabled",
    "Title": "Attached EBS volumes should have encryption enabled",
    "Description": "Because sensitive data can exist and to help protect data at rest, ensure encryption is enabled for your Amazon Elastic Block Store (Amazon EBS) volumes.",
    "QueryID": "aws_ebs_attached_volume_encryption_enabled",
    "DocumentURI": "policies/aws_ebs_attached_volume_encryption_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/EBS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS EBS volumes are a key resource for storing data in AWS Cloud. These volumes can contain sensitive data such as Personally Identifiable Information (PII), Protected Health Information (PHI), credit card data, intellectual property, etc. Therefore, it is critical for organizations to protect this data from unauthorized access, tampering, or accidental deletion.\n\n**Control: Enable encryption for Amazon EBS volumes**\n\nAmazon provides the ability to encrypt EBS volumes to protect the data at rest. Here's a markup explanation of how to ensure that encryption is enabled:\n\n```markdown\n#### Check if encryption is enabled for Amazon EBS volumes:\n\n1. Open the AWS Management Console.\n2. Navigate to the Amazon EC2 dashboard.\n3. In the navigation pane, click 'Volumes' under 'Elastic Block Store'.\n4. Select the EBS volume to check.\n5. In the 'Description' tab, check the value of the 'KMS key' field:\n   - If it's set to 'aws/ebs', the volume is encrypted using the default AWS managed CMK.\n   - If it's set to a specific KMS key, the volume is encrypted using a customer-managed CMK.\n   - If it's set to '-', the volume is not encrypted.\n\n#### To enable encryption for existing unencrypted EBS volumes:\n\n- Unfortunately, AWS doesn't provide an option to encrypt an existing unencrypted EBS volume directly. However, you can create a snapshot of the unencrypted volume, create a copy of that snapshot while enabling encryption, and then create a new encrypted volume from the encrypted snapshot.\n\n#### To enable encryption for new EBS volumes:\n\n1. Navigate to the 'Volumes' page in the Amazon EC2 dashboard.\n2. Click 'Create Volume'.\n3. In the 'Create Volume' dialog box, specify the necessary volume attributes.\n4. In the 'Encryption' section, select 'Enabled'. Select a master key from the 'Master Key' drop-down list or leave it at the default AWS managed key.\n5. Click 'Create Volume'.\n\nFor ensuring compliance, you may want to set up CloudWatch alarms or AWS Config rules to monitor and check the encryption status of your EBS volumes regularly.\n```\n\nEnabling EBS encryption not only enhances data security but also meets regulatory compliance requirements."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance with this AWS control can be difficult to quantify directly. However, the costs incurred can be grouped into the following categories:\n\n1. **Financial Loss**: This can occur due to penalties from regulatory bodies if compliance requires data encryption or due to loss of customer trust and business in the event of a data breach.\n\n2. **Legal Consequences**: Non-compliance can lead to lawsuits and legal claims from customers or partners whose data was compromised due to inadequate security measures. \n\n3. **Reputation Damage**: If the data breach becomes public, the organization’s reputation may suffer significantly, leading to loss of customers and potential future business opportunities. \n\n4. **Remediation Costs**: If a data breach occurs, significant resources may be required to fix the vulnerabilities, recover from the breach, and implement improved security measures.\n\nIn summary, ensuring encryption for Amazon EBS volumes is a proactive measure that can prevent significant costs and damage in the event of a data breach. Therefore, it is highly beneficial to comply with this AWS control."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, an e-commerce company is storing their customers’ transaction details on Amazon EBS volumes. This includes sensitive information such as credit card details. If these volumes are left unencrypted, it exposes a security vulnerability risk where unauthorized individuals might gain access to this sensitive data.\n\nBy enabling encryption for EBS volumes, this security risk is mitigated as the stored data is encrypted. Even in the event of a security breach, the intruder would get access to encrypted data which is of no use without the encryption keys. Thus, enabling encryption for Amazon EBS volumes can help safeguard sensitive data against potential security threats.  \n\nIn this case, AWS EBS encryption turns out to be very useful in protecting sensitive customer data and maintaining data confidentiality, ensuring compliance with data protection regulations and maintaining customers trust. \n\n```markdown\nExample:\n- Company: XYZ E-commerce \n- Use Case: Protecting Customers' Transaction Details\n- AWS Service: Amazon Elastic Block Store (EBS)\n\nXYZ E-commerce stores their customer transaction details on Amazon EBS volumes. Given the sensitivity of this data, which includes credit card information, it's critical that these volumes are encrypted to protect the data. \n\nBy utilizing encryption on Amazon EBS volumes, XYZ E-commerce can ensure that its stored data is secure. This encryption reduces the possible damage of a security breach by rendering the obtained data useless without the associated encryption keys. Accordingly, Amazon EBS encryption not only plays a crucial role in maintaining customer data confidentiality but it also ensures compliance with various data protection regulations and boosts customer trust in XYZ E-commerce platform's commitment to data security.\n```\n"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_lambda_function_restrict_public_access",
    "Title": "Lambda functions should restrict public access",
    "Description": "Manage access to resources in the AWS Cloud by ensuring AWS Lambda functions cannot be publicly accessed.",
    "QueryID": "aws_lambda_function_restrict_public_access",
    "DocumentURI": "policies/aws_lambda_function_restrict_public_access.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/Lambda"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# Manage Access to AWS Lambda Functions \n\nAWS Lambda is a compute service that lets you run code without provisioning or managing servers. While this service is convenient and efficient, it's your responsibility to ensure that your AWS Lambda functions are not publicly accessible and thereby vulnerable to attackers. To secure your AWS Lambda functions, you need to manage the service's access to resources in the AWS Cloud.\n\n## Steps to Manage Access \n\n1. **Create an IAM role:** Identity and Access Management (IAM) roles are used to provide permissions to the entities (people, applications, services) that assume them. You can create a role using the AWS Management Console, AWS CLI, or AWS SDKs.\n\n2. **Attach policies to IAM role:** After creating an IAM role, you'll need to attach policies to it. These policies define what actions are allowed or denied by the role on specified resources. \n\n3. **Assign IAM role to Lambda Function:** Once the IAM role with the attached policy is created, you assign it to the AWS Lambda function. To do so, simply specify the IAM role's ARN as the function's execution role during function creation.\n\n4. **Use VPC to further secure your function:** If your function accesses resources inside a Virtual Private Cloud (VPC), you must associate it with a security group, your virtual firewall. With a VPC, you have control over your virtual networking environment, including subnet creation, routing table configurations, network gateways, and security settings.\n\n5. **Review and update your IAM policies:** Regularly check your IAM policies to make sure you're granting the least privilege- or just-enough-access for your Lambda function to perform its tasks, nothing more.\n\nRemember, these precautions are a preventative measure against any unauthorized access to your AWS Lambda functions, helping keep your sensitive data and processes within the AWS Cloud secure."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to this AWS control can be severe and have both financial and reputational implications:\n\n1. **Data Breaches**: If AWS Lambda functions are publicly accessible, it can lead to unauthorized access and potentially cause data breaches. Data breaches can lead to substantial financial penalties, the cost of remediation, and significant reputational damage.\n\n2. **Increased Vulnerability to Attacks**: Publicly accessible Lambda functions can be discovered and attacked by malicious actors. Successful attacks may lead to potentially harmful operations, such as disrupting services, injecting malicious code, stealing sensitive data, or launching attacks on other parts of the system.\n\n3. **Violation of Compliance Standards**: Most cybersecurity laws and regulations require businesses to take appropriate measures to prevent unauthorized access to data and systems. Publicly accessible Lambda functions could be seen as non-compliance to these regulations and may lead to penalties and fines.\n\n4. **Cost Implication**: Lambda functions are paid based on the execution time. If a function is publicly accessible, it may be triggered inappropriately and frequently, thereby unnecessarily increasing the cost.\n\nThus, it is vital to ensure AWS Lambda functions cannot be publicly accessed to avoid these potential costs. To secure your functions, follow AWS's best practices, such as granting least privilege access, using AWS Identity and Access Management (IAM) roles, and regularly auditing the access policies."
      ],
      "x-kaytu-usefulness-example": [
        "This AWS control ensures that only authorized individuals or systems have access to AWS Lambda functions, providing enhanced data security. \n\nFor example, consider a situation where a company has deployed various AWS Lambda functions to handle backend processing of their web application. If these Lambda functions are publicly accessible, it creates a potential security risk where malicious entities can unauthorizedly manipulate or misuse these functions, leading to severe consequences such as data breaches or operational disruptions. \n\nBy implementing AWS control to manage access to these Lambda functions, the company can restrict access to only specific IP addresses, certain users, or specific systems. This adds an additional layer of security, reduces potential risk vectors, and helps ensure the system's resilience against cyber threats. \n\nIn the context of compliance, this control can also help meet specific regulatory requirements that demand strict access control over systems and data. \n\nHence, this AWS control's usefulness lies in providing stronger data security, ensuring system integrity, and aiding in regulatory compliance."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_snapshot_prohibit_public_access",
    "Title": "RDS snapshots should prohibit public access",
    "Description": "Manage access to resources in the AWS Cloud by ensuring that Amazon Relational Database Service (Amazon RDS) instances are not public.",
    "QueryID": "aws_rds_db_snapshot_prohibit_public_access",
    "DocumentURI": "policies/aws_rds_db_snapshot_prohibit_public_access.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/RDS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Manage Access to Amazon RDS Instances \n\nAWS provides services to control and manage access to its resources. One such resource is Amazon Relational Database Service (Amazon RDS), a web service that makes it easier to set up, operate, and scale a relational database in the cloud. \n\nHowever, for the sake of security and data privacy, it's crucial to ensure that the RDS instances are not publicly accessible. You must put measures in place so that your data is not leaked or exploited by unauthorized users.\n\n## Why Should RDS Instances Not Be Public?\n\nMaking an RDS instance publicly accessible exposes your database to potential security threats. Unauthorized users could gain access to sensitive data, affecting data integrity and potentially incurring additional costs. \n\n## How to Ensure RDS Instances Are Not Public \n\nTo check whether your RDS instances are public, follow these steps:\n\n1. Open the Amazon RDS console.\n2. In the navigation pane, select **Databases**.\n3. Choose the database instances that you want to secure.\n4. In the **Details** section, examine the **Public accessibility** attribute. If the attribute is set to **Yes**, the database instance is publicly accessible. \n\nYou can modify the public accessibility of your Amazon RDS instance at any time by modifying the instance and setting the **Publicly Accessible** option to **No**.\n\nAdditionally, ensure that the security groups associated with your RDS instances are correctly configured and only allow access for trusted IPs and IP ranges. It's crucial to regularly review and update these settings in line with your organization's security policies.\n\nThis way, you can effectively manage access to resources in the AWS Cloud and ensure the safety of your data in Amazon RDS instances.\n"
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the AWS control - \"Manage access to resources in the AWS Cloud by ensuring that Amazon Relational Database Service (Amazon RDS) instances are not public\" can be significant and can include-\n\n1. **Security and Data Breaches**: If an Amazon RDS instance is public, it is exposed to the internet and can potentially be accessed by anyone, leading to possible data breaches. This could lead to severe financial losses, damaged reputation, loss of customer trust, and possible legal issues.\n\n2. **Violation of Compliance Rules**: Many industries, especially those in healthcare, finance, or government sectors, require strict adherence to compliance rules. One such common rule is the requirement to secure customer data. Non-compliance may result in fines, loss of business, and legal consequences.\n\n3. **Increased Vulnerability to Attacks**: A public RDS instance is more vulnerable to attacks such as SQL injections, brute force, or Distributed Denial of Service (DDoS). Cyberattacks may cause system downtime, leading to lost revenue and increased costs in restoring services.\n\n4. **Loss of Data Control**: Making an RDS instance public can lead to more people accessing and possibly manipulating the data, which can lead to inconsistencies or data corruption.\n\n5. **Unwanted Charges**: Unwanted access or attacks on the instance could lead to an unexpectedly high AWS bill, due to increased data transfer or the use of additional resources.\n\nIn a nutshell, non-compliance with this AWS control can lead to financial losses, reputation damage, legal implications, and increased vulnerability to cyber threats. Compliance should be maintained to ensure security, confidentiality, and integrity of data, while also reducing business risks."
      ],
      "x-kaytu-usefulness-example": [
        "For a company operating in the financial sector, managing access to resources in the AWS cloud is critical for information security and regulatory compliance. For instance, they use Amazon RDS to manage their databases that contain sensitive customer and transaction data. \n\nEnsuring that the Amazon RDS instances are not public constrains the access to these secure financial databases. This way, only authorized users are able to access, view, and manipulate the data, while risk of unauthorized access or data leak is significantly reduced. \n\nThis control is thus very valuable for the said company, as it helps maintain the strict data security and privacy standards required in the financial industry, protecting both the company and its customers. \n\n```markup\nResource:\n  myDB:\n    Type: 'AWS::RDS::DBInstance'\n    Properties:\n      PubliclyAccessible: false\n      DBInstanceIdentifier: 'myDatabase'\n      AllocatedStorage: '5'\n      DBInstanceClass: 'db.t2.small'\n      Engine: 'mysql'\n      MasterUsername: 'test'\n      MasterUserPassword: 'testpassword'\n```\n\nIn the above AWS CloudFormation template, an Amazon RDS database is being created with the `PubliclyAccessible` property set to `false`, which makes the RDS instance non-public and accessible only to authorized users."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_group_user_role_no_inline_policies",
    "Title": "IAM groups, users, and roles should not have any inline policies",
    "Description": "Ensure an AWS Identity and Access Management (IAM) user, IAM role or IAM group does not have an inline policy to control access to systems and assets.",
    "QueryID": "aws_iam_group_user_role_no_inline_policies",
    "DocumentURI": "policies/aws_iam_group_user_role_no_inline_policies.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/IAM"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS Identity and Access Management (IAM) helps to securely control access to AWS services and resources for your users. It allows creation and management of AWS users and groups, and use of permissions to allow and deny their access to AWS resources.\n\nAn \"inline policy\" is a policy that's embedded in a single IAM user, group, or role. It's recommended to avoid using inline policies and instead use managed policies, as they offer more flexibility and are generally considered as best practice.\n\nThe control you have asked about is to ensure that an IAM user, group, or role in your AWS ecosystem does not have an inline policy controlling access to your systems and assets.\n\nIn other words, it's best to avoid directly attaching policies to users, roles or groups as it's easier to manage permissions when they are consolidated within managed policies.\n\nFirstly, we have to check whether the IAM user, IAM role or IAM group have an inline policy:\n\n```markdown\n**To check for Inline Policies for IAM Users**\n\n1. Open the AWS Management Console, navigate to IAM service.\n2. In the navigation pane, click on \"Users\".\n3. In the User Name list, click on the name of the desired user.\n4. In the user's detail page, click on \"Inline policies\". If the button says \"There are no inline policies to show\", then the selected user doesn't have any inline policies.\n\nSame can be followed for IAM Roles and IAM Groups, in the IAM service in the AWS Management Console, navigate to Roles/Groups instead of Users to check whether they have inline policies.\n\n**To remove an Inline Policy if it exists**\n\n1. Follow the steps above to navigate to the User's, Role's or Group's detail page.\n2. Under Inline Policies, click on the policy you wish to remove.\n3. Click on \"Remove policy\" and confirm the removal.\n```\nWithout this control, users, roles or groups might have permissions that are more broad than required, posing a security risk."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to this AWS control can be seen in terms of both security risks and operational inefficiencies:\n\n1. **Security Risks:** Inline policies are policies that are embedded directly into a single entity (user, group, or role). They are not stood up as separate, standalone policies. This means if an IAM user, role, or group has an inline policy controlling access to systems and assets, AWS has less visibility and control over these policies. If an inline policy is improperly configured, it can potentially grant unauthorized access to sensitive resources, leading to security breaches or data leaks.\n\n2. **Operational Inefficiencies:** From an operational perspective, using inline policies can lead to unmanageable policy management practices. For instance, if you have multiple IAM users or groups that need the same permissions, you'll have to attach the same inline policy individually, instead of creating a single managed policy that can be attached to all of them. This not only increases the workload for administrators but also makes it challenging to keep track of which permissions are associated with which entities.\n\nCosts can manifest in several ways:\n\n- **Financial Impact:** You may experience financial loss as a result of a data breach that arises due to improperly managed inline policies. This can be direct (such as from theft of sensitive financial data) or indirect (like reputational damage leading to loss of customers).\n\n- **Regulatory Penalties:** If you are subject to data protection regulations (like GDPR or PCI DSS), non-compliance can result in hefty fines.\n\n- **Operational Costs:** The additional time and resources required to manage multiple separate inline policies can add to your operational costs.\n\nTo prevent these potential costs, it is recommended to use managed policies where possible, which are easier to administer and audit. This practice aligns with the AWS best practice of granting least privilege, or only the necessary access to perform tasks."
      ],
      "x-kaytu-usefulness-example": [
        "AWS Identity and Access Management (IAM) is a useful service for managing access to AWS resources securely. However, relying on inline policies could make managing permissions more complex and prone to errors. For instance, consider an organization that has multiple AWS users and dozens of services running. If each user and service is managed with inline policies, administrators would have to track and update individual policies every time a change is made.\n\nFor example, say a user has permissions to access an S3 bucket via an inline policy. If the organization decides to revoke this user’s access from S3, the administrator would have to find and edit the inline policy associated with this specific user. If there are hundreds of users with similar inline policies, this becomes a daunting task.\n\nOn the other hand, using managed policies or groups would be more efficient. Administrators could manage permissions at a group level rather than an individual level. When a change is required, they only need to update a single managed policy that applies to an entire group of users or roles, which can significantly reduce errors and the time spent on managing IAM.\n\nIn this context, the AWS control 'Ensure an AWS IAM user, IAM role or IAM group does not have an inline policy to control access to systems and assets' promotes using managed policies or groups for better permission management and security practices."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_elb_classic_lb_use_tls_https_listeners",
    "Title": "ELB classic load balancers should only use SSL or HTTPS listeners",
    "Description": "Ensure that your Elastic Load Balancers (ELBs) are configured with SSL or HTTPS listeners. Because sensitive data can exist, enable encryption in transit to help protect that data.",
    "QueryID": "aws_elb_classic_lb_use_tls_https_listeners",
    "DocumentURI": "policies/aws_elb_classic_lb_use_tls_https_listeners.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ELB"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Enforce SSL/HTTPS on Elastic Load Balancers (ELBs)\n\nAmazon Web Services (AWS) provides Elastic Load Balancers (ELBs) to automatically distribute incoming application traffic across multiple targets, such as EC2 instances. This ensures the availability and scalability of the applications.\n\n**Understanding the need for SSL or HTTPS on ELBs**\n\nAny application, whether it's a simple website or a complex service-based architecture, can handle sensitive data such as user credentials, personal identification information (PII), payment details etc. These data need to be protected from unauthorized access and attacks like eavesdropping, data breaches, etc. \n\nThis is where SSL (Secure Sockets Layer) or HTTPS (HTTP Secure) comes into play. These protocols secure the data in transit between the user's computer and the server by encrypting the data. It helps ensure that any data exchanged between the two is unreadable to anyone else.\n\n**Enforcing SSL or HTTPS on ELBs**\n\nTo help protect sensitive data while it’s in transit, AWS recommends that your ELBs are configured with SSL or HTTPS listeners. By setting up these listeners, it ensures that all the data being transmitted between the ELBs and the end user is secure and encrypted, reducing the risk of data being intercepted or manipulated. \n\nHere's how you can set it up:\n\n1. Open the **Amazon EC2 console**.\n\n2. On the navigation pane, under **Load Balancing**, choose **Load Balancers**.\n\n3. Select your load balancer.\n\n4. On the **Listeners** tab, choose **Add listener**.\n\n5. For **Protocol**, select HTTPS (for Web traffic).\n\n6. Fill in the other required fields like the port, default actions, certificate etc.\n\n7. Choose **Save**.\n\nBy ensuring your ELBs are configured with SSL or HTTPS listeners, you're enforcing a critical control on data security within your AWS environment. \n\nNote: Don't forget to keep your SSL certificates up-to-date as expired certificates can lead to unsecured connections. You can manage your certificates using AWS Certificate Manager."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS Control can result in several potential costs:\n\n1. **Data Breach:** Not configuring Elastic Load Balancers (ELBs) with SSL or HTTPS can leave data unprotected when it is in transit. This can lead to data breaches where sensitive information is stolen by malicious actors. The cost of a data breach can be enormous, both financially and in terms of reputational damage. \n\n2. **Regulatory Fines:** Many industries and geographic areas have strict regulations about data protection and privacy. Non-compliance can result in hefty fines and sanctions. For example, under GDPR in the EU, companies can be fined up to 4% of global annual turnover for serious infractions.\n\n3. **Loss of Customer Trust:** If customers find out that their personal data is not being adequately protected, they may lose trust in the organization and take their business elsewhere. Over time, this can lead to a significant loss of revenue.\n\n4. **Legal Expenses:** In addition to regulatory fines, companies may also face legal action from customers or third parties affected by a data breach. This can lead to additional costs in legal fees and potential settlements.\n\n5. **Remediation Costs:** After a data breach, companies need to spend time and resources to investigate the breach, fix the security issue, and possibly provide identity protection services to affected customers.\n\n6. **Business Disruption:** In some cases, a data breach can cause significant enough disruption to a company's operations to affect its bottom line.\n\nTherefore, it can be seen that ensuring that your Elastic Load Balancers (ELBs) are configured with SSL or HTTPS listers is not just best practice but is also crucial for the financial and operational health of the business."
      ],
      "x-kaytu-usefulness-example": [
        "An example instance could be a healthcare organization using AWS services to store and manage patient data. This information includes sensitive details like personal identification information, medical history, etc. Therefore, it's important to maintain a high level of security when this data is being transmitted.\n\nBy ensuring that the Elastic Load Balancers (ELBs) are configured with SSL or HTTPS listeners, the data being transmitted is encrypted, adding an additional layer of security. This means, even if the transmitted data somehow gets intercepted, it still remains protected due to the encryption.\n\nHere's a potential scenario: The organization's app which lets users access and manage their medical data, uses APIs hosted on EC2 instances. To handle heavy traffic and ensure high availability, these are put behind an Elastic Load Balancer. Now, when these APIs are hit with requests, the connections must be secure to protect any sensitive data. Therefore, SSL or HTTPS encryption is a must.\n\n```\nLoadBalancer:\n   Type: \"AWS::ElasticLoadBalancing::LoadBalancer\"\n   Properties:\n      Listeners: \n      - LoadBalancerPort: '80'\n        InstancePort: '80'\n        Protocol: HTTP\n        SSLCertificateId: \"arn:aws:iam::123456789012:server-certificate/certName\"\n      - LoadBalancerPort: '443'\n        InstancePort: '443'\n        Protocol: HTTPS\n        SSLCertificateId: \"arn:aws:iam::123456789012:server-certificate/certName\"\n```\nThe above is an example how to configure the load balancer with SSL in AWS template. It ensures that any data in transit is encrypted and secure, demonstrating the importance of the control in question."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_dynamodb_table_protected_by_backup_plan",
    "Title": "DynamoDB table should be protected by backup plan",
    "Description": "Ensure if Amazon DynamoDB tables are protected by a backup plan. The rule is non compliant if the DynamoDB Table is not covered by a backup plan.",
    "QueryID": "aws_dynamodb_table_protected_by_backup_plan",
    "DocumentURI": "policies/aws_dynamodb_table_protected_by_backup_plan.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/DynamoDB"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "This AWS Control is essentially a set of preconditions that say DynamoDB tables should be safeguarded by a backup plan. Here is the same control described in a markup format:\n\n```markdown\n# AWS Control: Backup Plan for Amazon DynamoDB Tables\n\nThis control ensures that Amazon DynamoDB tables have an active backup plan, which is crucial to protect your business data from loss or accidental deletion.\n\nA DynamoDB Table is considered compliant when it is included in a backup plan. Conversely, if a DynamoDB Table is not covered by a backup plan, the rule is in a non-compliant state.\n\nTo implement this control, follow these steps:\n\n1. Navigate to the AWS Backup console.\n2. Create a backup plan or edit an existing one to include your DynamoDB tables.\n3. Make sure you set appropriate backup and retention parameters as per your business needs.\n4. Regularly monitor your backup activities and revise the backup plan if your requirements change.\n\nThis helps to ensure that your important DynamoDB data is always recoverable, thus providing a robust safety net against data loss or corruption.\n```\nThe markup format above is written in Markdown, a lightweight markup language with plain-text-formatting syntax which you could use to structure and style your text on the web. It's worth mentioning this is an illustrative control and may differ from actual implementation procedures or process to set up backups for Amazon DynamoDB tables."
      ],
      "x-kaytu-noncompliance-cost": [
        "If an Amazon DynamoDB table is found to be non-compliant with the rule ensuring it's protected by a backup plan, the costs could be significant and multi-dimensional, which include:\n\n1. **Data loss**: Without a backup plan, critical data could be lost if a disaster or malfunction occurred. Data loss can significantly hamper business operations and also lead to revenue loss.\n\n2. **Recovery costs**: If data is lost, the cost to recover or rebuild that data could be significant. This includes manpower, resources, and potentially third-party services to assist in data recovery.\n\n3. **Business Operations Interruption**: The downtime incurred by data loss or the need to recover data can interrupt business operations, leading to a loss in productivity and revenue, and damage the business reputation.\n\n4. **Non-compliance penalties**: If under any regulatory requirements, a data backup/retention policy is mandatory, failure to comply may result in heavy fines and penalties.\n\n5. **Data Security**: Backup is also an important part of data security. In case of any mishap, cyber-attack, or data tampering, the backed-up data can act as a reliable and quick source to restore the required information.\n\n6. **Customer Trust**: Failure to properly backup data can lead to a loss of customer confidence and trust, which can further lead to loss of customers and sales.\n\nIt’s crucial to ensure that all DynamoDB tables are covered by a backup plan to mitigate these risks and costs."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, a company hosts their customer data on Amazon DynamoDB. This includes sensitive information such as addresses, contact details, and purchase history. An accidental removal or modification of this data could be catastrophic for their business operations and customer relationships.\n\nBy implementing an AWS control that ensures DynamoDB tables are protected by a backup plan, the company can recover the information in case of unexpected data loss. For example, if a table were accidentally deleted or a batch write operation mistakenly overwrote items in your table, having a recent backup could allow the company to restore the lost information.\n\nThe company would be non-compliant with this control if their DynamoDB Table is not covered by a backup plan, thus exposing them to potential risk of data loss. By remaining compliant, the company ensures business continuity, maintains customer trust, and avoids the financial and resource costs associated with data recovery."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_redshift_cluster_prohibit_public_access",
    "Title": "Redshift clusters should prohibit public access",
    "Description": "Manage access to resources in the AWS Cloud by ensuring that Amazon Redshift clusters are not public.",
    "QueryID": "aws_redshift_cluster_prohibit_public_access",
    "DocumentURI": "policies/aws_redshift_cluster_prohibit_public_access.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/Redshift"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# Manage Amazon Redshift Clusters Access\n\nAmazon Web Services (AWS) offers a range of cloud computing services, including Amazon Redshift, a fast, fully-managed, petabyte-scale data warehousing service that makes it simple and cost-effective to efficiently analyze all your data. However, your clusters' security is crucial for data safety.\n\n## Importance of Non-public Redshift Clusters\n\nThe accessibility of Redshift clusters must be carefully managed to prevent data breaches. Allowing a Redshift cluster to be publicly accessible can expose them to unnecessary risks and potential attacks. Therefore, AWS recommends that Redshift clusters are not made publicly accessible.\n\n## Managing Access to Redshift Clusters\n\nTo control the accessibility of your Redshift clusters, follow these steps:\n\n1. **Sign into the AWS Management Console**\n\n2. **Open the Amazon Redshift Dashboard**\n   Navigate to the Amazon Redshift console at `https://console.aws.amazon.com/redshift/`.\n\n3. **View Cluster details**\n   In the navigation pane, choose `Clusters`. On the clusters page, choose the name of the cluster that you want to modify.\n\n4. **Modify Cluster settings**\n   On the Configuration tab, for `Publicly accessible`, ensure that it is set to `No`. If the setting is `Yes`, choose `Modify` to change it.\n\nBy ensuring your Amazon Redshift clusters are not public, you maintain rigorous control over your data warehouse, thereby preventing unnecessary access to sensitive data.\n\nPlease note that some services or applications may require public accessibility due to their nature. Always review your security policies thoroughly before applying changes."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with Amazon RedShift clusters being public can result in multiple costs:\n\n1. **Security Breaches**: If Amazon RedShift clusters are public, they can be accessed by anyone on the internet. This can lead to data breaches if any malicious actor tries to access the cluster and gain unauthorized access.\n\n2. **Data Loss** : With an open access policy, there's an escalated risk of data loss due to accidental deletions or alterations.\n\n3. **Regulatory Fines** : Non-compliance can lead to regulatory fines for violating data privacy laws and regulations. If your organization stores sensitive data (like personal or customer-related), it needs to comply with regulations such as GDPR, HIPAA, etc.\n\n4. **Reputation Damage** : Data breaches can result in serious reputational damage. It erodes trust amongst clients and users, and this can have long-term impacts on your client base and business reputation.\n\n5. **Monetary Loss** : In case of a data breach or data loss, the cost of recovery can be very high. This includes the cost of identifying the breach, recovering lost data, implementing additional security measures, and possible downtime.\n\nIt is thus essential to manage access to resources in the AWS cloud by ensuring that Amazon Redshift clusters are not public. This can be achieved through proper configuration and security group settings coupled with regular audits. This not only helps in maintaining data integrity and security but also ensures regulatory compliance."
      ],
      "x-kaytu-usefulness-example": [
        "Consider a situation where a company named \"TechCorp\" uses Amazon Redshift to analyze their business data. They rely on it heavily to identify trends, make projections, and build business strategies.\n\nHowever, making the Redshift clusters public would mean potentially exposing sensitive data to malicious users. This could translate into severe financial and reputational losses for \"TechCorp\".\n\nIn this context, the AWS control that ensures that Amazon Redshift clusters are not public becomes exceptionally useful.\n\nFor instance:\n\n- A developer at \"TechCorp\" accidentally configures the Redshift cluster to be publicly accessible. The AWS control, which continuously checks the public accessibility of all Redshift clusters, indicates this misconfiguration. The security team at \"TechCorp\" receives an alert about this issue and rectifies the setting before any threat actors could exploit it.\n\n- \"TechCorp\" works with several partners who also need access to certain data in the Redshift cluster. The AWS control allows them to manage access at a granular level, ensuring that each partner only has access to what's necessary for their operations. This prevents any possible data leaks and maintains the confidentiality of \"TechCorp\"'s data.\n\nHence, the AWS control that ensures Redshift clusters are not public is highly beneficial in protecting \"TechCorp\"'s data and maintaining its business integrity."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_s3_bucket_default_encryption_enabled_kms",
    "Title": "S3 bucket default encryption should be enabled with KMS",
    "Description": "To help protect data at rest, ensure encryption is enabled for your Amazon Simple Storage Service (Amazon S3) buckets.",
    "QueryID": "aws_s3_bucket_default_encryption_enabled_kms",
    "DocumentURI": "policies/aws_s3_bucket_default_encryption_enabled_kms.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/S3"
      ],
      "x-kaytu-explanation": [
        "AWS Simple Storage Service (S3) is a scalable service for object storage in the cloud. To ensure data protection, you should enable encryption for all S3 buckets. \n\nEncryption of data at rest in S3 provides an additional layer of security to keep your data safe from unauthorized access. AWS provides two methods for you to encrypt data: server-side encryption and client-side encryption. \n\nServer-side encryption further has three options:\n1. S3 managed keys (SSE-S3)\n2. AWS Key Management Service (SSE-KMS)\n3. Server Side Encryption with customer-provided keys (SSE-C)\n\nWith these methods, each object is encrypted with a unique key, and AWS also encrypts the key itself with a master key that it regularly rotates. \n\nHere is some sample markup text to enable server-side encryption at the bucket level:\n\n```markdown\n{\n    \"rules\" : [\n        {\n            \"applyServerSideEncryptionByDefault\" : {\n                \"sseAlgorithm\" : \"AES256\"\n            }\n        }\n    ]\n}\n```\n\nThis is a JSON structure that specifies the server-side encryption configuration rule for a bucket. The `\"applyServerSideEncryptionByDefault\"` element requests Amazon S3 to encrypt all stored objects using server-side encryption with AWS S3-managed encryption keys.\n\nReplacing \"AES256\" with \"aws:kms\" would instruct AWS to use AWS Key Management Service for data-at-rest encryption.\n\nYou can enter this markup text when creating a new S3 bucket or apply it later by modifying a bucket's properties.\n\nRemember, combining features such as IAM policies, bucket policies, and enabling logging and versioning will give your S3 data an extra level of security and this with 𝑆3 𝑏𝑢𝑐𝑘𝑒𝑡 𝑒𝑛𝑐𝑟𝑦𝑝𝑡𝑖𝑜𝑛 is a great way to increase the security of your S3 data at rest."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the mentioned AWS Control, which requires encryption for Amazon S3 buckets, can be significant, especially in terms of operational, reputational, legal and financial aspects. \n\n1. **Operational Cost:** If sensitive data is stored in S3 buckets without encryption, it may be susceptible to breaches. A significant data breach could disrupt your business operations, leading to productivity loss and potentially causing services to be unavailable for a period of time.\n\n2. **Reputational Cost:** A data breach can cause significant reputational damage to businesses. Customers trust businesses with their personal data, and if that trust is broken, it can be challenging to regain. A loss in reputation could result in a loss of clients or customers.\n\n3. **Legal and Regulatory Cost:** Various laws and regulations require businesses to protect personal data. Non-compliance with these requirements can result in significant fines and penalties. For example, under GDPR, firms can be fined up to 4% of their annual global turnover or €20 Million (whichever is greater) for non-compliance.\n\n4. **Financial Cost:** Lastly, the direct financial costs associated with responding to a data breach can be substantial. These costs can include forensic analysis to understand the breach, public relations efforts, legal expenses, regulatory fines, and potential litigation from affected customers.\n\nHence, it is crucial to comply with AWS controls and ensure that encryption is enabled for your Amazon S3 buckets to protect data at rest."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, an e-commerce company is storing sensitive customer information (credit card details, addresses, etc.) in Amazon S3 buckets as part of their daily operations. Enabling encryption for these S3 buckets is crucial in this case. If there was an attempt to breach the company’s data, the encryption would make it unreadable and useless to the attacker, thus protecting the customers' information. \n\n```markdown\nHere is a step-by-step guide:\n1. Open the Amazon S3 console at https://console.aws.amazon.com/s3/.\n2. In the **Bucket name** list, choose the name of the bucket that you want to enable default encryption for.\n3. Choose **Properties**.\n4. In the **Default encryption** section, choose **Edit**.\n5. Choose **Enable**.\n6. In the **Server-side encryption** section, choose **Amazon S3 key (SSE-S3)**.\n7. Choose **Save changes**.\n```\nWith these steps, your S3 buckets' data will be encrypted and secure against unauthorized access."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_secretsmanager_secret_last_changed_90_day",
    "Title": "Secrets Manager secrets should be rotated within specific number of days",
    "Description": "Ensure if AWS Secrets Manager secrets have been rotated in the past specified number of days. The rule is non compliant if a secret has not been rotated for more than 'maxDaysSinceRotation' number of days. The default value is 90 days.",
    "QueryID": "aws_secretsmanager_secret_last_changed_90_day",
    "DocumentURI": "policies/aws_secretsmanager_secret_last_changed_90_day.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/SecretsManager"
      ],
      "x-kaytu-explanation": [
        "AWS Secrets Manager is a service that helps you protect access to your applications, services, and IT resources without the front-end burden of managing passwords. It enables you to rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.\n\nThis rule checks if the AWS Secret Manager secrets have been rotated within a specified number of days. The 'maxDaysSinceRotation' value holds the maximum number of days that is allowed since the last rotation. If a secret has not been rotated for more days than specified in 'maxDaysSinceRotation', the rule becomes non-compliant. By default, the 'maxDaysSinceRotation' value is set to 90 days.\n\nThis check is important for maintaining security. Regular rotation of secrets prevents misuse in case your secrets are compromised and prevents someone who gains unauthorized access from performing destructive actions. \n\nHere is an example in markup format:\n\n    ```yaml\n    AWS::Config::ConfigRule:\n        Properties:\n            ConfigRuleName: 'secret-rotation-check'\n            Description: 'Checks that secrets have been rotated within a specified number of days'\n            Scope:\n                ComplianceResourceTypes:\n                  - \"AWS::SecretsManager::Secret\"\n            Source:\n                Owner: 'AWS'\n                SourceIdentifier: 'AWAFS_SecretsManager_Rotated_Secrets'\n            MaximumExecutionFrequency: 'TwentyFour_Hours'\n            InputParameters: \n              maxDaysSinceRotation: '90'\n    ```"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS Control can result in significant financial, operational, and reputational costs:\n\n1. **Financial Costs**: Failure to rotate secrets can lead to data breaches. According to a study by IBM Security and Ponemon Institute, the average cost of a data breach in 2020 was $3.86 million. This includes costs related to activities such as investigations, notifications, remediation, regulatory fines, and lawsuits.\n\n2. **Operational Costs**: There may also be operational costs associated with a breach. These can include downtime, lost productivity, business disruption, revenue loss, as well as costs associated with the investigation, remediation, and strengthening of security measures post-breach.\n\n3. **Reputational Costs**: One of the largest costs associated with a breach can be reputational. If customer or client data is compromised due to inadequate security practices, this can negatively affect customer trust and brand reputation, leading to a decrease in the customer base, a drop in sales, and potentially, a decrease in the company's market value.\n\n4. **Regulatory Costs**: In many jurisdictions, organizations are legally required to protect sensitive data. Failure to comply with these regulations can result in fines and penalties. For example, under the General Data Protection Regulation (GDPR) in Europe, failing to adequately protect personal data can result in fines of up to 20 million Euro or 4% of the company's global annual turnover, whichever is higher.\n\nTherefore, ensuring AWS Secrets Manager secrets are regularly rotated is critical to minimizing these potential costs."
      ],
      "x-kaytu-usefulness-example": [
        "This AWS Control is useful in the context of a security-sensitive organization that handles confidential data. Let's imagine a leading financial institution uses AWS Secret Manager to handle its database credentials, API keys, and other sensitive secrets.\n\nThis control is set up to ensure they regularly rotate their secrets every 90 days, minimizing the potential impact if a secret is compromised. If the rule finds a secret that has not been rotated in the specified period, it becomes non-compliant, alerting the security team.\n\nExample:\n\n```\nResources:\n  SecretRotationRule:\n    Type: 'AWS::Config::ConfigRule'\n    Properties:\n      ConfigRuleName: secret-rotation-rule\n      Description: Ensure AWS Secrets are rotated in previous 90 days\n      Scope:\n        ComplianceResourceTypes:\n          - 'AWS::SecretsManager::Secret'\n      Source:\n        Owner: CUSTOM_LAMBDA\n        SourceDetails:\n          - EventSource: aws.config\n            MessageType: ConfigurationItemChangeNotification\n          - EventSource: aws.config\n            MessageType: OversizedConfigurationItemChangeNotification\n        SourceIdentifier: arn:aws:lambda:us-east-1:123456789012:function:check_secret_rotation\n      InputParameters:\n        maxDaysSinceRotation: '90'\n      MaximumExecutionFrequency: TwentyFour_Hours\n```\n\nIn this instance, a Lambda function '`check_secret_rotation`' is triggered by AWS Config every time a Secret Manager secret is evaluated. The function checks the last rotated date of the secret, and if it's more than 90 days ago, marks it as non-compliant in AWS Config, providing an alert mechanism for secrets not regularly rotated."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_iam_policy_inline_no_blocked_kms_actions",
    "Title": "Ensure inline policies attached to IAM users, roles, and groups should not allow blocked actions on KMS keys",
    "Description": "Checks if the inline policies attached to IAM users, roles, and groups do not allow blocked actions on all AWS Key Management Service (KMS) keys. The rule is non - compliant if any blocked action is allowed on all KMS keys in an inline policy.",
    "QueryID": "aws_iam_policy_inline_no_blocked_kms_actions",
    "DocumentURI": "policies/aws_iam_policy_inline_no_blocked_kms_actions.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/IAM"
      ],
      "x-kaytu-explanation": [
        "This AWS control checks whether the policies attached to AWS users, roles, and groups (also known as Identity and Access Management or IAM entities) are preventing specific, undesirable actions on AWS Key Management Services (KMS) keys.\n\nKMS keys are important cryptographic assets that protect your data. It's essential to control who can execute actions like decrypt, encrypt, or delete these keys. \n\nThe specific actions that should be blocked are defined by your organization's security policy. The control checks any inline policy (a policy that's embedded directly into a single IAM entity) attached to IAM users, roles, and groups and verifies that none of these policies grant permissions for the blocked actions.\n\nInline policies are especially important to check because they directly grant the permissions to their associated IAM entity. It does not matter what other policies that IAM entity has; if they have an inline policy that allows a blocked action, they can perform it. \n\nIf the control finds an inline policy that allows any of the blocked actions on all KMS keys, then it reports that the control is \"non-compliant\". It means that your AWS configuration violates your organization's security policy. Take immediate action to correct the offending policy and return to a compliant state."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS Control can potentially lead to several costs:\n\n1. **Financial Cost**: Malicious actors could potentially perform unwanted operations which could lead to unnoticeable financial losses. For example, an attacker might be able to use your KMS keys to perform resource-intensive operations that add to your AWS bill. \n\n2. **Data Privacy Violation**: Unauthorized use of Key Management Service (KMS) keys could lead to data leakages. This could not only lead to financial penalties due to non-compliance with data protection laws, but also damage to the company's reputation, which could in turn lead to loss of business.\n\n3. **Operational Disruptions**: Improper management of KMS keys could disrupt your regular operations. For instance, if keys are inappropriately deleted or altered, it could prevent your services from functioning correctly, which could cost you in the form of down-time.\n\n4. **Regulatory Fines**: If your organization operates within a regulated industry, non-compliance to this rule could result in hefty fines from regulatory bodies.\n\n5. **Security Incidents**: Non-compliance to the rule could lead to an increased risk of security incidents or breaches, which could have significant financial implications for your organization in terms of incident response and recovery efforts. \n\nIn short, non-compliance to this AWS control that checks if the inline policies attached to IAM users, roles, and groups do not allow blocked actions on all AWS Key Management Service (KMS) keys can have substantial financial, operational, and reputational costs. Therefore, it is crucial to ensure that all inline policies are compliant with this rule."
      ],
      "x-kaytu-usefulness-example": [
        "This AWS control is particularly useful in maintaining the security and integrity of your data. \n\nFor instance, imagine a scenario where your organization operates on highly sensitive data, and you utilise AWS Key Management Services (KMS) to control access to this data. It's critical for your business that only specific IAM users, roles, or groups have the ability to perform certain actions on these KMS keys. \n\nWith this AWS control, you can easily verify if any of your current inline policies attached to IAM users, roles, and groups are violating this rule. If a violation occurs, you would be notified that these actions are blocked.\n\n```markdown\nExample:\n----------\n1. **Company XYZ** uses AWS KMS for cryptographic keys to encrypt sensitive data. They have a policy that only specific roles can 'Encrypt' and 'Decrypt' with these keys. They have enabled the above-mentioned AWS Control.\n\n2. **Employee-A** is assigned a new IAM role with attached inline policies. An inadvertent mistake allows this role to 'Decrypt' using any KMS key, which is against the company's security policy.\n\n3. AWS Control immediately flags the role as non-compliant. \n\n4. The security team is alerted about this non-compliance and they promptly fix the inline policy to block the 'Decrypt' action for **Employee-A**.\n\n5. Thus, the AWS Control helped prevent a potential security breach by ensuring that blocked actions are not being allowed on all KMS keys in an inline policy.\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_efs_file_system_protected_by_backup_plan",
    "Title": "EFS file systems should be protected by backup plan",
    "Description": "Ensure if Amazon Elastic File System (Amazon EFS) File Systems are protected by a backup plan. The rule is non compliant if the EFS File System is not covered by a backup plan.",
    "QueryID": "aws_efs_file_system_protected_by_backup_plan",
    "DocumentURI": "policies/aws_efs_file_system_protected_by_backup_plan.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/EFS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "The AWS control you mentioned relates to the backup of Amazon Elastic File System (EFS). It is a rule that ensures that all EFS files are protected by a backup plan. A backup plan is a defined policy that specifies how Amazon EFS data is to be backed up to ensure data preservation and recovery in case of accidental deletion, data corruption, or any incidents that might result in loss of data. \n\nHere is the explanation in markup format:\n\n```markdown\n# AWS Control: EFS Backup Plan\n\nThis AWS control rule ensures the protection of Amazon Elastic File System (Amazon EFS) through the use of backup plans. \n\n## Rule\n\nIf the EFS File System is not included in a backup plan, then the rule is non compliant. \n\n## Compliance\n\nCompliance with this rule is achieved when all EFS file systems are covered by a backup plan, safeguarding the data and ensuring it can be restored in case of data loss incidents. \n\n## Non-Compliance\n\nIf an EFS File System is found not to be covered by a backup plan, the rule is marked as non-compliant, and immediate action is required to rectify this situation. \n\n## Importance\n\nFollowing this rule is crucial as it ensures the persistent storage and protection of your Amazon EFS data which could otherwise be at risk. This backup strategy enhances data integrity, disaster recovery and reduces downtime.\n```\nThis markup can be used in various documentation mediums including README files, internal wikis, and as comments in your infrastructure code."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the AWS control that ensures Amazon Elastic File System (Amazon EFS) File Systems are protected by a backup plan can be significantly high and may broadly include the following areas:\n\n1. **Data loss**: If AWS EFS data is not backed up and an unexpected event such as a system failure, accidental deletion, or malicious attack occurs, the data may be permanently lost. This could lead to both direct financial losses and less quantifiable damage such as loss of customer trust or reputation.\n\n2. **Business Continuity/Disaster Recovery**: In case of a disaster, the lack of a backup for EFS could mean loss of critical system files required to run your applications. This can result in extended downtime for your web applications or services, thus disrupting business operations and leading to revenue loss.\n\n3. **Compliance Violations**: Many industries are governed by regulatory bodies that enforce data protection laws; and lack of appropriate backup could lead to penalties and fines. Organizations might also fail audits which can further lead to repercussions like losing business due to lack of compliance certification, or legal consequences for failing to adhere to data protection standards.\n\n4. **Increased Recovery Time and Cost**: Without a backup plan in place, the recovery from a data loss incident can be time-consuming and costly. The efforts required to manually recover data or rebuild systems can be considerable.\n\nIt's worth noting that these costs can vary greatly depending on the nature of the data, the extent of the data loss, and the specific regulatory environment applicable to a given organization or industry. But the risks and potential costs make it clear that ensuring EFS File Systems are properly backed up should be a priority for AWS users."
      ],
      "x-kaytu-usefulness-example": [
        "For example, a tech startup may use Amazon Elastic File System (EFS) to store data used by its applications. The data is important for the day-to-day operations and a loss of data may cause severe business interruptions. In case of hardware failure, data corruption, or even human error, data could be lost permanently.\n\nBy enforcing a backup plan for the EFS File Systems, the startup ensures that the critical data can be recovered. So, when an unexpected incident happens and results in a data loss, the startup can easily restore the data from the backup. This can minimize the downtime and the impact to the business.\n\nHere is an example implementation in AWS Backup:\n\n```markup\n{\n    \"BackupPlan\": {\n        \"BackupPlanName\": \"efs-backup-plan\",\n        \"Rules\": [\n            {\n                \"RuleName\": \"efs-daily\",\n                \"TargetBackupVaultName\": \"efs-backup-vault\",\n                \"ScheduleExpression\": \"cron(0 12 * * ? *)\",\n                \"StartWindowMinutes\": 60,\n                \"CompletionWindowMinutes\": 240,\n                \"Lifecycle\": {\n                    \"DeleteAfterDays\": 30\n                }\n            },\n            {\n                \"RuleName\": \"efs-monthly\",\n                \"TargetBackupVaultName\": \"efs-backup-vault\",\n                \"ScheduleExpression\": \"cron(0 12 1 * ? *)\",\n                \"StartWindowMinutes\": 60,\n                \"CompletionWindowMinutes\": 240,\n                \"Lifecycle\": {\n                    \"DeleteAfterDays\": 365\n                }\n            }\n        ]\n    }\n}\n```\n\nThis example backup plan activates daily and monthly backups for EFS File Systems, keeping daily backups for 30 days and monthly backups for a year."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_fsx_file_system_protected_by_backup_plan",
    "Title": "FSx file system should be protected by backup plan",
    "Description": "Checks if Amazon FSx File Systems are protected by a backup plan. The rule is non compliant if the Amazon FSx File System is not covered by a backup plan.",
    "QueryID": "aws_fsx_file_system_protected_by_backup_plan",
    "DocumentURI": "policies/aws_fsx_file_system_protected_by_backup_plan.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "service": [
        "AWS/FSx"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "This AWS Control uses the rules to check whether all the Amazon FSx File Systems in your AWS environment are protected by a backup plan. If any Amazon FSx File System is found without a backup plan, this rule becomes non-compliant.\n\nA compliant resource for this rule is an Amazon FSx file system that is part of an AWS Backup plan. A non-compliant resource is an Amazon FSx file system that is not part of an AWS Backup plan. \n\n```markdown\n## AWS Control: Amazon FSx Backup Check \n\nThis control validates if Amazon FSx File Systems are properly protected by a backup plan. If any Amazon FSx File System does not have a backup plan, the rule will be non-compliant. \n\n### Compliant Resource:\n\nThe AWS resources that satisfy this rule are the Amazon FSx file systems that are part of an AWS Backup plan.\n\n### Non-Compliant Resource:\n  \nThe AWS resources that fail to meet this rule are the Amazon FSx file systems that are not part of an AWS Backup plan.\n```\n\nThe importance of this rule lies in ensuring that all File Systems have a backup plan attached to them for disaster recovery purposes. Being non-compliant with this rule could lead to potential data loss if anything were to happen to an unprotected file system."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the AWS control that checks if Amazon FSx File Systems are protected by a backup plan could lead to several operational and financial impacts. This includes:\n\n1. **Data Loss**: If a FSx file system is not backed up properly, any failure or corruption that leads to data loss can end up being permanent. The data recovery operation can end up being impossible, which would have severe implications especially if it involves critical operational data.\n\n2. **Financial Implications**: A major failure without backup may require a complete re-creation of the lost data or system. This could involve considerable human resources and time, leading to financial costs. Additionally, the value of lost critical data itself can translate to substantial direct financial losses.\n\n3. **Operational Downtime**: The lack of a backup plan can lead to greater durations of operational downtime in the event of a data loss or failure incident. The time taken to recreate lost data or restore operations would be significantly higher without a backup.\n\n4. **Compliance Violations and Legal Liabilities**: If the data involves regulated and sensitive information, not having a backup could lead to non-compliance with data protection regulations. This could result in fines, legal implications, and damage to the organization’s reputation.\n\n5. **Customer Trust**: The inability to quickly recover from an incident due to a lack of backups can impact the trust of customers, especially if it involves downtime in the client-facing functions or data breaches.\n\nThus, it is crucial to ensure compliance with this AWS control."
      ],
      "x-kaytu-usefulness-example": [
        "Scenario:\nLet's say your company is a multinational financial services firm that relies heavily on Amazon FSx as a tool to provide fully managed native Microsoft Windows file system capabilities. Your company uploads and download countless volumes of business-critical data every day, which includes valuable client information, financial documentation, uncompiled source code, and proprietary research. \n\nGiven the massive volume and the utmost importance of the data your company is dealing with, any loss of data can lead to irreversible consequences such as financial losses, reputational damage, or failure to comply with external and internal data regulations. \n\nTherefore, you set up a backup plan in AWS to make sure all your valuable data stored in Amazon FSx is regularly backed up and can be easily restored whenever necessary. However, with the complexity of your system, it is hard to keep track of which file systems are protected by a backup plan manually.\n\nUsing the AWS Control that checks whether each Amazon FSx File System is covered by a backup plan will be extremely useful for your company. If any system is found to be non compliant (not protected by a backup plan), the AWS Control will alert your company, and you can immediately create a backup plan for that particular system. This control ensures that no critical FSx File System is left unprotected and helps your company prevent devastating data loss, hence ensuring business continuity, integrity, and compliance with data regulations."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_backup_recovery_point_encryption_enabled",
    "Title": "Backup recovery points should be encrypted",
    "Description": "Ensure if a recovery point is encrypted. The rule is non compliant if the recovery point is not encrypted.",
    "QueryID": "aws_backup_recovery_point_encryption_enabled",
    "DocumentURI": "policies/aws_backup_recovery_point_encryption_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/Backup"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "This AWS config rule ensures that all data recovery points are encrypted. The rule checks whether AWS Backup recovery points are encrypted. If a recovery point is not encrypted, then the rule is marked as non-compliant. \n\n**AWS Config Rule:**\n\n```\n{\n  \"ConfigRuleName\": \"backup-recovery-point-encrypted\",\n  \"Description\": \"Checks that AWS Backup recovery points are encrypted\",\n  \"Scope\": {\n    \"ComplianceResourceTypes\": [\"AWS::Backup::BackupVault\"]\n  },\n  \"Source\": {\n    \"Owner\": \"AWS\",\n    \"SourceIdentifier\": \"BACKUP_RECOVERY_POINT_ENCRYPTED\",\n    \"SourceDetails\": [\n      {\n        \"EventSource\": \"aws.config\",\n        \"MessageType\": \"ConfigurationItemChangeNotification\"\n      },\n      {\n        \"EventSource\": \"aws.config\",\n        \"MessageType\": \"OversizedConfigurationItemChangeNotification\"\n      }\n    ]\n  },\n  \"InputParameters\": {\n    \"keyId\": \"\u003cKMS-key-ID\u003e\"\n  }\n}\n```\n\nIn the above code block:\n\n- `ConfigRuleName` is the name of the AWS Config rule.\n\n- `Description` explains what the rule does.\n\n- `Scope` is the types of AWS resources that the rule checks.\n\n- `Source` specifies who the owner of the rule is, and the source identifier signals which config rule to apply.\n\n- `Owner` is set to \"AWS\" since this is a managed rule, created and managed by AWS.\n\n- `SourceIdentifier` is the unique identifier for the rule. \n\n- `SourceDetails` lists the messages that trigger the rule evaluation.\n\n- `InputParameters` provides the ID of the KMS encryption key that is used to encrypt the backup recovery points.\n\nThis rule improves the overall security of your AWS environment by ensuring that all of your data recovery points are encrypted. For especially sensitive data, failing to encrypt a recovery point might expose your data to unnecessary risks."
      ],
      "x-kaytu-noncompliance-cost": [
        "Failing to comply with this AWS Control could present the following costs:\n\n1. **Security Cost:**\n   Not encrypting a recovery point could potentially expose sensitive data to threat actors. Non-encrypted data is vulnerable to breaches \u0026 attacks, which could compromise your business's critical information. \n\n2. **Financial Cost:**\n   If a data breach occurs owing to poorly-protected recovery points, this could lead to significant financial penalties. Moreover, the cost of resolving the breach and the potential loss of business due to reputational damage can be substantial.\n\n3. **Regulatory Compliance Cost:**\n   Non-compliance with data privacy laws and regulations such as GDPR, HIPAA, etc., could result in hefty fines \u0026 penalties. These regulations mandate the protection of sensitive information, \u0026 non-compliant businesses may face serious ramifications.\n\n4. **Operational Cost:**\n   In the event of a data breach, your operations may be disrupted, impacting the productivity of your business. You may also need to invest more resources into investigating the issue and repairing the damage caused by the breach, increasing your operational costs.\n\n5. **Reputation Cost:**\n   Customer trust is vital in any business. If your business suffers a data breach due to unencrypted recovery points, you may lose your customer’s faith. This could lead to a loss of business and impact your organization's reputation in the long term."
      ],
      "x-kaytu-usefulness-example": [
        "This AWS control instance is crucial in maintaining the security and integrity of data. \n\nFor example:\n\nA finance company often needs to store sensitive client data involving transactions and personal details. They use AWS for their data storage and backups. In this case, they could have set up a recovery point for a data set. \n\nHowever, if this recovery point is not encrypted, it becomes a prime target for cyber attackers who could intercept this data during a recovery process and compromise the sensitive financial data of multiple clients. The stolen data could then be misused for identity theft, fraudulent transactions, or sold on the dark web. \n\nBut, with this AWS control in place, the finance company will be made aware of the non-compliant, non-encrypted recovery point. They can then take immediate action to encrypt the recovery point, thus securing the data during recovery, and ensuring the privacy and safety of the business and its clients. \n\nThis instance shows the usefulness of being able to ensure that a recovery point is encrypted in maintaining the security of sensitive data."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_es_domain_node_to_node_encryption_enabled",
    "Title": "Elasticsearch domain node-to-node encryption should be enabled",
    "Description": "Ensure node-to-node encryption for Amazon Elasticsearch Service is enabled. Node-to-node encryption enables TLS 1.2 encryption for all communications within the Amazon Virtual Private Cloud (Amazon VPC).",
    "QueryID": "aws_es_domain_node_to_node_encryption_enabled",
    "DocumentURI": "policies/aws_es_domain_node_to_node_encryption_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ES"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS Elasticsearch Service is a fully managed service that makes it easy to deploy, secure, operate, and scale Elasticsearch to search, analyze, and visualize data in real-time. \n\nOne of the key security features of the service is node-to-node encryption, which ensures that the data moving between the nodes of an Elasticsearch cluster is safe and secure. This means all communications between these nodes are encrypted, offering an additional layer of security for your data.\n\nHere's a guideline on enabling this feature, written in a markup format:\n\n```markdown\n# Enable Node-to-Node Encryption for Amazon Elasticsearch Service\n\nTo enhance security within your Elasticsearch cluster, you should enable node-to-node encryption. This feature enables TLS 1.2 encryption for all communications within the Amazon Virtual Private Cloud (Amazon VPC).\n\n## Prerequisites\n\nBefore enabling node-to-node encryption, ensure the following:\n\n- You have access to the AWS Management Console \n- You have the necessary permissions to configure the Elasticsearch cluster settings\n\n## Procedure \n\nFollow these steps to enable node-to-node encryption:\n\n1. Open the Amazon Elasticsearch Service console.\n2. In the navigation pane, choose **\"Domain names\"**.\n3. Choose the name of the Elasticsearch domain that you want to modify.\n4. Under the **\"Domain Configurations\"** section, choose **\"Modify Configurations\"**.\n5. Scroll down to find the **\"Node-to-Node Encryption\"** setting.\n6. If not already enabled, choose **\"Yes\"** to enable node-to-node encryption.\n7. Choose **\"Submit\"** to apply changes.\n\nAs a result, all data communication within your Elasticsearch cluster in the Amazon VPC will be encrypted.\n```\n\nBy enabling this feature, you're adding an extra layer of security for in-flight data within your Amazon VPC."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS control of ensuring node-to-node encryption for Amazon Elasticsearch Service can have several costs which include:\n\n1. **Risk of Data Breaches**: Without node-to-node encryption, your sensitive data transferred within the VPC is vulnerable to unauthorized interception. This could potentially lead to data breaches, causing significant financial and reputational damages.\n\n2. **Legal Penalties and Compliance Issues**: For organizations handling sensitive data(like healthcare or financial data), not enabling encryption could lead to non-compliance with regulations such as GDPR or HIPAA. This non-compliance can result in hefty fines, legal penalties, and loss of trust with customers.\n\n3. **Confidentiality Issues**: Without encryption, the confidentiality of your data within the VPC is compromised. Anyone with access to the network can view the unencrypted data, potentially leading to misuse of information.\n\n4. **Loss of Trust**: In the event of a data breach, the fallout can also include loss of trust from your clients and customers, which can take a long time to rebuild and result in lost business.\n\n5. **Risk to Data Integrity**: Unencrypted data can not only be viewed but can be altered as well. This puts the integrity of the data at risk as unauthorized changes can go unnoticed.\n\n6. **Potential business disruption**: Any data breach involving unencrypted data might force a company to halt its business operations to deal with this issue, leading to business disruption and potential revenue loss.\n\nIn conclusion, while it might seem cost-effective to not enable node-to-node encryption for Amazon Elasticsearch Service, the potential risks and costs related to data security, legal penalties, and damage to reputation may far outweigh the initial savings. Thus, not complying with this control can have dire consequences."
      ],
      "x-kaytu-usefulness-example": [
        "```\nExample:\n\nIn the context of a financial institution, they may be handling highly sensitive customer data like transactional information, banking details, credit scores, and so forth. Security and data protection is paramount in such situations. \n\nLet's assume that this institution uses Amazon Elasticsearch service for its robust search and analytics capabilities. To ensure the intra-node communication is secure within the VPC, the financial institution must enable node-to-node encryption. \n\nBy doing so, even if a potential attacker gains access to the network, they will not be able to decipher or misuse the information as it is encrypted using TLS 1.2 encryption. This provides an added layer of security, optimally safeguarding sensitive data from potential threats.\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_vpc_subnet_auto_assign_public_ip_disabled",
    "Title": "VPC subnet auto assign public IP should be disabled",
    "Description": "Ensure if Amazon Virtual Private Cloud (Amazon VPC) subnets are assigned a public IP address. The control is compliant if Amazon VPC does not have subnets that are assigned a public IP address. The control is non compliant if Amazon VPC has subnets that are assigned a public IP address.",
    "QueryID": "aws_vpc_subnet_auto_assign_public_ip_disabled",
    "DocumentURI": "policies/aws_vpc_subnet_auto_assign_public_ip_disabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/VPC"
      ],
      "x-kaytu-explanation": [
        "This AWS control examines your Amazon Virtual Private Cloud (Amazon VPC) to ensure that your VPC subnets are not assigned a public IP address. \n\n\u003cblockquote\u003e\n\n*Compliance*:\n\nThe control will be deemed as compliant under the specific circumstance where Amazon VPC does not possess any subnets that are assigned a public IP address.\n\n*Non Compliance*:\n\nAlternatively, the control will be determined as non-compliant when Amazon VPC possesses one or more subnets that are allocated a public IP address.\n\n\u003c/blockquote\u003e\n\nThis is seen as a security measure, protecting your Amazon VPC, since assigning public IP addresses can open your VPC to unwanted outside access. This control ensures that this does not occur, upholding the safety and privacy of your VPC."
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to this AWS Control, which ensures that Amazon Virtual Private Cloud (Amazon VPC) subnets are not assigned a public IP address, can be significantly high, in both monetary and non-monetary terms. These costs can be categorized into potential data breach costs, compliance costs, and reputational costs.\n\n## 1. Data Breach Costs\nAssigning a public IP address to VPC subnets can expose your infrastructure to potential security threats. Unauthorized individuals or entities can gain access, leading to potential data breaches. The cost associated with these breaches can be:\n\n- **Financial loss**: The monetary loss due to the exploitation of critical business information. This could either be direct (i.e., theft of financial data) or indirect (i.e., theft of intellectual property leading to financial loss).\n- **Incident response costs**: These include the costs of analyzing the breach, containing it, and recovering from it.\n- **Regulatory fines**: If sensitive user information is exposed in the breach, enterprises could face substantial fines and penalties from regulatory bodies.\n\n## 2. Compliance Costs\nIf your organization needs to comply with various national or international regulations, non-compliance with this AWS Control could result in:\n\n- **Regulatory fines**: Non-compliance can attract fines and penalties from regulatory bodies.\n- **Cost of compliance rectification**: Resources will need to be dedicated to rectify the non-compliance, which could involve significant expenditure of time and money.\n\n## 3. Reputational Costs\nData breaches, particularly those that expose customer data, can lead to a loss of customer trust and a subsequent reduction in business due to reputational damage. The cost of lost business and the effort required to rebuild the company's reputation and regain customer trust can be substantial."
      ],
      "x-kaytu-usefulness-example": [
        "Consider an instance where an organization is deploying a web application in the AWS environment and the application needs to interact with the internet. All the instances of the web application should be hosted inside a Virtual Private Cloud (VPC) for security reasons.\n\n```markup\n\u003cExample\u003e\n    For instance, consider the data within the application is sensitive, and the organization applies security controls to limit access to only internal IP addresses. A VPC is thus necessary because it creates a secure and isolated environment to host the application inside AWS.\n\n    By ensuring that no Amazon VPC subnets are assigned a public IP address (as in the applied AWS Control), the organization can add an additional layer of security to their application. This control can prevent any direct interaction of the web application instances with the internet, thereby reducing the potential risk of attacks.\n\n    However, if a VPC subnet was found to be assigned a public IP address, it would be flagged as non compliant, as it would be exposing the web application to potential security risks. This makes it essential for the organization to audit their AWS environment regularly to ensure compliance with the control.\n\u003c/Example\u003e\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_elb_application_classic_lb_logging_enabled",
    "Title": "ELB application and classic load balancer logging should be enabled",
    "Description": "Elastic Load Balancing activity is a central point of communication within an environment.",
    "QueryID": "aws_elb_application_classic_lb_logging_enabled",
    "DocumentURI": "policies/aws_elb_application_classic_lb_logging_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ELB"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "```\n# Elastic Load Balancing in AWS\n\n**Elastic Load Balancing (ELB)** is a load balancing service provided by [Amazon Web Services (AWS)](https://aws.amazon.com/). It is used to automatically distribute incoming application traffic across multiple targets, such as Amazon EC2 instances, containers, IP addresses, and Lambda functions.\n\n## How ELB Works\n\nELB acts as a central point of communication within an AWS environment. It accepts incoming traffic from clients and routes requests to its registered targets (i.e., EC2 instances) in one or more Availability Zones. The load balancer continually checks the health of its registered targets and only sends requests to the healthy targets. When the load balancer detects an unhealthy target, it stops routing traffic until it becomes healthy again.\n\n## Types of ELB\n\nAWS provides three types of Elastic Load Balancer:\n\n1. **Application Load Balancer** (ALB): Best suited for load balancing HTTP and HTTPS traffic. It operates at the request level and provides advanced routing, target group stickiness, and other features.\n\n2. **Network Load Balancer** (NLB): Suited for load balancing TCP, UDP, and TLS traffic where extreme performance is required. It operates at the connection level.\n\n3. **Classic Load Balancer** (CLB): Provides basic load balancing across multiple EC2 instances. It operates at both the request level and connection level.\n\n## Benefits of Using ELB\n\n**Scalability**: ELB can automatically scale its capacity according to incoming application and network traffic.\n\n**Fault Tolerance**: ELB provides robustness and fault tolerance for your applications by ensuring that traffic is always directed to healthy targets.\n\n**Security**: You can use AWS managed SSL/TLS certificates for secure communications. ELB also integrates with AWS WAF, providing robust security features.\n\n**Operational Monitoring and Logging**: ELB provides access logs capturing detailed information about all requests sent to your load balancer, and integrates with Amazon CloudWatch metrics to provide operational visibility.\n```\n"
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance to the control that Elastic Load Balancing activity is a central point of communication within an environment can result in detrimental consequences in both security and performance aspects of your AWS environment.\n\n## Security Implications\nElastic Load Balancers (ELB) help to evenly distribute network traffic across many servers to ensure that no single server is overwhelmed with too much traffic. If this control is not complied with, it could lead to an uneven distribution of traffic leading to potential server crashes. Also, it can expose vulnerabilities where certain nodes become overused and thereby an easy target for malicious attacks.\n\n## Performance Implications\nNon-compliance with this control could also lead to performance-related issues. This could involve a slower response due to some servers being overwhelmed with too much traffic. It could also lead to a complete halting of services if one server crashes due to being overworked. This directly impacts the user experience, service reliability, and could even lead to financial losses due to service downtime.\n\n## Compliance and Regulatory Implications\nFrom a compliance perspective, following the recommended controls for Elastic Load Balancing ensures you meet certain regulatory standards that require secure and reliable network connectivity. Not observing this control could lead to substantial issues including surcharges, penalties, or legal consequences depending on the nature and severity of the non-compliance.\n\nTherefore, it is necessary to comply with this control to maintain a secure and stable communication environment in AWS operations."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, consider a multi-tier web application that receives heavy traffic. This application is comprised of multiple microservices distributed across different Amazon EC2 instances. To manage the traffic more effectively and ensure reliable service performance, Elastic Load Balancing is deployed.\n\nThe load balancer acts as the single point of contact for all incoming network traffic. It distributes the incoming application traffic across all EC2 instances in such a way that no single instance gets overwhelmed with too many requests. \n\n```markdown\nIn this scenario, Elastic Load Balancing activity is critical to:\n\n1. ***Efficiently Load Balance Traffic:*** It evenly distributes network load to the EC2 instances. This ensures that each instance shares the processing and prevents any particular instance from becoming a bottleneck.\n2. ***Fault Tolerance:*** In case any EC2 instance becomes unhealthy or fails, the load balancer automatically redirects traffic to the remaining operational instances.\n3. ***Scalability:*** During periods of high traffic, the load balancer uses auto-scaling to automatically add more instances to the pool to handle the increased traffic.\n4. ***Security:*** All traffic goes through the load balancer, it is also a centralized location to deploy security measures like SSL/TLS for encrypted connections.\n```\n\nThus, Elastic Load Balancing plays an integral role in managing network traffic, ensuring service reliability, and enhancing application security."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_s3_bucket_cross_region_replication_enabled",
    "Title": "S3 bucket cross-region replication should be enabled",
    "Description": "Amazon Simple Storage Service (Amazon S3) Cross-Region Replication (CRR) supports maintaining adequate capacity and availability.",
    "QueryID": "aws_s3_bucket_cross_region_replication_enabled",
    "DocumentURI": "policies/aws_s3_bucket_cross_region_replication_enabled.md",
    "ManualVerification": false,
    "Severity": "low",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/S3"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "Amazon Simple Storage Service (Amazon S3) Cross-Region Replication (CRR) is a feature that automatically replicates data across different AWS regions. This feature enhances the availability and durability of your data, making it a great solution for maintaining data backups, ensuring faster response times, and complying with data residence requirements.\n\nUsing CRR, you can ensure that your data is stored adequately across multiple geographically disparate regions, thus maintaining a high level of data availability. This is particularly useful in cases of regional failures, as your data will still be accessible from another region. \n\nIt works by identifying the changes (PUT, POST, DELETE, or COPY) made in the source bucket and replicating them to the destination bucket in a different region. It applies to all objects present in the source bucket, including all previous versions of the files (when versioning is enabled). \n\n```markdown\n**Key Features of Amazon S3 Cross-Region Replication (CRR):**\n- **Increased durability and availability:** By replicating data across multiple regions, the chances of data loss are significantly reduced. \n- **Maintain data backups:** It provides a simple method to keep backups of important data. The replicated data that's stored in another region serves as a backup that can be used in the event of any unforeseen issues.\n- **Ensure faster response times:** In case of high demand, you can serve your customers from the closest region where your data is replicated, ensuring faster data delivery.\n- **Data residence requirements:** For businesses that need to comply with data-sovereignty regulations that require data to be stored in a particular geographic location, CRR can help them meet those requirements easily.\n```\n\nPlease note that CRR incurs additional costs for data transfer between regions. Make sure to consider these costs while planning your AWS budget."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the Amazon S3 Cross-Region Replication (CRR) control can have serious implications for a business, both financially and operationally. This can result in:\n\n1. **Loss of Data**: If a disaster or system failure occurs in one region, you could potentially lose all of your data if it's not replicated to another region.\n\n2. **Downtime**: Without cross-region replication, service interruption can occur if the main server fails. This could lead to loss of business, customer dissatisfaction, and tarnished reputation.\n\n3. **Operational Disruptions**: In the event of a system failure in the master region, operations would need to be paused or significantly disrupted while the issue is fixed.\n\n4. **Increased Costs**: Recovering lost data can be very expensive. Moreover, dealing with system failures and operational disruptions can increase costs in terms of extra labor hours, lost productivity, and possibly even compensation for customers.\n\n5. **Regulatory Fines**: Regulation such as GDPR and HIPAA require certain standards for data recovery and availability. If found non-compliant, organizations can be subject to hefty fines.\n   \n6. **Breach of SLAs**: Most businesses have some form of Service Level Agreement (SLA) in place with their customers. Non-compliance with Amazon S3 CRR could potentially lead to a breach of these SLAs, leading to legal implications and possible loss of business.\n\nBy maintaining adequate capacity and availability through AWS S3 CRR, businesses can protect themselves from the potential costs of non-compliance while ensuring smooth operations and uninterrupted service to their customers."
      ],
      "x-kaytu-usefulness-example": [
        "An example of the usefulness of Amazon Simple Storage Service (Amazon S3) Cross-Region Replication (CRR) can be seen in a business scenario where a company operates from multiple global locations. The company uses the S3 service for storing various types of data such as business reports, customer databases, product catalogues, and advertisement materials.\n\nAs such, the company must ensure that its data is always available and can be accessed by all its offices around the world. Using the Cross-Region Replication feature, the company can automatically and asynchronously copy objects across multiple S3 buckets located in different AWS regions.\n\nThis functionality ensures high availability and redundancy for their data. If one data center goes down due to an unexpected event like natural disaster, technical issues or other emergencies, the data remains safe and accessible in the other regions. This way, the business operations aren't disrupted and service downtime minimized, hence maintaining adequate capacity and availability at all times.\n\nTo illustrate, let's consider the markup example:\n\n```markdown\nCompany XYZ operates in multiple regions: North America, Europe, and Asia. They host their digital products, such as software packages and user manuals, on an Amazon S3 bucket located in the North America region.\n\nHowever, their Asian and European users experience higher latency when accessing these resources due to geographical distance. To improve their user experience, Company XYZ decides to use Amazon S3 Cross-Region Replication (CRR).\n\nThey create additional S3 buckets in the Asia and Europe regions, and setup CRR to automatically replicate files from the North America bucket to the other two. \n\nWith this setup, when a new product version is uploaded to the North America bucket, it's automatically copied to the other two, keeping everything in sync. The result is lower latency and faster download times for their global users, ensuring that their operations run smoothly, the customer experience is improved and adequate data capacity and availability is maintained.\n```\nThis is a simple yet effective demonstration of how Amazon S3 Cross-Region Replication can help businesses maintain adequate capacity and availability of their data in real world scenarios."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_instance_encryption_at_rest_enabled",
    "Title": "RDS DB instance encryption at rest should be enabled",
    "Description": "To help protect data at rest, ensure that encryption is enabled for your Amazon Relational Database Service (Amazon RDS) instances.",
    "QueryID": "aws_rds_db_instance_encryption_at_rest_enabled",
    "DocumentURI": "policies/aws_rds_db_instance_encryption_at_rest_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/RDS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: Enabling Encryption on Amazon RDS Instances\n\nIn Amazon Web Services (AWS), a key component of data security is protecting data at rest. Especially for Amazon Relational Database Service (RDS) instances, enabling encryption is a crucial measure to enhance security.\n\n## Importance of Encryption for RDS\nEncryption transforms data into ciphertext — a format that is unreadable without the decryption key. In terms of RDS, enabling encryption can help protect stored data, automated backups, read replicas, and snapshots. \n\n## Enabling Encryption\nTo enable encryption on RDS instances, follow these steps while creating a new instance:\n1. In the AWS Management Console, choose **RDS** under Database.\n2. Choose **Create Database**, and select your preferred Database creation method (Standard or Easy Create).\n3. Consider your database settings — for instance, the engine type, version, and template.\n4. Under the **Additional configuration** section, find the **Enable encryption** option.\n5. When you tick the box, select your desired master key from the drop-down menu. This key will be used to encrypt your database and all backups.\n6. Continue and complete the setup process.\n\nRemember, only newly created RDS instances can be encrypted. If you want to encrypt an existing database, consider creating a snapshot, copying and encrypting it, then launching a new encrypted database.\n\nEnabling encryption for Amazon RDS instances fortifies your data security. It offers enhanced protection against unauthorized access and data breaches, thus complying with data privacy regulations and customer trust."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the aforementioned AWS control pertaining to encryption for Amazon RDS instances may incur several severe costs, such as:\n\n1. **Data Breaches**: Unencrypted data is a huge risk as it can be accessed and understood without needing a special key. This would make the sensitive data susceptible to unauthorized access and data breaches, which can not only lead to financial loss but also negatively affect the organization's reputation.\n\n2. **Regulatory Penalties**: Depending on the nature of your data, not ensuring encryption can lead to non-compliance with various legal and regulatory mandates (like GDPR, HIPAA, etc.). These breaches can result in hefty fines and penalties.\n\n3. **Loss of Trust**: If sensitive information is compromised, the affected parties can lose trust in the company’s ability to safeguard their information, leading to loss of customers or partners.\n\n4. **Recovery And Mitigation Cost**: If a data breach occurs, significant resources will have to be put into damage control, investigation, and countermeasures, which can represent significant costs.\n\n5. **Legal Liabilities**: In the event of a breach, the company could face lawsuits, especially if it is found to have been negligent in its duty to protect customer data. This would involve attorney fees, court costs, and potential settlement or judgment amounts.\n\nIn conclusion, failing to ensure encryption of Amazon RDS instances could lead to substantial financial implications, reputational damage, and legal troubles. It is, therefore, crucial to not neglect this aspect of securing an organization's data. AWS offers managed services that make it easier to set up and manage encryption, so there isn't a compelling reason to avoid doing so."
      ],
      "x-kaytu-usefulness-example": [
        "An example of the usefulness of enabling encryption for Amazon RDS instances is given below:\n\nA large financial institution wants to migrate its data to the cloud. They have a significant number of financial and personal data about their clients, that is deemed sensitive. It's their utmost priority to secure this data, as any unauthorized access or breach could have severe consequences, both legally and reputation wise. \n\nFor this purpose, they are considering Amazon Web Services (AWS) to store their data. They plan to use Amazon Relational Database Service (RDS) for storing this data. However, to meet regulatory compliance and to further secure data from any potential threats or unauthorized access, they need to assure that data at rest is also secure.\n\nThis is where the AWS control to enable encryption for Amazon RDS instances comes into play. By enabling encryption for their RDS instances, the financial institution can ensure that all data at rest is encrypted. Thus, making it unreadable without the necessary encryption keys, providing an added layer of protection for sensitive data.\n\nThis ensures that even if someone manages to gain unauthorized access to the raw storage, it would be nearly impossible for them to read the data due to encryption. In this way, this AWS control aids in enhancing the security posture of the financial institution's data, addressing a critical aspect of their compliance requirements. \n\nNot only does this protect the institution, but it also develops trust among their clients, witnessing that their data is handled with the utmost level of security, meeting all the necessary regulatory requirements."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_backup_recovery_point_min_retention_35_days",
    "Title": "Backup recovery points should not expire before retention period",
    "Description": "Ensure a recovery point expires no earlier than after the specified period. The rule is non-compliant if the recovery point has a retention point less than 35 days.",
    "QueryID": "aws_backup_recovery_point_min_retention_35_days",
    "DocumentURI": "policies/aws_backup_recovery_point_min_retention_35_days.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/Backup"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "As an AWS control, this essentially means that a recovery point (a backup of the data in a database) should not be set to expire or get deleted before a certain specified period, in this case, 35 days. If the recovery point is set to be retained less than these 35 days, the rule will be considered non-compliant.\n\nThis rule is essential for data recoverability in information systems. If a system failure or data corruption occurs, the administrator will be able to restore the system to a state not more than 35 days old, ensuring minimal loss of data.\n\nThis rule is typically employed within the context of AWS Backup, a fully-managed AWS service providing cost-effective, automated backup solutions for AWS resources.\n\n```xml\n\u003crule\u003e\n    \u003ctitle\u003eRecovery Point Retention Period\u003c/title\u003e\n    \u003cdescription\u003eEnsure a recovery point is not set to expire before 35 days\u003c/description\u003e\n    \u003ccriteria\u003e\n        \u003ccondition\u003e\n            \u003cattribute\u003eRecovery Point Retention Period\u003c/attribute\u003e\n            \u003coperator\u003eLESS THAN\u003c/operator\u003e\n            \u003cvalue\u003e35\u003c/value\u003e\n        \u003c/condition\u003e\n    \u003c/criteria\u003e\n    \u003cactions\u003e\n        \u003ccompliance\u003eNON-COMPLIANT\u003c/compliance\u003e\n    \u003c/actions\u003e\n\u003c/rule\u003e\n```\n\nThe XML markup rule above represents the control. If the \"Recovery Point Retention Period\" is less than 35 days, it marks the rule as \"NON-COMPLIANT\"."
      ],
      "x-kaytu-noncompliance-cost": [
        "The AWS control that ensures a recovery point expires no earlier than the specified period is a key part of your disaster recovery and compliance strategy. Non-compliance to this control can result in significant costs from several standpoints.\n\n1. **Data Loss**: A recovery point with retention less than the specified 35 days may lead to data loss. If data is accidentally deleted or corrupted, and the situation is not detected within the retention period, it may not be recoverable.\n\n2. **Audit Failure**: AWS controls like this one are often used to meet internal and external audit requirements. Non-compliance can result in audit failure, which might lead to financial penalties and reputational harm.\n\n3. **Downtime Costs**: If critical data is lost and can't be immediately restored because it's past the recovery point expiration, it can lead to prolonged periods of downtime. This can result in productivity losses, customer dissatisfaction, and associated financial impacts.\n\n4. **Legal and Compliance Risks**: Certain industries and types of data are regulated and require data to be retained for specific periods. Non-compliance with these retention requirements can lead to hefty legal penalties and sanctions. \n\nThese potential costs underline the importance of ensuring compliance with this AWS control that mandates the minimum retention period for recovery points."
      ],
      "x-kaytu-usefulness-example": [
        "Example:\n\nABC Corporation has a policy to keep its data for at least 35 days for backup and recovery purposes. This policy is in place to combat any unexpected data loss, system malfunction, or cyber-attack. To ensure this policy is implemented and maintained, AWS Control is used. \n\nUsing AWS Control, a rule is set up to ensure that recovery points (or backups) expire no earlier than after 35 days. This rule checks if any of the recovery points has a retention policy of less than 35 days. If such a case is found, the rule is marked as non-compliant, alerting the IT team to take appropriate action.\n\nThis rule ensures ABC Corporation's compliance with the company's data retention policy and adds an extra layer of security, making sure that data is always accessible for at least 35 days. This AWS Control is particularly useful in managing, supervising, and enforcing the company's backup and data retention policies."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_instance_deletion_protection_enabled",
    "Title": "RDS DB instances should have deletion protection enabled",
    "Description": "Ensure Amazon Relational Database Service (Amazon RDS) instances have deletion protection enabled.",
    "QueryID": "aws_rds_db_instance_deletion_protection_enabled",
    "DocumentURI": "policies/aws_rds_db_instance_deletion_protection_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/RDS"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "AWS provides a feature named `Deletion Protection` for Amazon RDS instances. This feature can prevent RDS databases from being accidentally deleted. When `Deletion Protection` is enabled, you can't delete a database through AWS Console, AWS CLI or AWS API operations.\n\n### How to enable Deletion Protection on existing RDS instances\n\nFor those who have already created RDS instances and wish to enable `Deletion Protection`, follow the steps below,\n\n1. Sign in to the AWS Management Console and open the Amazon RDS console at https://console.aws.amazon.com/rds/.\n\n2. In the navigation pane, choose `Databases`, and then choose the RDS instance that you want to modify.\n\n3. Choose `Modify`. This leads to the `Modify DB Instance` page.\n\n4. In the `Deletion protection` section, select the `Enable deletion protection` checkbox.\n\n5. Choose `Continue` and check the summary of modifications.\n\n6. To apply the changes immediately, enable the `Apply immediately` option. \n\n### How to enable Deletion Protection during the creation of RDS instances\n\nFor those who are creating a new RDS instance, `Deletion Protection` can be enabled using the steps below,\n\n1. In the `Settings` section, specify the DB instance identifier.\n\n2. In the `Deletion protection` section, select the `Enable deletion protection` checkbox.\n\n3. Complete other settings and launch the RDS instance.\n\nIt is recommended to always enable `Deletion Protection` for RDS instances to avoid the accidental loss of databases."
      ],
      "x-kaytu-noncompliance-cost": [
        "AWS recommends enabling deletion protection in Amazon RDS instances to prevent accidental database deletion. \n\nNon-compliance to this control can lead to the following potential costs:\n\n1. **Data Loss**: The most significant cost of non-compliance is the potential for data loss. If an RDS instance is accidentally or maliciously deleted without being backed up, all the data contained within it could be lost. \n\n2. **Financial Impact**: Data loss can result in financial repercussions. The implications of data loss can be direct, if the data itself had financial value; or indirect, if the loss affects business operations, customer relations, or regulatory compliance.\n\n3. **Business Continuity**: Deleting critical RDS instances can halt operations, causing significant business disruption. \n\n4. **Reputation Damage**: If the lost data was customer-related or led to extended downtime, it could damage the company's reputation.\n\n5. **Recovery Costs**: If deletions occur and you do not have adequate backups, you may need to invest in data recovery efforts–if recovery is even possible.\n\n6. **Regulatory Fines**: If the instance contained data subject to regulation (HIPAA, GDPR, PCI DSS, etc.) and it is lost without possibility of recovery, your company could face regulatory fines and penalties.\n\nTherefore, compliance with this control is crucial to prevent accidental deletions and the associated fallout."
      ],
      "x-kaytu-usefulness-example": [
        "In the scenario of a commercial organization running a crucial customer management system on an Amazon RDS instance, deletion protection becomes highly useful. An accidental deletion or malicious attack can result in loss of vital customer data and disrupt the proper functioning of the system.\n\nIf deletion protection is enabled for the RDS instance, it'll prevent the database from being accidentally deleted. This control ensures that the database cannot be deleted until explicitly modified to allow deletion, adding an extra layer of security and reducing the risk of data loss. \n\nImplementation of this control is particularly necessary in environments where multiple people have access to administrative privileges, thereby increasing the likelihood of accidental deletion. This protection, therefore, helps the organization to maintain data integrity and consistency, and save significant restoration cost and time. \n\n```\nExample in Markup:\n\nTo check if your RDS DB instances have deletion protection feature enabled, perform the following steps:\n\n1. Sign in to the AWS Management Console.\n2. Navigate to RDS dashboard at https://console.aws.amazon.com/rds/\n3. In the navigation pane, click `DB Instances`.\n4. Choose the DB instance that you want to check. \n5. In the details section, check the `Deletion Protection` attribute.\n6. If the `Deletion Protection` attribute value is set to `No`, the selected Amazon RDS instance does not have deletion protection enabled.\n\nAWS CLI can also be used to check if your RDS DB instances have deletion protection feature enabled. Here is a sample command:\n\n```sh\naws rds describe-db-instances --region us-east-1 --query 'DBInstances[*].[DBInstanceIdentifier,DeletionProtection]'\n```\nIn the output of this command, if the `DeletionProtection` attribute value is set to `false`, that means the selected Amazon RDS instance does not have deletion protection enabled.\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_redshift_cluster_maintenance_settings_check",
    "Title": "Amazon Redshift should have required maintenance settings",
    "Description": "Ensure whether Amazon Redshift clusters have the specified maintenance settings. Redshift clusters `allowVersionUpgrade` should be set to `true` and `automatedSnapshotRetentionPeriod` should be greater than 7.",
    "QueryID": "aws_redshift_cluster_maintenance_settings_check",
    "DocumentURI": "policies/aws_redshift_cluster_maintenance_settings_check.md",
    "ManualVerification": false,
    "Severity": "medium",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/Redshift"
      ],
      "x-kaytu-explanation": [
        "This AWS Control implies to setting certain configurations in the Amazon Redshift Cluster for proper maintenance and optimization. \n\n- **`allowVersionUpgrade`**: By setting this to `true`, you enable the automatic upgrades to the engine version of your Amazon Redshift clusters when a new version is released. This ensures that your Redshift clusters are always running on the latest version, enhancing security, performance and other additional features.\n\n- **`automatedSnapshotRetentionPeriod`**: This is the duration in days that the automated snapshots are retained. By setting this to a number greater than 7, you ensure that the snapshots of the clusters are available for at a minimum of one week. Snapshots are used to restore your cluster to its prior state, in case of failures or calamities.\n\nIn markup format, the settings would be represented as:\n\n```\n---\nResources:\n  MyRedshiftCluster:\n    Type: 'AWS::Redshift::Cluster'\n    Properties:\n      ...\n      AllowVersionUpgrade: true\n      AutomatedSnapshotRetentionPeriod: 10\n      ...\n---\n```\nHere, 10 is the number of days the snapshots are retained. It can be any number greater than 7 as per your requirement."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to this AWS Control implies that your Amazon Redshift clusters might be under risk due to the following reasons:\n\n1. **allowVersionUpgrade**: When this option is set to `false`, it prevents your Amazon Redshift clusters from being upgraded to newer versions when they become available. The newer versions often come with critical performance improvements, bug fixes, and security patches. If you are not upgrading your clusters, you might face increased security vulnerabilities, reduced system efficiency and overall poor performance.\n\n   The cost of non-compliance here also includes potential financial losses due to system downtime or breaches, loss of customer trust, and even possible legal implications if data breaches occur.\n\n2. **automatedSnapshotRetentionPeriod**: If this is set to 7 or less, older automated snapshots of your Redshift clusters get deleted after 7 days. If an issue arises that requires restoring your system to a point in time beyond the 7-day limit, you may not have the necessary backups available. This could result in significant data losses and the resources spent to recover or recreate this lost data, if that's even possible.\n\n   Additionally, if these data losses lead to interruptions in service or unfulfilled service level agreements (SLAs), it could also mean lost revenue, penalties and damage to your brand's reputation.\n\nTherefore, it's crucial to comply with this AWS Control to ensure that your Amazon Redshift clusters are always maintained with the latest updates and that backups are retained for an adequate period of time."
      ],
      "x-kaytu-usefulness-example": [
        "```markdown\nExample:\n\nConsider a scenario in a corporate setup where multiple Amazon Redshift clusters are used for big data analysis and predictive analytics. These clusters contain a vast amount of crucial company data.\n\nIn such a case, ensuring `allowVersionUpgrade` is set to `true` is necessary as it ensures your clusters are always running on the latest version. This is crucial not only for using any new features and improvements, but also to keep your clusters protected with the latest security updates, which can prevent unauthorized access or potential data loss scenarios.\n\nFurthermore, setting the `automatedSnapshotRetentionPeriod` to be greater than 7 is significant for maintaining proper backups. If your organization faces any data loss or corruption, you can use these snapshots to restore your database to a previous state. Thus, keeping a sufficient automated snapshot retention period is key for disaster recovery and business continuity. This setting allows running your critical business functions without interruption even in the event of a disaster.\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_redshift_cluster_encryption_logging_enabled",
    "Title": "Redshift cluster audit logging and encryption should be enabled",
    "Description": "To protect data at rest, ensure that encryption is enabled for your Amazon Redshift clusters. You must also ensure that required configurations are deployed on Amazon Redshift clusters. The audit logging should be enabled to provide information about connections and user activities in the database.",
    "QueryID": "aws_redshift_cluster_encryption_logging_enabled",
    "DocumentURI": "policies/aws_redshift_cluster_encryption_logging_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/Redshift"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "Below is the AWS control explained in a markup format:\n\n```markdown\n## AWS Control: Encrypt data at rest in Amazon Redshift Clusters\n\nThe data stored in Amazon Redshift Clusters should be encrypted to secure it from unauthorized access.\n\n1. **Encrypt Amazon Redshift Clusters:** Ensure that encryption is enabled for your Amazon Redshift clusters to protect them against any potential threats or breaches.\n\n    **How to enable encryption:**  \n    Encryption can be enabled during the creation of a new Amazon Redshift Cluster or can also be added to an existing cluster by creating a snapshot, copying the snapshot with encryption, and then restoring the cluster from the encrypted snapshot.\n\n2. **Configure Amazon Redshift Clusters:** Required configurations need to adhere to lessening the potential for security risks. Carefully manage and review configurations of your clusters, network, and database settings. \n\n3. **Enable audit logging:** To monitor activities within your Amazon Redshift clusters, enabling audit logging is fundamental. The logs provide information about connections to the database and user activities. It helps in identifying potential security issues, tracking changes and maintaining a comprehensive audit trail of work done in your Redshift environment.\n\n    **How to enable audit logging:**  \n    Audit logging can be enabled from the AWS Management Console, AWS CLI, or Amazon Redshift API by updating the `loggingEnabled` parameter to `true` in the `DBClusterParameterGroup` resource.\n```\nThis tutorial provides you with important details on implementing security precautions on your Redshift Clusters. Following these steps will ensure a higher level of security for your data.\n"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance to the said AWS control may lead to several critical and costly impacts including:\n\n1. **Data Breach**: Without encryption, sensitive data stored in Amazon Redshift clusters is vulnerable to unauthorized access and potential theft. The cost associated with a data breach can be extraordinarily high, owing to regulatory penalties, restitutions, damage to brand reputation and potential loss of customers or clients.\n\n2. **Regulatory Fines**: Compliance to various regulations such as GDPR, HIPAA, etc., necessitates the safeguarding of data at rest via encryption. Non-compliance can result in hefty penalties and fines.\n\n3. **Loss of Audit Trails**: Without enabling audit logging, there's a lack of visibility and accountability for activities conducted within the database. This might restrict the ability to investigate and mitigate a security incident, understand user behavior or meet compliance requirements related to logging and monitoring.\n\n4. **Increased Vulnerability**: Without required configurations on Amazon Redshift clusters, security vulnerabilities might be exploited by malicious actors, leading to potential losses.\n\n5. **Legal Repercussions**: Not ensuring sufficient security measures, which result in a data breach, may lead to legal actions by impacted parties, resulting in financial liabilities and potential reputational damage."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, Bob is a cybersecurity specialist at an organization dealing with sensitive client data. This company utilizes Amazon Redshift for cloud storage and big data solutions. Bob recognizes the importance of securing this data to maintain customer trust and comply with data protection regulations.\n\nTo ensure data security, Bob enables an AWS Control over the Redshift clusters. He turns on encryption for data at rest, making sure all stored data is scrambled and unreadable without a proper decryption key. This step is crucial in protecting against unauthorized access. \n\nBob also confirms that all necessary configurations are activated on Redshift clusters. These configurations include network settings, access roles, and permissions, ensuring only authorized users have access to the data.\n\nFinally, Bob activates the audit logging. Audit logs provide a record of all connections to the database and all user activities within it. This information helps Bob track who accessed what data and when, making it easier to identify potential security threats. \n\nThis AWS Control, therefore, helps Bob maintain a safe, secure, and compliant data environment, where sensitive client information is well-protected against unauthorized access and potential breaches. By managing encryption, configurations, and audit logging, Bob significantly enhances his organization's data security posture."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_autoscaling_launch_config_public_ip_disabled",
    "Title": "Auto Scaling launch config public IP should be disabled",
    "Description": "Ensure if Amazon EC2 Auto Scaling groups have public IP addresses enabled through Launch Configurations. This rule is non compliant if the Launch Configuration for an Auto Scaling group has AssociatePublicIpAddress set to 'true'.",
    "QueryID": "aws_autoscaling_launch_config_public_ip_disabled",
    "DocumentURI": "policies/aws_autoscaling_launch_config_public_ip_disabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/AutoScaling"
      ],
      "x-kaytu-explanation": [
        "This AWS control specifies a security policy to scrutinize whether Amazon EC2 Auto Scaling groups are enabled with public IP addresses through Launch Configurations. \n\nAuto Scaling groups in AWS provide flexibility and automatic scalability to manage applications by automatically adjusting the number of Amazon EC2 instances based on the load or other metrics. Launch Configurations, on the other hand, are used to create new instances. If `AssociatePublicIpAddress` is set to 'true', it means that every instance that is launched has a publicly accessible IP address.\n\nThis opens up a potential security vulnerability as instances with public IP addresses are accessible over the internet, and they may undergo attacks if proper security measures are not in place. \n\nSo, this rule determines the compliance status based on whether the `AssociatePublicIpAddress` attribute is set to 'true'. If it's 'true', the rule will be non-compliant, indicating a potential security risk. Thus, it encourages best practice for security by limiting unnecessary public access to instances.\n\nHere is how you can explain this control in markup format:\n\n```\n## AWS Control: Public IP Address Check for EC2 Auto Scaling Groups\n\nThis control checks whether Amazon EC2 Auto Scaling groups have public IP addresses enabled through their respective Launch Configurations.\n\n**Compliance:**\n\nIf the `AssociatePublicIpAddress` in the Launch Configuration for an Auto Scaling group is set to 'true', the rule is non-compliant, indicating a potential security risk.\n\n**Recommendation:**\n\nTo ensure maximum security for your EC2 instances, it's a best practice not to associate public IP addresses unnecessarily. We recommend revising your Launch Configuration settings for each Auto Scaling group, disabling public IP addresses where not required.\n\n```\n"
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS Control bears significant risk and potential costs associated with security, data breaches, and loss of reputation. \n\n1. **Security risks**: Public IP addresses could expose your EC2 instances to potential threats and attacks from the internet. Each instance could become a possible entry point for malicious actions such as hacking, DDOS attacks, ransomware etc. \n\n2. **Data breach**: If instances are exposed to the public internet and compromised, sensitive data stored in these instances could be stolen leading to a data breach, which could cost hefty fines depending on the jurisdiction and the nature of the data.\n\n3. **Operational Interruption**: A successful breach could disrupt your services, leading to unavailability of your services. This could in turn cause loss of business, customer dissatisfaction and tarnished brand reputation.\n\n4. **Legal consequences**:  If you are handling customer data, and it's breached due to non-compliance, you may face legal actions from customers as well as regulatory bodies. \n\n5. **Remediation Cost**: Post breach, The cost of investigations, remediation and fortification of security measures can be significant, these expenses can have a heavy burden on the organization.\n\nWhile the exact monetary cost may depend on the scale of your operations, nature of your data, and the extent of the breach (should one occur), it will almost certainly be greater than the costs of preventing exposure in the first place. Therefore, it is highly recommended to comply with such controls. \n\n`AssociatePublicIpAddress` should be kept 'false' unless there's a definite use case that requires it, and even so, the security measures should be put in place to mitigate any potential risk."
      ],
      "x-kaytu-usefulness-example": [
        "This AWS Control is particularly useful in scenarios where the security and privacy of the application running is paramount. For instance, an organization dealing with sensitive customer data may wish to run their workloads in AWS. \n\nTo further enhance their security procedures and restrict unwanted access, they may apply this rule to ensure that none of the Amazon EC2 instances, created by the Auto Scaling groups, have public IP addresses enabled through their respective Launch Configurations.\n\nThe public IP addresses could expose the EC2 instances and any applications running on them to potential vulnerabilities or attacks from the public internet. Therefore, ensuring that the `AssociatePublicIpAddress` setting is not set to 'true' is crucial in these scenarios.\n\n``` \nExample:\nOrganisation: Alpha Corp\nApplication: Sensitive Data Handling\nDescription: Alpha Corp uses AWS to host its application for handling customer data. To ensure the security and privacy \nof users' data, Alpha Corp enforces the rule that no EC2 instances used by any Auto Scaling groups have public IP \naddresses enabled via the Launch Configurations. This minimizes the instances' exposure to potential threats on the internet.\n```"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ecs_task_definition_user_for_host_mode_check",
    "Title": "ECS task definition container definitions should be checked for host mode",
    "Description": "Check if Amazon Elastic Container Service (Amazon ECS) task definition with host networking mode has 'privileged' or 'user' container definitions.The rule is non compliant for task definitions with host network mode and container definitions of privileged=false or empty and user=root or empty.",
    "QueryID": "aws_ecs_task_definition_user_for_host_mode_check",
    "DocumentURI": "policies/aws_ecs_task_definition_user_for_host_mode_check.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/ECS"
      ],
      "x-kaytu-explanation": [
        "This AWS Control checks if Amazon Elastic Container Service (ECS) task definitions using host networking mode have either 'privileged' or 'user' container definitions.\n\nTask definitions are used to specify the Docker container(s) required to run a task within ECS. Privileged and User are two parameters that can be specified in a container definition.\n\nA \"privileged\" container is given access to all devices on the host. If the task definition specifies 'privileged=true', the container has the same level of access to the underlying resources as the host. This privilege can be misused to gain unauthorized access or perform unauthorized actions and hence this control ensures it is not misused.\n\nThe \"user\" directive defines the username or UID that is used to run the container. If left empty, the default is to use the root user. Running containers as the root user is generally not recommended due to potential security risks. \n\nThe rule is marked as non-compliant if the task definition with host network mode has:\n\n- 'privileged' set to false or not specified at all (empty)\n- 'user' set to root or not set at all (empty)\n\nHere's a markup representation of how such a control definition might look like:\n\n```\nrule_id \"Ensure ECS task definition with host networking mode does not have 'privileged=false' or 'user=root'\"\ndescription \"Checks if host-based ECS task definitions have 'privileged' or 'user' container definitions set.\"\non resource.type == \"aws_ecs_task_definition\"\nthen {\n  check resource.host_network_mode = \"true\"\n  check (resource.container_definitions.privileged != \"false\" \n         and resource.container_definitions.privileged != \"\"  \n         and resource.container_definitions.user != \"root\" \n         and resource.container_definitions.user != \"\")\n}\n```\n\nPlease note that the above is just a pseudocode representation and may not necessarily work in a real cloud-based policy enforcement context."
      ],
      "x-kaytu-noncompliance-cost": [
        "There are several costs of non-compliance to this AWS control:\n\n1. **Security Risks:** If the container definitions for 'privileged' is set to false or empty, and 'user' is set to root or empty, it could potentially expose your ECS tasks to security vulnerabilities. This is because setting 'privileged' to true allows processes within the container to gain additional permissions and not be constrained by the security policies of the container, possibly leading to access control and data security risks.\n\n2. **Regulatory Compliance Costs:** If you're in a regulated industry or operating under standards such as GDPR, ISO 27001, HIPAA, PCI-DSS etc., non-compliance to this AWS control might result in failing audits, incurring heavy fines from regulatory bodies, and causing damage to your organization's reputation.\n\n3. **Operational Issues and Downtime:** Lack of appropriate settings could lead to operational issues and potential downtime for applications running in Amazon ECS. Over time, this could impact business continuity and result in revenue loss.\n\n4. **Increased Maintenance Costs:** Without these settings, ECS task definitions might behave unexpectedly or fail, leading to increased costs related to debugging, maintenance, and support.\n\n5. **Data Breach:** In the worst scenario, non-compliance could lead to a data breach resulting in financial and reputational harm. The cost of a breach typically includes investigative services, remediation activities, notifications, credit monitoring, potential legal liabilities, and public relations efforts.\n\nTherefore, maintaining compliance to AWS controls like this one is crucial for a healthy, secure, and robust cloud environment. Regular audits and remediation efforts should be undertaken to ensure adherence to such rules."
      ],
      "x-kaytu-usefulness-example": [
        "The following instance demonstrates the usefulness of the Amazon AWS control which checks if ECS task definition has 'privileged' or 'user' container definitions:\n\nConsider a scenario where a company has an application deployed on Amazon ECS, using host networking mode. \n\nThe application processes sensitive data and thus, the company wants to ensure that the utmost security measures are taken. To ensure this, they enforce a rule that checks if Amazon Elastic Container Service (ECS) task definition with host networking mode has 'privileged' or 'user' container definitions. \n\nThe usefulness of this control is the improved security it provides. Being able to enforce this control means that the company can ensure containers are not being run with unnecessary permissions, which would otherwise pose a significant security risk.\n\n```markdown\n## Example \n\n**Amazon ECS Task Definition:**\n- Task Definition Name: SensitiveDataTask\n- Network Mode: host\n- Container Definitions:\n    - Name: SensitiveDataContainer\n      Image: aws_account_id.dkr.ecr.region.amazonaws.com/my-web-app\n      Memory: 256\n      CPU: 0\n      Essential: true\n      Privileged: false\n      User: root\n\nAccording to the company's control, the task is non-compliant because the task definition has a container running in 'privileged' mode as 'false' or 'root' user. The company can then take further actions to modify this configuration to comply with its security measures.\n```\nBy having visibility into these configurations and the ability to enforce these rules, the company is benefiting from improved security management of its tasks running on Amazon ECS, thus, minimizing the potential exposure to security risks. This is an example of the usefulness of this specific AWS control."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_vpc_security_group_restrict_ingress_ssh_all",
    "Title": "VPC security groups should restrict ingress SSH access from 0.0.0.0/0",
    "Description": "Amazon Elastic Compute Cloud (Amazon EC2) Security Groups can help manage network access by providing stateful filtering of ingress and egress network traffic to AWS resources.",
    "QueryID": "aws_vpc_security_group_restrict_ingress_ssh_all",
    "DocumentURI": "policies/aws_vpc_security_group_restrict_ingress_ssh_all.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/VPC"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "Security groups in Amazon Elastic Compute Cloud (Amazon EC2) function as virtual firewalls to control inbound and outbound traffic for one or more instances. The rule set you configure in a security group directs the kind of traffic the group allows. Each security group works at the instance level, not the subnet level, meaning it can work across all your AWS resources. \n\nThis control can provide stateful filtering, which tracks the state of incoming and outgoing network connections. It enables the security group to make whatever allowances or restrictions it needs on a per-connection basis rather than relying on predetermined fixed allowances or restrictions. \n\nThe primary use of security groups is managing who can communicate with your AWS resources, enhancing your ability to secure your workloads in the cloud. \n\nBelow is a simple markup representation:\n\n```markdown\n# Amazon EC2 Security Groups\n\nAmazon EC2 Security Groups are a kind of **virtual firewalls** for managing network access to AWS resources.\n\n## Features\n- It controls both **inbound** and **outbound** traffic for one or more instances.\n- It works at the **instance level**, not the subnet level.\n- Provides **stateful filtering**.\n\n## Benefits\n- Helps in managing who can communicate with your AWS resources.\n- Enhances your ability to **secure your workloads** in the cloud.\n```"
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance with this AWS control can be significant and can include:\n\n1. **Increased Security Risks**: Without proper network access management, unauthorized entities might gain access to your AWS resources, leading to possible data breaches and theft of sensitive information.\n\n2. **Interruption of Services**: Improper use of Security Groups can lead to issues with the availability and performance of your services. For example, a misconfigured Security Group could unintentionally block legitimate traffic to your instances, causing service interruptions.\n\n3. **Regulatory and Compliance Penalties**: If you operate in an industry that requires compliance with specific regulations (such as HIPAA or PCI DSS), improper network access management may lead to non-compliance. This can result in heavy fines and legal penalties.\n\n4. **Loss of Customer Trust**: If a data breach or service interruption occurs due to inadequate network access control, it can harm your reputation and lead to a loss of customer trust, which ultimately affects your bottom line.\n\n5. **Increases in Costs**: Inefficient management of network access can lead to unnecessary costs. For example, you might end up paying for more bandwidth than necessary if you allow all traffic to pass through unrestricted.\n\nIt's crucial to properly configure Security Groups and regularly review and update them as necessary to mitigate these risks and costs."
      ],
      "x-kaytu-usefulness-example": [
        "For example, consider a company who has hosted its web application on AWS using Amazon EC2 instances. The application includes web servers for serving web requests, app servers for processing workloads, and a database server to store data. \n\nEach of these server types should only be accessible via specific ports and IPs. For instance, the web server should be accessible to any internet user on port 80 for HTTP and port 443 for HTTPS, the app servers should only be accessible to the web servers on a specific port, and the database server should only be accessible to the app servers on a specific port.\n \nThis is where Amazon EC2 Security Groups come in.\n\n        ```markdown\n        1. **Web Server Security Group**\n            - Ingress: Allow TCP ports 80 and 443 from any IP (0.0.0.0/0)\n            - Egress: Allow TCP port \u003cApp-Server-Port\u003e to App-Server-Security-Group-ID\n        2. **App Server Security Group**\n            - Ingress: Allow TCP port \u003cApp-Server-Port\u003e from Web-Server-Security-Group-ID\n            - Egress: Allow TCP port \u003cDB-Server-Port\u003e to DB-Server-Security-Group-ID\n        3. **DB Server Security Group**\n            - Ingress: Allow TCP port \u003cDB-Server-Port\u003e from App-Server-Security-Group-ID\n        ```\n\nWith Security Groups, the company can ensure that each server type is only accessible on the required ports and to the required IPs, improving their system's security."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_cloudtrail_trail_logs_encrypted_with_kms_cmk",
    "Title": "CloudTrail trail logs should be encrypted with KMS CMK",
    "Description": "To help protect sensitive data at rest, ensure encryption is enabled for your Amazon CloudWatch Log Groups.",
    "QueryID": "aws_cloudtrail_trail_logs_encrypted_with_kms_cmk",
    "DocumentURI": "policies/aws_cloudtrail_trail_logs_encrypted_with_kms_cmk.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/CloudTrail"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# Amazon CloudWatch Log Groups Encryption\n\nAmazon CloudWatch is a monitoring service for AWS resources and applications. CloudWatch provides data and actionable insights to monitor applications, understand, and respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health.\n\nAmazon CloudWatch Logs lets you monitor, store, and access your log files from Amazon EC2 instances, AWS CloudTrail, and other sources. Log data can contain extremely sensitive information, such as personal data or confidential business information.\n\n## Importance of Encryption  \n\nTo enhance the security posture and maintain the privacy of data, it's essential to protect sensitive data. Encryption is one of the best ways to prevent unauthorized access to data. By encrypting the data at rest, you protect it from unauthorized or accidental disclosure to unapproved users or processes.\n\n## Encrypting Amazon CloudWatch Logs\n\nHere's an example of how to ensure encryption is enabled for your Amazon CloudWatch Log Groups:\n\n```markup\n1. Open the AWS Management Console.\n2. Navigate to CloudWatch service.\n3. In the navigation pane, select `Logs`.\n4. Choose the `Log Group` that you want to encrypt.\n5. Under `Actions`, select `Encrypt` and then choose the AWS KMS key that you want to use for encryption. \n\nPlease note, you can also choose to encrypt with AWS managed keys or customer managed keys.\n```\n\nBy ensuring your Amazon CloudWatch Log Groups are encrypted, you allow only authorized users and systems with the correct decryption keys to access the sensitive and confidential data in your logs.\n\n## Conclusion\n\nIn conclusion, encryption at rest for Amazon CloudWatch Log Groups is a critical feature to help protect sensitive data. Always ensure that encryption is enabled and managed correctly to reduce the risk of unauthorized access or data breaches."
      ],
      "x-kaytu-noncompliance-cost": [
        "Non-compliance with this AWS Control can cause financial and reputational damages in various ways:\n\n1. **Data Breach Costs**: If the log data is unencrypted, it might be accessed by unauthorized parties, either internally or externally. This can lead to a data breach, and subsequently, to legal and financial penalties for not adequately protecting sensitive data.\n\n2. **Non-compliance Penalties**: Depending on the jurisdiction and industry of the AWS client, specific standards and regulations (e.g., GDPR, HIPAA, PCI-DSS) might apply concerning data protection. By not enabling encryption, the client might fail compliance audits leading to additional penalties and fines.\n\n3. **Loss of Customer Trust**: If sensitive data were leaked or stolen due to inadequate data protection, the resulting public relations issue might cause customers to lose trust in the company's ability to protect their data.\n\n4. **Remediation Costs**: If a gap in data security is discovered, either via an internal audit or through breach detection, the company would need to invest in remediation actions to secure unprotected data and to implement encryption correctly.\n\n5. **Operational Disruption**: In the course of addressing a security issue or undergoing a compliance review, normal business operations may be disrupted, leading to potential loss of revenue until the issues are fully resolved.  \n\n6. **Increased Insurance Premiums**: Some business insurance plans increase premiums following a data breach. Non-compliance with this control can, therefore, result in higher recurring costs for business insurance.\n\nOverall, failing to comply with this AWS control could significantly impact a company's bottom line as well as its reputation."
      ],
      "x-kaytu-usefulness-example": [
        "For instance, consider a healthcare organization that leverages AWS for managing, analyzing and monitoring their data. This includes patients' medical records, personal information, and other sensitive data that must remain confidential as per the Health Insurance Portability and Accountability Act (HIPAA) regulations.\n\nWhile making use of Amazon CloudWatch Log Groups to monitor and troubleshoot their systems, they must protect these logs from unauthorized access, as it could include sensitive patient information. If these logs are not encrypted, they could be susceptible to security risks, such as data breaches.\n\nHere is where enabling encryption for Amazon CloudWatch Log Groups proves beneficial. Once enabled, all the new data being sent to the Log Group will be automatically encrypted and protected with a specified AWS Key Management Service (KMS) key. Even if an attacker somehow gains access, the encrypted data will be unreadable, thereby protecting sensitive information effectively and ensuring compliance with regulatory standards like HIPAA.\n\n```\nResources:\n  CloudWatchLogGroup:\n    Type: 'AWS::Logs::LogGroup'\n    Properties:\n      LogGroupName: MySecureLogs\n      KmsKeyId: arn:aws:kms:us-east-1:123456789012:key/abcd1234-a123-456a-a12b-a123b4cd56ef\n```\n\nIn the above CloudFormation template snippet, the `KmsKeyId` parameter is used to specify the AWS KMS key to encrypt the CloudWatch Log Group named `MySecureLogs`. This is how ensuring encryption for CloudWatch Log Groups could be useful for an organization, providing an additional layer of security for their sensitive data."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_dynamodb_table_point_in_time_recovery_enabled",
    "Title": "DynamoDB table point-in-time recovery should be enabled",
    "Description": "Enable this rule to check that information has been backed up. It also maintains the backups by ensuring that point-in-time recovery is enabled in Amazon DynamoDB.",
    "QueryID": "aws_dynamodb_table_point_in_time_recovery_enabled",
    "DocumentURI": "policies/aws_dynamodb_table_point_in_time_recovery_enabled.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/DynamoDB"
      ],
      "soc_2": [
        "true"
      ],
      "x-kaytu-explanation": [
        "# AWS Control: DynamoDB Point-In-Time Recovery Enabled\n\nThis AWS (Amazon Web Services) control is a rule that checks whether point-in-time recovery is enabled on your Amazon DynamoDB tables. \n\n## What is Point-In-Time Recovery in DynamoDB?\n\nPoint-In-Time Recovery (PITR) in Amazon DynamoDB is a feature that allows you to restore your table data to any minute in the past 35 days. This can help protect your DynamoDB tables from accidental write or delete operations. \n\n## Why is this Control Important?\n\nHaving PITR enabled on your DynamoDB tables is crucial for data recovery situations. If data in a table is unintentionally modified or deleted, you can restore the table to the state it was at any point in time over the last 35 days. This ensures that you won't lose important data and can quickly get your applications up and running again if something goes wrong. \n\n## How Does This Control Work?\n\nIf this control is enabled, it will regularly check if PITR is enabled for your DynamoDB tables and will alert you if it is not. This provides a proactive approach to ensuring that your DynamoDB tables are always safe and recoverable.\n\n```markdown\nExample of a code snippet in CloudFormation to enable point-in-time recovery in a DynamoDB table:\n\nResources:\n  DynamoDBTable:\n    Type: 'AWS::DynamoDB::Table'\n    Properties:\n      AttributeDefinitions:\n        -\n          AttributeName: 'id'\n          AttributeType: 'N'\n      KeySchema:\n        -\n          AttributeName: 'id'\n          KeyType: 'HASH'\n      ProvisionedThroughput:\n        ReadCapacityUnits: '5'\n        WriteCapacityUnits: '5'\n      TableName: 'MyTable'\n      PointInTimeRecoverySpecification:\n        PointInTimeRecoveryEnabled: true\n```"
      ],
      "x-kaytu-noncompliance-cost": [
        "The cost of non-compliance with this AWS Control can result in several potential negative consequences:\n\n1. **Data Loss** - If the rule is not enabled, there could be a potential risk of data loss. If the data on DynamoDB is accidentally deleted or corrupted without appropriate backups, it may be impossible to retrieve it, leading to disruption in your services and loss of business.\n\n2. **Business Disruption** - In the case of an incident or outage, without appropriate backups and recovery processes in place, business operations could be significantly disrupted. The period required to restore the system and data could lead to downtime and consequently, losses to business.\n\n3. **Increased Costs** - In the case of an incident, without point-in-time recovery enabled, trying to recover lost data can be a sizeable and unexpected expense. The cost of manually trying to restore or replace data can outweigh the cost of maintaining regular backups.\n\n4. **Compliance Violation** - If your business operates in a regulated industry, not having appropriate backup and recovery mechanisms can lead to compliance violations, penalties, and severe reputational damage. \n\n5. **Loss of Customer Trust** - Data loss and resulting service disruptions can lead to loss of customer trust and potential loss of customers, impacting your brand's reputation and revenue. \n\nIn conclusion, non-compliance with AWS controls that guide data backup and the enabling of point-in-time recovery in Amazon DynamoDB can lead to significant financial losses, reputational damage, and potential regulatory penalties. As such, it is crucial to ensure compliance with such controls to maintain data integrity and continuity of business operations."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_apigateway_rest_api_stage_use_ssl_certificate",
    "Title": "API Gateway stage should uses SSL certificate",
    "Description": "Ensure if a REST API stage uses a Secure Sockets Layer (SSL) certificate. This rule is compliant if the REST API stage does not have an associated SSL certificate.",
    "QueryID": "aws_apigateway_rest_api_stage_use_ssl_certificate",
    "DocumentURI": "policies/aws_apigateway_rest_api_stage_use_ssl_certificate.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/APIGateway"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_vpc_route_table_restrict_public_access_to_igw",
    "Title": "VPC route table should restrict public access to IGW",
    "Description": "Ensure if there are public routes in the route table to an Internet Gateway (IGW). The rule is non compliant if a route to an IGW has a destination CIDR block of '0.0.0.0/0' or '::/0'.",
    "QueryID": "aws_vpc_route_table_restrict_public_access_to_igw",
    "DocumentURI": "policies/aws_vpc_route_table_restrict_public_access_to_igw.md",
    "ManualVerification": false,
    "Severity": "high",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/VPC"
      ],
      "x-kaytu-explanation": [
        "AWS control is a term related to Amazon Web Services' infrastructure security and compliance standards. This specific control refers to a security protocol concerning the route configurations between your services on AWS and the public internet.\n\nHere's an explanation of this control in markup format:\n\n```\n- **Title**: Ensure no public routes to an Internet Gateway (IGW)\n\n- **Description**: The control checks whether there are routes in your AWS Route Table that are open to the public, specifically pointing to an Internet Gateway (IGW).\n\n- **Details**: \n  - An AWS route table contains a set of rules, called routes, that are used to determine where network traffic is directed.\n  - An Internet Gateway (IGW) is a component that connects an AWS network to the internet.\n  - This control checks if there are any routes to an IGW that can direct unrestricted, public traffic into your AWS services.\n  - The rule is non-compliant if a route to an IGW has a destination CIDR block of '0.0.0.0/0' or '::/0'. These CIDR blocks represent all possible IPv4 and IPv6 addresses respectively, meaning traffic from any location could be directed to your internal AWS services via the IGW.\n\n- **Remediation**: To ensure the security of your AWS network, you should remove or modify routes with destination CIDR blocks of '0.0.0.0/0' or '::/0'. This will limit the extent of traffic that can be directed to your IGW, thereby reducing the potential attack surface.\n```\n\nThe above control ensures your AWS resources are not exposed to the public, mitigating potential security risks."
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_elb_application_lb_deletion_protection_enabled",
    "Title": "ELB application load balancer deletion protection should be enabled",
    "Description": "This rule ensures that Elastic Load Balancing has deletion protection enabled.",
    "QueryID": "aws_elb_application_lb_deletion_protection_enabled",
    "DocumentURI": "policies/aws_elb_application_lb_deletion_protection_enabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/ELB"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_backup_recovery_point_manual_deletion_disabled",
    "Title": "Backup recovery points manual deletion should be disabled",
    "Description": "Checks if a backup vault has an attached resource-based policy which prevents deletion of recovery points. The rule is non compliant if the Backup Vault does not have resource-based policies or has policies without a suitable 'Deny' statement.",
    "QueryID": "aws_backup_recovery_point_manual_deletion_disabled",
    "DocumentURI": "policies/aws_backup_recovery_point_manual_deletion_disabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/Backup"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_cluster_aurora_protected_by_backup_plan",
    "Title": "RDS Aurora clusters should be protected by backup plan",
    "Description": "Checks if Amazon Aurora DB clusters are protected by a backup plan. The rule is non compliant if the Amazon Relational Database Service (Amazon RDS) Database Cluster is not protected by a backup plan.",
    "QueryID": "aws_rds_db_cluster_aurora_protected_by_backup_plan",
    "DocumentURI": "policies/aws_rds_db_cluster_aurora_protected_by_backup_plan.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/RDS"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_elb_application_network_lb_use_ssl_certificate",
    "Title": "ELB application and network load balancers should only use SSL or HTTPS listeners",
    "Description": "Ensure if Application Load Balancers and Network Load Balancers are configured to use certificates from AWS Certificate Manager (ACM). This rule is compliant if at least 1 load balancer is configured without a certificate from ACM.",
    "QueryID": "aws_elb_application_network_lb_use_ssl_certificate",
    "DocumentURI": "policies/aws_elb_application_network_lb_use_ssl_certificate.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ELB"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_codebuild_project_source_repo_oauth_configured",
    "Title": "CodeBuild GitHub or Bitbucket source repository URLs should use OAuth",
    "Description": "Ensure the GitHub or Bitbucket source repository URL does not contain personal access tokens, user name and password within AWS Codebuild project environments.",
    "QueryID": "aws_codebuild_project_source_repo_oauth_configured",
    "DocumentURI": "policies/aws_codebuild_project_source_repo_oauth_configured.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/CodeBuild"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_redshift_cluster_encryption_in_transit_enabled",
    "Title": "Redshift cluster encryption in transit should be enabled",
    "Description": "Ensure that your Amazon Redshift clusters require TLS/SSL encryption to connect to SQL clients.",
    "QueryID": "aws_redshift_cluster_encryption_in_transit_enabled",
    "DocumentURI": "policies/aws_redshift_cluster_encryption_in_transit_enabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/Redshift"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ssm_managed_instance_compliance_patch_compliant",
    "Title": "SSM managed instance patching should be compliant",
    "Description": "Enable this rule to help with identification and documentation of Amazon Elastic Compute Cloud (Amazon EC2) vulnerabilities.",
    "QueryID": "aws_ssm_managed_instance_compliance_patch_compliant",
    "DocumentURI": "policies/aws_ssm_managed_instance_compliance_patch_compliant.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/SSM"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_redshift_cluster_automatic_snapshots_min_7_days",
    "Title": "Amazon Redshift clusters should have automatic snapshots enabled",
    "Description": "This control checks whether Amazon Redshift clusters have automated snapshots enabled. It also checks whether the snapshot retention period is greater than or equal to seven.",
    "QueryID": "aws_redshift_cluster_automatic_snapshots_min_7_days",
    "DocumentURI": "policies/aws_redshift_cluster_automatic_snapshots_min_7_days.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/Redshift"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_elb_classic_lb_cross_zone_load_balancing_enabled",
    "Title": "ELB classic load balancers should have cross-zone load balancing enabled",
    "Description": "Enable cross-zone load balancing for your Elastic Load Balancers (ELBs) to help maintain adequate capacity and availability. The cross-zone load balancing reduces the need to maintain equivalent numbers of instances in each enabled availability zone.",
    "QueryID": "aws_elb_classic_lb_cross_zone_load_balancing_enabled",
    "DocumentURI": "policies/aws_elb_classic_lb_cross_zone_load_balancing_enabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/ELB"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_vpc_security_group_restrict_ingress_tcp_udp_all",
    "Title": "VPC security groups should restrict ingress TCP and UDP access from 0.0.0.0/0",
    "Description": "Manage access to resources in the AWS Cloud by ensuring common ports are restricted on Amazon Elastic Compute Cloud (Amazon EC2) Security Groups.",
    "QueryID": "aws_vpc_security_group_restrict_ingress_tcp_udp_all",
    "DocumentURI": "policies/aws_vpc_security_group_restrict_ingress_tcp_udp_all.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/VPC"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_dms_replication_instance_not_publicly_accessible",
    "Title": "DMS replication instances should not be publicly accessible",
    "Description": "Manage access to the AWS Cloud by ensuring DMS replication instances cannot be publicly accessed.",
    "QueryID": "aws_dms_replication_instance_not_publicly_accessible",
    "DocumentURI": "policies/aws_dms_replication_instance_not_publicly_accessible.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/DMS"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_secretsmanager_secret_automatic_rotation_enabled",
    "Title": "Secrets Manager secrets should have automatic rotation enabled",
    "Description": "This rule ensures AWS Secrets Manager secrets have rotation enabled. Rotating secrets on a regular schedule can shorten the period a secret is active, and potentially reduce the business impact if the secret is compromised.",
    "QueryID": "aws_secretsmanager_secret_automatic_rotation_enabled",
    "DocumentURI": "policies/aws_secretsmanager_secret_automatic_rotation_enabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/SecretsManager"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_vpc_default_security_group_restricts_all_traffic",
    "Title": "VPC default security group should not allow inbound and outbound traffic",
    "Description": "Amazon Elastic Compute Cloud (Amazon EC2) security groups can help in the management of network access by providing stateful filtering of ingress and egress network traffic to AWS resources.",
    "QueryID": "aws_vpc_default_security_group_restricts_all_traffic",
    "DocumentURI": "policies/aws_vpc_default_security_group_restricts_all_traffic.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/VPC"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_apigateway_stage_cache_encryption_at_rest_enabled",
    "Title": "API Gateway stage cache encryption at rest should be enabled",
    "Description": "To help protect data at rest, ensure encryption is enabled for your API Gateway stage's cache.",
    "QueryID": "aws_apigateway_stage_cache_encryption_at_rest_enabled",
    "DocumentURI": "policies/aws_apigateway_stage_cache_encryption_at_rest_enabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/APIGateway"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_elb_application_lb_redirect_http_request_to_https",
    "Title": "ELB application load balancers should redirect HTTP requests to HTTPS",
    "Description": "To help protect data in transit, ensure that your Application Load Balancer automatically redirects unencrypted HTTP requests to HTTPS.",
    "QueryID": "aws_elb_application_lb_redirect_http_request_to_https",
    "DocumentURI": "policies/aws_elb_application_lb_redirect_http_request_to_https.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ELB"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_ssm_managed_instance_compliance_association_compliant",
    "Title": "SSM managed instance associations should be compliant",
    "Description": "Use AWS Systems Manager Associations to help with inventory of software platforms and applications within an organization.",
    "QueryID": "aws_ssm_managed_instance_compliance_association_compliant",
    "DocumentURI": "policies/aws_ssm_managed_instance_compliance_association_compliant.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/SSM"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_vpc_security_group_restrict_ingress_common_ports_all",
    "Title": "VPC security groups should restrict ingress access on ports 20, 21, 22, 3306, 3389, 4333 from 0.0.0.0/0",
    "Description": "Manage access to resources in the AWS Cloud by ensuring common ports are restricted on Amazon Elastic Compute Cloud (Amazon EC2) security groups.",
    "QueryID": "aws_vpc_security_group_restrict_ingress_common_ports_all",
    "DocumentURI": "policies/aws_vpc_security_group_restrict_ingress_common_ports_all.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "audit_manager_control_tower": [
        "true"
      ],
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/VPC"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_sagemaker_notebook_instance_encryption_at_rest_enabled",
    "Title": "SageMaker notebook instance encryption should be enabled",
    "Description": "To help protect data at rest, ensure encryption with AWS Key Management Service (AWS KMS) is enabled for your SageMaker notebook.",
    "QueryID": "aws_sagemaker_notebook_instance_encryption_at_rest_enabled",
    "DocumentURI": "policies/aws_sagemaker_notebook_instance_encryption_at_rest_enabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/SageMaker"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_instance_and_cluster_enhanced_monitoring_enabled",
    "Title": "RDS DB instance and cluster enhanced monitoring should be enabled",
    "Description": "Enable Amazon Relational Database Service (Amazon RDS) to help monitor Amazon RDS availability. This provides detailed visibility into the health of your Amazon RDS database instances.",
    "QueryID": "aws_rds_db_instance_and_cluster_enhanced_monitoring_enabled",
    "DocumentURI": "policies/aws_rds_db_instance_and_cluster_enhanced_monitoring_enabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/RDS"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_rds_db_instance_automatic_minor_version_upgrade_enabled",
    "Title": "RDS DB instance automatic minor version upgrade should be enabled",
    "Description": "Ensure if Amazon Relational Database Service (RDS) database instances are configured for automatic minor version upgrades. The rule is non compliant if the value of 'autoMinorVersionUpgrade' is false.",
    "QueryID": "aws_rds_db_instance_automatic_minor_version_upgrade_enabled",
    "DocumentURI": "policies/aws_rds_db_instance_automatic_minor_version_upgrade_enabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/RDS"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_sagemaker_notebook_instance_direct_internet_access_disabled",
    "Title": "SageMaker notebook instances should not have direct internet access",
    "Description": "Manage access to resources in the AWS Cloud by ensuring that Amazon SageMaker notebooks do not allow direct internet access.",
    "QueryID": "aws_sagemaker_notebook_instance_direct_internet_access_disabled",
    "DocumentURI": "policies/aws_sagemaker_notebook_instance_direct_internet_access_disabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/SageMaker"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_elasticache_redis_cluster_automatic_backup_retention_15_days",
    "Title": "ElastiCache Redis cluster automatic backup should be enabled with retention period of 15 days or greater",
    "Description": "When automatic backups are enabled, Amazon ElastiCache creates a backup of the cluster on a daily basis. The backup can be retained for a number of days as specified by your organization. Automatic backups can help guard against data loss.",
    "QueryID": "aws_elasticache_redis_cluster_automatic_backup_retention_15_days",
    "DocumentURI": "policies/aws_elasticache_redis_cluster_automatic_backup_retention_15_days.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/ElastiCache"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_sagemaker_endpoint_configuration_encryption_at_rest_enabled",
    "Title": "SageMaker endpoint configuration encryption should be enabled",
    "Description": "To help protect data at rest, ensure encryption with AWS Key Management Service (AWS KMS) is enabled for your SageMaker endpoint.",
    "QueryID": "aws_sagemaker_endpoint_configuration_encryption_at_rest_enabled",
    "DocumentURI": "policies/aws_sagemaker_endpoint_configuration_encryption_at_rest_enabled.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "gdpr": [
        "true"
      ],
      "gxp_21_cfr_part_11": [
        "true"
      ],
      "gxp_eu_annex_11": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_171_rev_2": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_800_53_rev_5": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "rbi_cyber_security": [
        "true"
      ],
      "service": [
        "AWS/SageMaker"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  },
  {
    "ID": "aws_codebuild_project_plaintext_env_variables_no_sensitive_aws_values",
    "Title": "CodeBuild project plaintext environment variables should not contain sensitive AWS values",
    "Description": "Ensure authentication credentials AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY do not exist within AWS CodeBuild project environments. Do not store these variables in clear text. Storing these variables in clear text leads to unintended data exposure and unauthorized access.",
    "QueryID": "aws_codebuild_project_plaintext_env_variables_no_sensitive_aws_values",
    "DocumentURI": "policies/aws_codebuild_project_plaintext_env_variables_no_sensitive_aws_values.md",
    "ManualVerification": false,
    "Severity": "",
    "Tags": {
      "category": [
        "Compliance"
      ],
      "cis_controls_v8_ig1": [
        "true"
      ],
      "cisa_cyber_essentials": [
        "true"
      ],
      "fedramp_low_rev_4": [
        "true"
      ],
      "fedramp_moderate_rev_4": [
        "true"
      ],
      "ffiec": [
        "true"
      ],
      "hipaa_final_omnibus_security_rule_2013": [
        "true"
      ],
      "hipaa_security_rule_2003": [
        "true"
      ],
      "nist_800_53_rev_4": [
        "true"
      ],
      "nist_csf": [
        "true"
      ],
      "pci_dss_v321": [
        "true"
      ],
      "plugin": [
        "aws"
      ],
      "service": [
        "AWS/CodeBuild"
      ],
      "soc_2": [
        "true"
      ]
    },
    "Managed": true
  }
]